{"meta":{"title":"Simple Life","subtitle":null,"description":"A blog of simyy","author":"simyy","url":"http://simyy.com"},"pages":[{"title":"分类","date":"2016-04-07T14:37:48.000Z","updated":"2016-04-07T14:39:43.000Z","comments":true,"path":"categories/index.html","permalink":"http://simyy.com/categories/index.html","excerpt":"","text":""},{"title":"Tagcloud","date":"2016-04-03T10:23:05.000Z","updated":"2016-04-03T10:24:36.000Z","comments":true,"path":"tags/index.html","permalink":"http://simyy.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式事务","slug":"distributed-transaction","date":"2017-05-20T09:40:21.000Z","updated":"2017-05-20T09:47:54.000Z","comments":true,"path":"2017/05/20/distributed-transaction/","link":"","permalink":"http://simyy.com/2017/05/20/distributed-transaction/","excerpt":"简单总结下分布式事务相关二段式、三段式提交以及Paxos算法。","text":"简单总结下分布式事务相关二段式、三段式提交以及Paxos算法。 分布式系统的一致性在分布式领域中，任何一个分布式系统都无法同时满足一致性、可用性和容错性（也就是CAP理论）。 一般来说系统权衡的结果是达到系统的最终一致，而非强一致性。 分布式系统中数据不一致的问题？ 为了提高系统的整体性能，往往采用多机进行分布式部署，为了达到数据的同步必然存在着数据复制。但是数据复制在不同的数据节点之间由于网络等异常原因很容易造成数据不一致的情况。 一致性模型一致性模型可以分为三种： 强一致性：当更新操作完成后，任何后续操作访问的结果都是最新更新的值； 弱一致性：当更新操作完成后，系统不保证后续操作访问的结构都一致，但会尽可能保证在某个时间点可以让数据达到一致； 最终一致性：可以保证最终数据会达到一致。 二段式提交二段式提交协议主要保证分布式事务的原子性（保证所有节点的操作一致），为了达到操作的一致引入了一个协调者来掌管所有节点。 二段式提交可以分为两个阶段：准备阶段和提交阶段。 准备阶段：是一个投票的过程，步骤如下， 协调者向所有节点询问是否提交操作，等带所有参与节点响应； 参与者执行询问发起为止的所有事务操作（已经执行了事务），并把是否执行信息写入日志； 各个参与者向协调者回复询问结果（如果执行成功返回同意，否则返回终止）。 提交阶段：是一个确认的过程，步骤如下， 协调者接收到参与者的询问结果，如果包含终止结果，则需要对所有参与者发送回滚操作，否则向所有参与者发送提交命令； 参与者接受者接收到完成操作，释放整个事务期间占用的资源； 协调者接受到所有参与者完成操作，完成事务。 缺点？ 同步阻塞：所有参与者都是事务阻塞型的，当参与者占用公共资源时，其他节点在访问该公共资源的时候不得不阻塞等待。 单点故障：协调者管理所有参与节点，如果协调者异常，参与者都会一直阻塞下去。 数据不一致：对于提交阶段，如果协调者和参与者出现网络问题，参与者无法正常完成事务。 三段式提交三段式提交是二段式提交的改进版，主要不同在于： 引入超时机制（协调者和参与者） 增加了一个询问阶段（但不会执行） 三个阶段： can commit：协调者询问参与者是否可以执行事务(投票) pre commit：执行事务，并返回执行结果 do commit：提交事务，完成 为了避免阻塞，在提交阶段，如果协调者没有接收到参与者的回应，会执行终止事务，另外，如果参与者一直没有接收到协调者的操作命令，那么就会直接提交事务，但是这并不会避免数据不一致的问题。 Paxos算法http://www.csdn.net/article/2014-01-20/2818197-distributed-system/2","categories":[{"name":"分布式","slug":"分布式","permalink":"http://simyy.com/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://simyy.com/tags/分布式/"}]},{"title":"java threadlocal","slug":"java-threadlocal","date":"2017-04-16T07:19:20.000Z","updated":"2017-04-16T07:23:09.000Z","comments":true,"path":"2017/04/16/java-threadlocal/","link":"","permalink":"http://simyy.com/2017/04/16/java-threadlocal/","excerpt":"介绍java中threadlocal的使用方法。","text":"介绍java中threadlocal的使用方法。 threadlocalthreadlocal主要解决的是每个线程绑定自己的值，也就是说保证相同变量在不同线程的隔离性。 threadlocal保证每一个使用该变量的线程都提供一个变量值得副本，每一个副本的改动不会影响其他副本的值。 默认值默认情况下，初始化的threadlocal值为null，通过继承threadlocal并重写initialValue实现覆盖初始值。 1234567891011public class MyThreadLocal &#123; public static class ThreadLocal1 extends ThreadLocal &#123; @Override protected Object initialValue() &#123; return \"default value\"; &#125; &#125; public static ThreadLocal1 t1 = new ThreadLocal1();&#125; SimpleDateFormat安全格式化SimpleDateFormat类是用来对日期字符串进行解析和格式化输出。 DateFormat和SimpleDateFormat类不都是线程安全的，在多线程环境下调用format和parse方法应该使用同步代码来避免问题。 12345678910111213141516171819202122232425262728293031323334353637public static class DateUtil &#123; // 使用静态变量来存储SimpleDateFormat private static final SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; return sdf.format(date); &#125; public static Date parse(String strDate) throws ParseException &#123; return sdf.parse(strDate); &#125;&#125;public static class TestSimpleDateFormatThreadSafe extends Thread &#123; @Override public void run() &#123; while(true) &#123; try &#123; this.join(2000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; System.out.println(this.getName()+\":\"+DateUtil.parse(\"2013-05-24 06:02:20\")); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public static void main(String[] args) &#123; for(int i = 0; i &lt; 3; i++)&#123; new TestSimpleDateFormatThreadSafe().start(); &#125;&#125; 结果输出：123456789101112Exception in thread &quot;Thread-1&quot; Exception in thread &quot;Thread-2&quot; java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1890)Thread-0:Fri May 24 06:02:20 CST 2013 at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) at java.text.DigitList.getDouble(DigitList.java:169) at java.text.DecimalFormat.parse(DecimalFormat.java:2056) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at MyThreadLocal$DateUtil.parse(MyThreadLocal.java:28) at MyThreadLocal$TestSimpleDateFormatThreadSafe.run(MyThreadLocal.java:42) 这正是由于非线程安全的SimpleDateFormat造成的（详情）。 推荐方法：使用threadlocal对不同线程使用不同的SimpleDateFormat对象。 123456789101112131415161718192021222324public class ThreadLocalDateUtil &#123; private static final String date_format = \"yyyy-MM-dd HH:mm:ss\"; // 使用threadlocal来为不同线程生成相同参数的不同副本 private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;(); public static DateFormat getDateFormat() &#123; DateFormat df = threadLocal.get(); // 默认值为null, 可以继承threadlocal重写initialValue来实现默认值 if(df==null)&#123; df = new SimpleDateFormat(date_format); threadLocal.set(df); &#125; return df; &#125; public static String formatDate(Date date) throws ParseException &#123; return getDateFormat().format(date); &#125; public static Date parse(String strDate) throws ParseException &#123; return getDateFormat().parse(strDate); &#125; &#125; 内存泄露ThreadLocal有一个ThreadLocalMap静态内部类，这个Map为每个线程复制一个变量的拷贝，每一个内部线程都有一个ThreadLocalMap对象。 当线程调用ThreadLocal.set(T object)方法设置变量时， 获取当前线程引用， 获取线程内部的ThreadLocalMap对象 设置map的key值为threadLocal对象，value为参数中的object 当线程调用ThreadLocal.get()方法获取变量时， 获取当前线程引用 以threadLocal对象为key去获取响应的ThreadLocalMap， 如果此Map不存在则初始化一个，否则返回其中的变量 每个线程内部的ThreadLocalMap对象中的key保存的threadLocal对象的引用，但对threadLocal的对象的引用是WeakReference弱引用。 ThreadLocalMap是使用ThreadLocal的弱引用作为Key的，弱引用的对象在 GC 时会被回收。 对于一个正常的Map来说，调用Map.clear方法来清空map，所有对象就会释放。调用map.remove(key)方法，会移除key对应的对象整个entry，这样key和value 就不会任何对象引用，被java虚拟机回收。 内存泄露的原因？ Thread对象里面的ThreadLocalMap中的key是ThreadLocal的对象的弱引用，如果ThreadLocal对象会回收，那个这个对象对应的key就会变为null，那么ThreadLocalMap就无法移除其对应的value，那么value对象就无法被回收，导致内存泄露。 但是如果Thread运行结束，整个线程对象被回收，那么value所引用的对象也就会被垃圾回收。 对于没有使用线程池的方法来说，因为每次线程运行完就退出了，Map里面引用的所有对象都会被垃圾回收，所以没有关系。 如何避免？ ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 参考http://www.cnblogs.com/peida/archive/2013/05/31/3070790.htmlhttp://blog.xiaohansong.com/2016/08/06/ThreadLocal-memory-leak/?utm_source=tuicool&amp;utm_medium=referral","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"}]},{"title":"java 多线程","slug":"java-thread","date":"2017-04-11T16:46:09.000Z","updated":"2017-04-11T16:47:32.000Z","comments":true,"path":"2017/04/12/java-thread/","link":"","permalink":"http://simyy.com/2017/04/12/java-thread/","excerpt":"介绍java中多线程。","text":"介绍java中多线程。 Runnable &amp; ThreadThread类中的start方法通知线程规划器此线程已经就绪，等待调用线程对象的run方法。 使用\brun方法是同步的，此线程对象并不会交给线程规划器来进行处理，而是由当前主线程来同步执行。 使用start方法是同步的，但系统调用run方法的过程是异步执行，因此线程执行与start的顺序是不一致的。 java不支持多继承，对于已有父类的继承类来说，需要实现Runnable接口来实现线程类。 123456789101112public static class RunnableHello extends MyHello implements Runnable &#123; @Override public void run() &#123; // do something &#125;&#125;public static void test1() &#123; RunnableHello runnableHello = new RunnableHello(); Thread thread = new Thread(runnableHello); thread.start();&#125; synchronizedsynchronized关键字能够保证在同一时刻最多只有一个线程执行该段代码。 synchronized可用于标记对象、代码块、函数等。 123synchronized(someObj) &#123; // dosomething &#125;sychronized(this) &#123; // dosomething&#125;synchronized public void method() &#123; // dosomething &#125; interruptinterrupt：用于停止当前线程，不会立刻停止，而是先给当前线程打一个中断标记。 interrupted：测试当前线程是否已经中断状态，执行后清除状态标识为false。 isInterrupted：测试当前线程是否已经中断状态，不清除状态标识。 1234567891011121314151617181920212223242526// 观察打印结果public static class ExtendsThread1 extends Thread &#123; @Override public void run() &#123; try &#123; for (int i = 0; i &lt; 5000; i++) &#123; Thread.sleep(1000); if (this.isInterrupted()) &#123; System.out.println(\"interrupted\"); return; &#125; System.out.println(i); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;public static void test2() &#123; ExtendsThread1 thread1 = new ExtendsThread1(); thread1.start(); Thread.sleep(2000); thread1.interrupt();&#125; suspendsuspend用于暂停线程，但已经废弃了。 废弃原因：使用suspend和resume方法容易因为线程暂定导致数据不同步的情况，会造成死锁。 yieldyeild用于放弃当前占用的CPU资源，但放弃的时间不确定，下一次获得CPU的占用权的时间同样不能确定。 yeild特性： 静态的原生(native)方法； 状态切换的非实时性； 当前正在执行的线程把运行机会交给线程池中拥有相同优先级的线程； 它仅能使一个线程从运行状态转到可运行状态，而不是等待或阻塞状态。 prioritysetPriority方法可以用来设置线程的优先级，优先级高的线程更容易获得CPU的执行权。 java中线程优先级分为1~10，默认定义：123public final static int MIN_PRIORITY = 1;public final static int NORM_PRIORITY = 5;public final static int MAX_PRIORITY = 10; 线程的优先级是可继承的。 daemonjava中可以通过setDaemon方法设置守护线程。 守护线程是一种特殊的线程，只有当JVM停止运行时，该线程才会被结束。例如，垃圾回收器就是一个用于回收内存对象的守护线程。","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"}]},{"title":"java 异常","slug":"java-exception","date":"2017-04-08T14:30:12.000Z","updated":"2017-04-08T14:31:11.000Z","comments":true,"path":"2017/04/08/java-exception/","link":"","permalink":"http://simyy.com/2017/04/08/java-exception/","excerpt":"介绍java中的异常处理。","text":"介绍java中的异常处理。 异常java中的异常可分为检查的异常和非检查异常。 在java中，异常的类层次结构如下，12345 Throwable / \\ Error Exception / \\ / \\VMError AWTError IOException RuntimeException 所有异常都是继承自Throwable，分为Error和Exception。 Error一般是JVM问题导致，也就是说在应用程序控制和处理之外产生的，因此此类异常是不可检查的。 Exception是程序自身产生的异常，是可以通过try-catch和throws来控制。 Exception可被划分为非检查的RuntimeException和检查的IOException。 异常处理捕获异常在java中可以用try-catch语句来捕获异常（可捕获检查和非检查异常），123456789try &#123; // 可能会发生异常的程序代码 &#125; catch (Type1 id1)&#123; // 捕获并处置try抛出的异常类型Type1 &#125; catch (Type2 id2)&#123; // 捕获并处置try抛出的异常类型Type2 &#125; finally &#123; // 无论是否发生异常需要处理&#125; 异常是按照catch的顺序依次捕获的，所以需要按照继承关系来决定捕获顺序。 finally一般用来关闭一下IO操作，例如文件、网络连接。 抛出异常在java中可以使用throws来抛出异常，123public void method() throws Exception1, Exception2, ... &#123; // 执行&#125; throwablethrowable作为异常处理的基类，常用的功能函数有,123getCause(); // 获取异常原因getMessage(); // 异常信息printStackTrace(); // 堆栈跟踪异常信息 注意：RuntimeException不需要抛出，并不会造成编译错误。 自定义异常通过继承Exception以及其子类可以实现自定义异常，123456class MyException extends Exception &#123; public MyException() &#123;&#125; public MyException(String message) &#123; super(message); &#125;&#125; 由于继承Exception，则需要接口处理该异常时添加throws指明抛出检查异常，否则无法编译通过。 如果是继承RuntimeException，则不需要指定throws抛出异常，但是需要注意外层对于该异常的处理方法，这种异常会被隐藏起来。 如何选择异常类型，检查 or 非检查？ 设计异常一般有两种，内部使用和外部使用。 对于外部使用，也就是提供给调用方来处理的异常，一般是由于服务中依赖某不稳定的服务，提供服务方不能处理该异常，需要调用方自行处理该异常情况。 对于内部使用，也就是程序内部自定义的异常，一般是由于跳出当前逻辑处理，直接返回，那么，就需要在逻辑的最外层来统一处理该自定义异常。 性能在使用异常来做控制流程处理的时候，一般有两种方式：自定义异常和自定义返回结果。12345// 自定义异常class BizException extends RuntimeException &#123; ... &#125;// 自定义返回结果class BizResult&lt;T&gt; extends ObjectInfo &#123; ... &#125; 如果使用自定义返回结果，需要在每次调用函数时使用if-else来做判断，这样性能必然是最好的，但是在写业务逻辑的时候就会非常繁琐，每次调用都会添加一次if-else逻辑。 如果使用自定义异常，需要在调用服务的最外层都添加一个针对自定义异常的异常处理，业务逻辑看起来好一点，但是这样性能就会差一点。 如何提高异常性能？ 在参考中的一篇文章中，有提到通过重写fillInStackTrace来提高性能，12345678public synchronized Throwable fillInStackTrace() &#123; if (stackTrace != null || backtrace != null /* Out of protocol state */ ) &#123; fillInStackTrace(0); stackTrace = UNASSIGNED_STACK; &#125; return this;&#125; fillInStackTrace函数是一个同步的方法，在多线程的情况下必然会造成性能下降。在测试过程中发现，即使在单线程的情况下，仍然可以提升很大性能。 参考http://niehan.blog.techweb.com.cn/archives/259.htmlhttp://blog.csdn.net/beijiguangyong/article/details/9080727http://www.blogjava.net/stone2083/archive/2010/07/09/325649.html","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"}]},{"title":"java 日志注解","slug":"java-anotion-log","date":"2017-04-08T09:15:38.000Z","updated":"2017-04-08T09:30:35.000Z","comments":true,"path":"2017/04/08/java-anotion-log/","link":"","permalink":"http://simyy.com/2017/04/08/java-anotion-log/","excerpt":"介绍如何使用java注解来添加日志。","text":"介绍如何使用java注解来添加日志。 注解元注解元注解的作用是注解其他注解， @Target用于描述注解范围。 1234567CONSTRUCTOR:用于描述构造器FIELD:用于描述域LOCAL_VARIABLE:用于描述局部变量METHOD:用于描述方法PACKAGE:用于描述包PARAMETER:用于描述参数TYPE:用于描述类、接口(包括注解类型) 或enum声明 @Retention用于描述注解生命周期。 123SOURCE:在源文件中有效（即源文件保留）CLASS:在class文件中有效（即class保留）RUNTIME:在运行时有效（即运行时保留），可以通过反射获取该注解的属性值 @Documented用于生命构建注解文档。 @Inherited用于描述该注解是可被子类继承的。 注解格式1public @interface 注解名 &#123;定义体&#125; 日志注解Log注解定义，参数包含type日志类型、desc日志描述、throwable是否catch异常、withResult是否记录打印结果。12345678@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.METHOD, ElementType.PARAMETER &#125;)public @interface Log &#123; String type() default LogConst.RUN_LOG; String desc() default \"\"; boolean throwable() default true; boolean withResult() default false;&#125; 需要注意，注解的默认值必须是常量（不可以设置为枚举） 那么常量如何定义？12345678910public class LogConst &#123; // 日志类型 /* 访问日志 */ public final static String ACCESS_LOG = \"ACCESS\"; /* 事件日志 */ public final static String EVENT_LOG = \"EVENT\"; /* 运行日志 */ public final static String RUN_LOG = \"RUN\"; /* 异常日志 */ public final static String EXCP_LOG = \"EXCEPTION\"; 开启注解和AOP配置，12&lt;context:annotation-config/&gt;&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt; 在注解中其实还可以做更多的事？在注解中我们可以增加异常监控，增加事件数量监控等等。 具体实现通过切片来获取当前获取被注释执行的函数情况，这里使用aroundExec来实现。1234567891011121314151617@Aspect@Componentpublic class LogAop &#123;@Around(value = &quot;@annotation(com.simyy.web.aop.Log)&quot;)public Object aroundExec(ProceedingJoinPoint pjp) throws Throwable &#123; Method method = ((MethodSignature) pjp.getSignature()).getMethod(); String logType = method.getAnnotation(Log.class).type(); boolean throwable = method.getAnnotation(Log.class).throwable(); boolean withResult = method.getAnnotation(Log.class).withResult(); // 设置过无允许抛出异常或设置日志类型为异常日志 if (throwable == false || logType.equals(LogConst.EXCP_LOG)) &#123; return withoutExpProcess(pjp, logType , withResult); &#125; else &#123; return withExpProcess(pjp, logType , withResult); &#125;&#125;&#125; 对于异常日志处理方式就是通过增加try-catch来捕获并记录日常情况。其中，getEmptyObjectByClassType是用于在捕获异常后返回空对象或失败对象，getContentByType是用于按固定格式打印日志记录的函数。1234567891011121314151617181920212223242526272829303132333435363738private Object withoutExpProcess(ProceedingJoinPoint pjp, String logType, boolean withResult) throws Throwable &#123; Object[] args = pjp.getArgs(); Method method = ((MethodSignature) pjp.getSignature()).getMethod(); Class returnType = ((MethodSignature) pjp.getSignature()).getReturnType(); Long start = System.currentTimeMillis(); Object retVal; Exception exp = null; try &#123; retVal = pjp.proceed(); &#125; catch (Exception e) &#123; retVal = getEmptyObjectByClassType(returnType); exp = e; &#125; if (exp == null) &#123; // 异常日志类型不需要捕获正常运行记录，因此直接返回 if (logType.equals(LogConst.EXCP_LOG)) &#123; return retVal; &#125; Long useTime = System.currentTimeMillis() - start; String afterLog; if (withResult == true) &#123; afterLog = getContentByType(AFTER_WITH_RESULT, method, args, useTime, retVal); &#125; else &#123; afterLog = getContentByType(AFTER, method, args, useTime, retVal); &#125; LOGGER.info(afterLog); &#125; else &#123; String errorLog = getContentByType(ERROR, method, args, null, null); LOGGER.error(errorLog, exp); &#125; return retVal;&#125; 相反，对于不需要捕获异常的运行日志，只需要记录访问情况，1234567891011121314151617181920private Object withExpProcess(ProceedingJoinPoint pjp, String logType, boolean withResult) throws Throwable &#123; Object[] args = pjp.getArgs(); Method method = ((MethodSignature) pjp.getSignature()).getMethod(); Class returnType = ((MethodSignature) pjp.getSignature()).getReturnType(); Long start = System.currentTimeMillis(); String beforeLog = getContentByType(BEFORE, method, args, null, null); LOGGER.info(beforeLog); Object retVal = pjp.proceed(); Long useTime = System.currentTimeMillis() - start; if (withResult == true) &#123; String afterLog = getContentByType(AFTER_WITH_RESULT, method, args, useTime, retVal); LOGGER.info(afterLog); &#125; return retVal;&#125; 用于返回空对象的函数,1234567891011121314151617private Object getEmptyObjectByClassType(Class classType) &#123; if (classType.equals(List.class)) &#123; return Collections.EMPTY_LIST; &#125; if (classType.equals(Map.class)) &#123; return Collections.EMPTY_MAP; &#125; if (classType.equals(Set.class)) &#123; return Collections.EMPTY_SET; &#125; if (classType.equals(boolean.class) || classType.equals(Boolean.class)) &#123; return false; &#125; // 对于特殊的执行result对象需要特殊处理 // do somethind return null;&#125; 定义日志打印格式用于getContentByType函数，1234567891011private final String BEFORE_TMPL = &quot;TYPE=[%s] DESC=[%s] METHOD=[%s] PARAMS=[%s]&quot;;private final String AFTER_TMPL = &quot;TYPE=[%s] DESC=[%s] METHOD=[%s] PARAMS=[%s] TIME=[%d ms]&quot;;private final String AFTER_TMPL_WITH_RESULT = &quot;TYPE=[%s] DESC=[%s] METHOD=[%s] PARAMS=[%s] TIME=[%d ms] RESULT=[%s]&quot;;private final String ERROR_TMPL = BEFORE_TMPL;// 获取当前类名+函数名称private String getClassAndMethodName(Method method) &#123; String methodName = method.getName(); String className = method.getDeclaringClass().getSimpleName(); return className + &quot;.&quot; + methodName;&#125; More日志采用异步打印提高性能；增加异常监控或重要事件处理监控方案；毕竟反射还是带来一定的性能损耗。对于异常error的代码行这样会丢失，那可以通过Throable.getStackTrace()方法获取运行栈进行分析获取需要的错误行和错误原因。","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"}]},{"title":"python 定时任务","slug":"schedule","date":"2017-04-03T14:39:37.000Z","updated":"2017-04-08T09:25:20.000Z","comments":true,"path":"2017/04/03/schedule/","link":"","permalink":"http://simyy.com/2017/04/03/schedule/","excerpt":"介绍python中实现定时任务的几种方式。","text":"介绍python中实现定时任务的几种方式。 TimerTimer是threading提供的一个阻塞函数，用于延迟执行任务。为了利用Timer实现定时任务，需要使用一个递归来实现，如下12345678910111213from datetime import datetimefrom threading import Timeri = 0def ding(period): global i print i, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") i += 1 t = Timer(period, ding, (period,)) t.start()ding(3) schedsched是Python内置的一个调度模块（详情）。创建调度任务：12sched.scheduler(timefunc, delayfunc)# timefunc, delayfunc延迟等待指定的函数 sched使用调度队列来管理调度任务，1234567891011# 调度任务队列，用来存储未执行的任务scheduler.queue# 调度任务队列是否为空scheduler.empty()# 执行调度任务scheduler.run()# 增加任务，向任务队列中增加任务# delay延迟时间/priority优先级/action执行函数/argument函数参数scheduler.enter(delay, priority, action, argument)# 取消任务，从任务队列中删除任务scheduler.cancel(event) 实例，需要注意如果使用delayfunc中包含延时功能，当延时时间大于任务延时时间）可能会增大延时时间。12345678910111213141516171819import timeimport schedfrom datetime import datetimeschedule = sched.scheduler(time.time, time.sleep)i = 0def ding(period): global i print i, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") schedule.enter(period, 0, ding, (period,)) i += 1def main(period=3): schedule.enter(0, 0, ding, (period,)) schedule.run()main() apschedulerapscheduler是一个Python定时任务框架，全称Advanced Python Scheduler（文档）。 组件triggers是用于启动调度逻辑的触发器，每一个任务都拥有自己的触发器。1特定日期`date`、固定间隔`interval`、特定时间定时`cron` job stores是用来存储调度任务，可选内存、缓存、数据库作为存储媒介。123MemoryJobStore内存存储(默认)MongoDBJobStore数据库存储SQLAlchemyJobStore数据库存储 executors是用来执行调度任务的，可使用线程或进程池作为执行器。12ThreadPoolExecutor线程池执行器(默认)ProcessPoolExecutor进程池执行器 scheduler主要是用于选择阻塞、异步模型，提供了gevent、Tornador等框架的支持。1234567BlockingScheduler：阻塞式；BackgroundScheduler：用于后台执行（需要保证主线程运行，否则关闭）；AsyncIOScheduler：用于异步IO；GeventScheduler: 用于gevent；TornadoScheduler: 用于Tornador;TwistedScheduler: 用于Twisted;QtScheduler: 用于Qt; 举例例1中选择了阻塞调度器BlockingScheduler，并通过设置间隔执行interval，1234567891011121314# 例1from apscheduler.schedulers.blocking import BlockingSchedulerfrom datetime import datetimei = 0def ding(): global i print i, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") i += 1scheduler = BlockingScheduler()scheduler.add_job(ding, 'interval', seconds=3)scheduler.start() 例12中选择了后台调度器BackgroundScheduler，\b增加两个任务分别使用interval和cron来设置触发器，123456789101112131415161718# 例2from apscheduler.schedulers.background import BackgroundSchedulerfrom datetime import datetimei = 0def ding(name): global i print i, name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") i += 1scheduler = BackgroundScheduler()scheduler.add_job(ding, 'interval', seconds=3, args=('interval',))scheduler.add_job(ding, 'cron', second='*/1', args=('cron',))scheduler.start()print 'background scheduler start ... sleep'import timetime.sleep(10) 更多复杂的例子…","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"java object","slug":"java-object","date":"2017-03-30T14:16:32.000Z","updated":"2017-04-08T09:30:30.000Z","comments":true,"path":"2017/03/30/java-object/","link":"","permalink":"http://simyy.com/2017/03/30/java-object/","excerpt":"介绍java中的object类。","text":"介绍java中的object类。 RegisterNativesStackOverflow功能：JVM用来寻找本地函数 getClassStackOverflow功能：返回对象的运行时类 hashCode功能：获取给定对象的唯一整数，用于确定对象在HashTable中的位置，默认值为对象内存地址编号 可使用Apache Commons Lang重写12345678910111213141516@Override public int hashCode() &#123; final int PRIME = 31; return new HashCodeBuilder(getId()%2==0?getId()+1:getId(), PRIME).toHashCode(); &#125;@Override public boolean equals(Object o) &#123; if (o == null) return false; if (o == this) return true; if (o.getClass() != getClass()) return false; return new EqualsBuilder().append(getId(), ((Demo)o).getId()).isEquals(); &#125; equals功能：比较两个对象是否相等注意：如果重写equals方法必须重写hashCode方法 ==：基本类型\b是比较值，引用类型是比较对象地址，而equals方法无法作用于基本类型，默认情况（未重写）下是比较对象的地址。 123public boolean equals(Object obj) &#123; return (this == obj);&#125; clone功能：复制对象复制对象需要先分配一个和源对象同样大小的空间，然后在这个空间中创建一个新的对象。 在JAVA中创建对象的方法有两种：new和clone。new操作符的本意是分配内存。程序执行到new操作符时， 首先去看new操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。clone操作符的本意是复制内存。在第一步是和new相似的， 都是分配内存，调用clone方法时，分配的内存和源对象（即调用clone方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域， 填充完成之后，clone方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。 注意：clone是浅拷贝，也就是只拷贝基本类型的值以及对象的引用。 为了实现深拷贝，需要实现Clonable接口，覆盖并实现clone方法，除了调用父类中的clone方法得到新的对象， 还要将该类中的引用变量也clone出来，1234567891011121314151617181920212223static class Body implements Cloneable&#123; public Head head; public Body() &#123;&#125; public Body(Head head) &#123;this.head = head;&#125; @Override protected Object clone() throws CloneNotSupportedException &#123; Body newBody = (Body) super.clone(); newBody.head = (Head) head.clone(); return newBody; &#125; &#125; static class Head implements Cloneable&#123; public Face face; public Head() &#123;&#125; public Head(Face face)&#123;this.face = face;&#125; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; &#125; 如果在拷贝一个对象时，要想让这个拷贝的对象和源对象完全彼此独立，那么在引用链上的每一级对象都要被显式的拷贝，因此，彻底的深拷贝是非常困难的。 toString默认情况下，返回格式如下,1getClass().getName() + '@' + Integer.toHexString(hashCode()) 一般建议每一个类都重写该方法，便于开发和调试。 object lock所有对象都自动含有单一的锁。 JVM负责跟踪对象被加锁的次数。如果一个对象被解锁，其计数变为0。在任务（线程）第一次给对象加锁的时候，计数变为1。每当这个相同的任务（线程）在此对象上获得锁时，计数会递增。 只有首先获得锁的任务（线程）才能继续获取该对象上的多个锁。 每当任务离开一个synchronized方法，计数递减，当计数为0的时候，锁被完全释放，此时别的任务就可以使用此资源。 而wait、notify、notifyAll是Java语言提供的实现线程间阻塞(Blocking)和控制进程内调度(inter-process communication)的底层机制。 wait1Causes the current thread to wait until another thread invokes the notify() method or the notifyAll() method for this object. wait方法用来将当前线程置入等待状态，直到接到通知或被中断为止。在调用wait方法之前，线程必须要获得该对象的对象级别锁，即只能在同步方法或同步块中调用wait方法。进入wait方法后，当前线程释放锁。在从wait方法返回前，线程与其他线程竞争重新获得锁。如果调用wait方法时，没有持有适当的锁，则抛出IllegalMonitorStateException，它是RuntimeException的一个子类，因此，不需要try-catch结构。 notify1Wakes up a single thread that is waiting on this object&apos;s monitor. notify方法也要在同步方法或同步块中调用，即在调用前，线程也必须要获得该对象的对象级别锁，如果调用notify方法时没有持有适当的锁，也会抛出IllegalMonitorStateException。notify方法用来通知那些可能等待该对象的对象锁的其他线程。如果有多个线程等待，则线程规划器任意挑选出其中一个等待状态的线程来发出通知，并使它等待获取该对象的对象锁。其他处于等待状态的线程在未收到notify或notifyAll通知之前，会一直阻塞在等待状态（即使对象已经空闲）。 注意：处于等待状态的线程等待的是notify或notifyAll的通知而不是对象锁，也就是说notify和notifyAll只是回复竞争的机会。 notifyAll1Wakes up all threads that are waiting on this object&apos;s monitor. notifyAll方法与notify方法的工作方式相同，重要的一点差异是：全部被唤醒。notifyAll会唤醒所有处于等待状态的线程，变成等待获取该对象上的锁，一旦该对象锁被释放，他们就会去竞争对象锁。如果其中一个线程获得了该对象锁，它就会继续往下执行，在它退出synchronized代码块，释放锁后，其他的已经被唤醒的线程将会继续竞争获取该锁，一直进行下去，直到所有被唤醒的线程都执行完毕。 finalizefinalize是GC在回收对象之前最先调用的方法。C++中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性，只有当JVM发生GC时才会回收这部分对象的内存。","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"}]},{"title":"flex","slug":"flex","date":"2017-03-27T15:27:41.000Z","updated":"2017-04-08T09:30:58.000Z","comments":true,"path":"2017/03/27/flex/","link":"","permalink":"http://simyy.com/2017/03/27/flex/","excerpt":"flex属性用于设置或检索弹性盒模型对象的子元素如何分配空间。","text":"flex属性用于设置或检索弹性盒模型对象的子元素如何分配空间。flex属性是flex-grow、flex-shrink和flex-basis属性的简写属性。 注意：如果元素不是弹性盒模型对象的子元素，则 flex 属性不起作用。 属性包括flex-basis、flex-direction、flex-flow、flex-shrink、flex-grow、flex-wrap。 flex-basisflex-basis属性用于设置或检索弹性盒伸缩基准值，默认值auto。12345678可选：number|auto|initial|inherit;其中，number表示长度单位或百分比，auto表示根据内容决定当前盒子长度。//实例：指定class=&quot;test&quot;元素下，第二个div元素的宽度.test div:nth-of-type(2) &#123; -webkit-flex-basis: 80px; /* Safari 6.1+ */ flex-basis: 80px;&#125; nth-of-type()选择器？ nth-of-type(n) 选择器匹配属于父元素的特定类型的第 N 个子元素的每个元素。1234567891011121314//实例1：按奇偶顺序改变子元素颜色p:nth-of-type(odd)&#123; background:#ff0000;&#125;p:nth-of-type(even)&#123; background:#0000ff;&#125;//实例2：按3n+1的顺序添加背景颜色p:nth-of-type(3n+0)&#123; background:#ff0000;&#125; flex-directionflex-direction属性用于指定子元素的排列方向。1可选：row|row-reverse|column|column-reverse|initial|inherit flex-wrapflex-wrap属性用于指定flex容器内子元素的排列方式：单行或多行。1可选：nowarp单行|warp多行|warp-reverse反向多行 flex-flowflex-flow属性用于设置子元素的排列顺序，是flex-direction和flex-wrap复合属性。 flex-growflex-grow属性用于设置子元素扩大的比率。1可选：比例（当小于1时缩小）|initial|inherit flex-shrinkflex-shrink属性用于设置子元素缩小的比率。1可选：比例（当大于1时扩小）|initial|inherit flex-grow和flex-shrink其实可以实现同样的效果。","categories":[{"name":"fe","slug":"fe","permalink":"http://simyy.com/categories/fe/"}],"tags":[{"name":"css","slug":"css","permalink":"http://simyy.com/tags/css/"}]},{"title":"垃圾回收GC","slug":"gc","date":"2017-01-24T17:11:45.000Z","updated":"2017-04-08T09:35:02.000Z","comments":true,"path":"2017/01/25/gc/","link":"","permalink":"http://simyy.com/2017/01/25/gc/","excerpt":"现代语言中，大多数都实现了垃圾回收功能，不再需要开发者自己来释放无用的内存资源，大大提高了开发效率。","text":"现代语言中，大多数都实现了垃圾回收功能，不再需要开发者自己来释放无用的内存资源，大大提高了开发效率。 历史1960 年前后诞生于 MIT 的 Lisp 语言是第一种高度依赖于动态内存分配技术的语言。Lisp 中几乎所有数据都以“表”的形式出现，而“表”所占用的空间则是在堆中动态分配得到的。 Lisp 语言先天就具有的动态内存管理特性要求Lisp 语言的设计者必须解决堆中每一个内存块的自动释放问题，否则，Lisp 程序员就必然被程序中不计其数的 free / delete 语句淹没，这直接导致了垃圾收集技术的诞生和发展。 在学校和工作中，我使用过c/c++这种需要开发者自己来维护内存，时时刻刻需要小心内存泄露的问题，也使用过python、golang、js以及java这种不需要关系内存资源的释放，让自己更加专注于程序的设计和开发，但是也带来了性能的损耗。 垃圾回收GC并不是万能的，如果你接受了它的便利，那么你也需要接受GC带来的性能开销。 内存分配内存分配大概分为三种：静态分配、局部分配以及动态分配。123456789静态分配（ Static Allocation ）：静态变量和全局变量的分配形式。通常，它们无需释放和回收。自动分配（ Automatic Allocation ）：在栈中为局部变量分配内存的方法。栈中的内存可以随着代码块退出时的出栈操作被自动释放。动态分配（ Dynamic Allocation ）：在堆中动态分配内存空间以存储数据的方式。堆中的内存\b需要开发者自己来管理，谨慎对待每一个动态分配的对象，避免造成内存泄露。在软件开发中，如果你懒得释放内存，那么你也需要一台类似的机器人——这其实就是一个由特定算法实现的垃圾收集器。 因此，垃圾回收的目的是回收动态分配的内存。 垃圾回收GC垃圾回收技术主要包括：引用计数、跟踪式以及分代。 引用计数引用计数Reference Counting为每个内存对象维护一个引用计数器,1234567891011// 新的引用指向某对象时，该对象引用计数器加1if add refrence: counter +=1// 对象的引用被销毁时，该对象引用计数器减1if remove refrence: counter -= 1// 对象的计数器归零时，回收该对象所占用的内存资源if counter == 0: free object 每个计数器只记录了其对应对象的局部信息-被引用的次数，而没有全局的应用信息。 由于只维护局部信息，所以不需要扫描全局对象图就可以识别并释放死对象，但也因为缺乏全局对象图信息，所以无法处理循环引用的状况。 循环引用？ A引用B，B又引用了A，那么A和B引用计数器的值均为1，无法释放该资源。12A &lt;- BB &lt;- A 优点 &amp; 缺点12345678优点- 每次创建和销毁都要更新引用计数值，会引起额外的开销- 计数简单，只需要记录自身被引用次数- 实时的垃圾回收，不会造成中断缺点- 无法处理循环引用- 多线程对同一对象计数更新产设个竞争 Python使用了引用计数算法，为了解决循环引用的问题还设计了其他GC模块(标记清除和分代收集)。此外，Python为了解决引用技术的性能问题还引入了内存池机制。 跟踪式跟踪式垃圾回收是比较常见的垃圾回收技术，主要原则是从程序栈的若干个根对象出发，构造一个可达链，对于那些不可达的内存对象，做回收。 如果一个内存对象有被程序中的至少一个变量引用（直接指向或间接指向），则认为该对象可达，否则认为该对象不可达，可以被垃圾回收。 标记清除标记清除Mark-Sweep的基本思路是遍历以对象为节点、以引用为边构成的图，把所有可以访问到的对象打上标记，然后清扫一遍内存空间，把所有没标记的对象释放。 具体过程分两步： 从根（Root）开始\b遍历，被根引用了的对象标记为非垃圾对象，\b非垃圾对象引用的对象同样标记为非垃圾对象，以此递归直到标记完所有对象； 再次从根节点开始，对已被标记为垃圾的对象清除释放。 如图，红色代表垃圾对象，黄色代表非垃圾对象，绿色代表未使用空间。 缺点：内存碎片 由于标记清除不需要移动对象的位置，直接回收垃圾对象必然会造成内存碎片。 标记复制标记复制 Mark-Copy只需要对对象进行一次遍历。 它的基本思想是利用空间换时间，从根开始开始遍历对象，如果对象仍然存在引用，就把它复制到新的内存空间中，一次遍历结束之后，所有存在于新空间的对象就是所有的非垃圾对象，可直接释放掉原有内存空间，不会造成内存碎片。 如图，红色代表垃圾对象，黄色代表非垃圾对象，绿色代表未使用空间。 缺点：浪费内存 标记复制更快速但是需要额外开辟一块用来复制的内存，对垃圾比例较大的情况占优势。 此外，内存空间切换过程中需要暂停程序运行（也就是常说GC停顿）。 标记压缩标记压缩Mark-Compact是在标记清除算法的基础之上增加对象的移动，从而解决内存碎片的问题。 在压缩阶段，由于要移动可达对象，那么需要考虑移动对象时的顺序，一般分为下面三种： 任意顺序 - 即不考虑原先对象的排列顺序，也不考虑对象间的引用关系，随意的移动可达对象，这样可能会有内存访问的局部性问题。 线性顺序 - 在重新排列对象时，会考虑对象间的引用关系，比如A对象引用了B对象，那么就会尽可能的将A，B对象排列在一起。 滑动顺序 - 顾名思义，就是在重新排列对象时，将对象按照原先堆内存中的排列顺序滑动到堆的一端。 现在大多数的垃圾收集算法都是按照任意顺序或滑动顺序去实现的(由于线性顺序需要考虑对象的引用关系所以实现复杂)。 Two-Finger 算法数据移动过程类似快排，分别用两个指针指向内存的两端，分别向中心移动。如果右侧指针遇到垃圾数据直接回收，否则，移动左侧指针直到遇到可替换垃圾对象，然后替换左右指针的对象。 缺点：移动对象的代价 为了避免内存碎片，移动对象需要遍历整个内存空间，还需要暂停程序更换映射地址空间。 分代回收分代回收是根据对象的存活周期的不同将内存划分为几块。一般把java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 年轻代垃圾回收 年轻代回收特点： 新对象的内存分配都是先在Eden区域中进行的； 当Eden区域的空间不足于分配新对象时，就会触发年轻代上的垃圾回收minor gc（minor garbage collection）； 每个对象都有一个年龄，代表对象经历过的minor gc的次数； 当触发minor gc后，所有存活的对象都会被拷贝到一个新的survivor区域，并且年龄增加1；当对象的年龄足够大，它会从survivor内存区域升级到老年代中。 年老代垃圾回收当老年代内存区域（图中Tenured）无法容纳新对象时，这时候就会触发老年代的垃圾回收major gc（major garbage collection”）。 垃圾回收算法PS和CMS 常用的垃圾回收算法有Parallel Scavenge(PS) 和Concurrent Mark Sweep(CMS)，它们的不同之处体现在年老代的垃圾回收过程中，而年轻代的垃圾回收过程在这两种垃圾回收器中基本上是一致的。PS在执行垃圾回收时使用了多线程来一起进行垃圾回收，这样可以提高垃圾回收的效率，CMS在进行垃圾回收时，应用程序可以同时运行。 在年轻代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对他进行分配担保，就必须使用“标记-整理”算法进行回收。 Parallel ScavengePS垃圾回收是由标记清除和标记压缩算法组成。 标记清除算法的一个缺陷就是它会引起内存碎片问题。继而有可能会引发连续的major gc。假设当前存在的内存碎片有10M，但最大的内存碎片只能容纳2M的对象，这个时候如果有一个3M的对象从Survivor区域升级到Tenured区域，那Tenured区域也没有办法存放这个3M的对象。结果就是不断的触发major gc，直到Out of Memory。所以，PS垃圾回收器在清除非可达对象后，还会进行一次compact，来消除内存碎片。 Concurrent Mark SweepCMS在进行垃圾收集时，应用程序是可以并行运行的，因此，它减少了垃圾收集时暂停应用程序的时间。垃圾回收的四个阶段如下，1234567891011121. Initial Mark阶段程序暂停运行`Stop-The-Word`，标记到根对象的第一层孩子节点即停止 ，然后程序恢复运行。由于只标记一层节点，所以暂停时间很短。2. Concurrent Mark阶段以Initial Mark阶段标记的节点为根对象，重新开始标记Tenured区域中的可达对象（不需要暂停应用程序，因此称为&quot;Concurrent Mark&quot;）。由于CMS垃圾回收器和应用程序同时运行，Concurrent Mark阶段它并不保证在Tenured区域的可达对象都被标记了（分配新对象）。3. Remark阶段暂停应用程序，确保所有的可达对象都被标记（Concurrent Mark阶段未标记的，`可多线程标记`）。4. Concurrent Sweep阶段恢复应用程序的执行，执行sweep来清除所有非可达对象所占用的内存空间。 与PS相比，CS垃圾回收算法可以减少程序暂停的时间。 Garbage FirstCMS垃圾收集器虽然减少了暂停应用程序的运行时间，但是由于它没有Compact阶段，它还是存在着内存碎片问题。于是，为了去除内存碎片问题，同时又保留CMS垃圾收集器低暂停时间的优点，JAVA7发布了一个新的垃圾收集器 - G1垃圾收集器。 G1垃圾收集器和CMS垃圾收集器有几点不同，最大的不同是内存的组织方式变了。Eden，Survivor和Tenured等内存区域不再是连续的了，而是变成了一个个大小一样的region - 每个region从1M到32M不等。 G1垃圾回收的四个阶段如下，123456789101112131. Initial Mark阶段同CMS垃圾收集器的Initial Mark阶段一样，程序暂停运行`Stop-The-Word`，标记到根对象的第一层孩子节点即停止 ，然后程序恢复运行。由于只标记一层节点。但它不会单独暂停应用程序的执行，而是在G1触发minor gc是发生。2. Concurrent Mark阶段同CMS垃圾收集器的Concurrent Mark阶段一样，重新开始标记Tenured区域中的可达对象（不需要暂停应用程序）。但G1还会回收掉Tenured region中对象的存活率很小或者基本没有对象存活的内存区域（不需要等待clean up阶段，因此成为Garbage First），并计算每个 region的对象存活率\b以供clean up阶段使用 。3. Remark阶段 同CMS垃圾收集器的Remark阶段一样, 但G1采用一种叫做SATB(snapshot-at-the-begining)的算法能够在Remark阶段更快的标记可达对象。4. Clean up/Copy阶段与CMS中不同，它有一个Clean up/Copy阶段，在minor gc发生的同时G1会挑选出那些对象存活率低的region进行回收。 由于Initial Mark阶段和Clean up/Copy阶段都是跟minor gc同时发生的，相比于CMS，G1暂停应用程序的时间更少，从而提高了垃圾回收的效率。","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"gc","slug":"gc","permalink":"http://simyy.com/tags/gc/"}]},{"title":"单点登录","slug":"sso","date":"2016-12-12T16:51:05.000Z","updated":"2017-04-08T09:33:06.000Z","comments":true,"path":"2016/12/13/sso/","link":"","permalink":"http://simyy.com/2016/12/13/sso/","excerpt":"单点登录全称Single Sign On（简称SSO），是指在多系统中用户只需要登录一个系统，便可在其他所有系统中得到授权而无需再次登录。","text":"单点登录全称Single Sign On（简称SSO），是指在多系统中用户只需要登录一个系统，便可在其他所有系统中得到授权而无需再次登录。 传统方法对于小型站点来说，可以直接通过cookie和session在多个页面下实现登录状态的共享。session常被保存在redis/memcache集群中，以便提高访问效率。使用cookie和session有哪些问题？cookie不安全，需要通过加密算法来处理；使用cookie不能跨域。 单点登录主要思想是设计一个独立于各个系统的登录系统，也即是说要把用户登录的权限验证统一管理。 基本原理单点登录需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。 认证步骤通过SSO认证登录，然后把授权信息发送给各个系统，具体步骤如下： 用户登录某一系统，未登录则跳转到sso认证中心 用户在sso输入用户名密码，认证通过，生成授权令牌 sso把授权令牌发送到各个子系统和用户 用户重新访问原有系统， 通过验证授权令牌，登入系统 注销步骤由于单点登录的状态是统一管理的，那么用户在任何一个系统中注销登录状态的同时需要保证其他系统的授权令牌同时失效，具体步骤如下： 用户在某一个系统注销登录 注销信息会传递给sso认证中心 认证中心再通知各个系统销毁授权令牌 系统特点 sso需要保存一份全局的用户登录状态（也就是授权令牌和登录用户的关系）； sso作为登录状态的唯一管理者，每次授权和销毁都需要同步最新的授权令牌的状态到各个子系统； sso和其他系统之间需要建立一套数据同步机制。 常见问题大量授权请求如何处理？大量授权请求会带来大量的授权数据，由于\b授权数据的临时性，可将这些授权信息存储的缓存系统中，常见memcache/redis等；大量的授权请求会产生大量的授权令牌的生成，常见的授权令牌都需要加密处理，必然影响系统性能，可通过预生成授权令牌的方式来降低批量生成令牌造成的资源竞争。 全局和局部的授权令牌如何同步处理？SSO和系统发生认证和注销操作都需要同步给系统中的每一个系统；授权令牌采用按时间过期的方式，定期更新授权令牌到各个子系统；至少保证登录注销的信息传递到SSO认证中心。 应用常见的单点登录包括一些网站或应用的微信认证登录、QQ认证登录、微博认证登录；淘宝、360、微博、网易、美团的登录认证方式。 其他类似系统全局唯一ID生成器（不仅仅是UUID），可参考微信消息的唯一ID的生成方式","categories":[{"name":"web","slug":"web","permalink":"http://simyy.com/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://simyy.com/tags/web/"}]},{"title":"redis 跳跃表","slug":"redis-skiplist","date":"2016-12-06T15:52:58.000Z","updated":"2017-04-08T09:25:50.000Z","comments":true,"path":"2016/12/06/redis-skiplist/","link":"","permalink":"http://simyy.com/2016/12/06/redis-skiplist/","excerpt":"本文介绍redis中的跳跃表的原理和实现。","text":"本文介绍redis中的跳跃表的原理和实现。 跳跃表跳跃表通过在每个节点维护多个指向其他节点的指针来提高节点的访问效率。","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"redis 字典","slug":"redis-dict","date":"2016-12-03T05:37:02.000Z","updated":"2017-04-08T09:26:04.000Z","comments":true,"path":"2016/12/03/redis-dict/","link":"","permalink":"http://simyy.com/2016/12/03/redis-dict/","excerpt":"本文将介绍redis字典的具体实现。","text":"本文将介绍redis字典的具体实现。 基本结构redis字典和python的字典（java中的map）的性质是一样的，也被称为关联数组或映射，是一种保存键值对的数据结构。 redis字典使用哈希表作为底层实现，字典中的键不允许相同。 123456typedef struct dictht &#123; dictEntry **table; // 哈希表 unsigned long size; // 哈希表大小 unsigned long sizemask; // 哈希表大小掩码 unsigned long used; // 已使用大小&#125; dictht; 哈希表中每一个键值对用dictEntry来表示， 12345678910typedef struct dictEntry &#123; void *key; // 键 union &#123; // 值 void *val; uint64_t u64; int64_t s64; double d; &#125; v; struct dictEntry *next; // 链表指针&#125; dictEntry; 其中，next指针是用来解决哈希表的键冲突的，所有key的哈希值相同的节点都会添加在链表上。v`值支持4种格式：指针、unit64_t、int64_t、double类型。 哈希算法添加新的键值对时，需要先计算键的哈希值和索引值，然后按照索引值把键值对插入到对应的位置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 查找key在索引中的位置static int _dictKeyIndex(dict *d, const void *key, unsigned int hash, dictEntry **existing)&#123; unsigned int idx, table; dictEntry *he; if (existing) *existing = NULL; /* Expand the hash table if needed */ if (_dictExpandIfNeeded(d) == DICT_ERR) // 哈希表扩展 return -1; for (table = 0; table &lt;= 1; table++) &#123; // 遍历两个哈希table idx = hash &amp; d-&gt;ht[table].sizemask; // 获取table上索引位置 /* Search if this slot does not already contain the given key */ he = d-&gt;ht[table].table[idx]; while(he) &#123; // 遍历桶内数据（冲突链） if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123; if (existing) *existing = he; return -1; &#125; he = he-&gt;next; &#125; if (!dictIsRehashing(d)) break; &#125; return idx;&#125;// 添加键值对dictEntry *dictAddRaw(dict *d, void *key, dictEntry **existing)&#123; int index; dictEntry *entry; dictht *ht; if (dictIsRehashing(d)) _dictRehashStep(d); // 如果在rehash，则采用渐进方式 /* Get the index of the new element, or -1 if * the element already exists. */ if ((index = _dictKeyIndex(d, key, dictHashKey(d,key), existing)) == -1) // -1代表已存在 return NULL; ... ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0]; entry = zmalloc(sizeof(*entry)); entry-&gt;next = ht-&gt;table[index]; // 链表头插法 ht-&gt;table[index] = entry; ht-&gt;used++; /* Set the hash entry fields. */ dictSetKey(d, entry, key); return entry;&#125; 功能实现rehash当桶后面的链表不断增长时，访问目标键值变慢，就需要rehash来加快访问速度（减少链表长度）。 此外，随着键值对的添加和删除，需要动态调整哈希表的大小。 在redis中，字典的存储结构如下，1234567typedef struct dict &#123; dictType *type; void *privdata; dictht ht[2]; // 默认使用ht[0],ht[1]用于rehash long rehashidx; /* 是否执行rehash的标识rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */&#125; dict; 其中，ht[2]用来存储字典内容，只有在进行rehash操作时，ht[1]才会被用到（默认hash数据都存储在ht[0]上）。 12345678910111213141516171819202122232425262728293031323334353637383940414243int dictRehash(dict *d, int n) &#123; int empty_visits = n*10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while(n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; dictEntry *de, *nextde; ... assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx); while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; d-&gt;rehashidx++; // 如果当前table桶中数据为空，则增加索引值 if (--empty_visits == 0) return 1; &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; // 获取当前哈希桶中的第一个键值对 /* Move all the keys in this bucket from the old to the new hash HT */ while(de) &#123; // 遍历哈希桶中的所有数据 unsigned int h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; // rehash原有数据到新的哈希表的桶中 de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; &#125; /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) &#123; //如果原有hash表中所有数据已转移完成，则释放旧哈希表 zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; &#125; /* More to rehash... */ return 1;&#125; 在进行rehash的过程中，字典会同时在ht的两个table上进行查找、删除、更新操作，而新增只会在ht[1]上操作。 哈希表扩展redis提供了dictExpand函数来扩展哈希表的大小。 123456789101112131415161718192021222324252627282930313233343536373839// 哈希表扩展int dictExpand(dict *d, unsigned long size)&#123; dictht n; /* the new hash table */ unsigned long realsize = _dictNextPower(size); // 计算哈希表的大小 ... /* Allocate the new hash table and initialize all pointers to NULL */ n.size = realsize; n.sizemask = realsize-1; n.table = zcalloc(realsize*sizeof(dictEntry*)); n.used = 0; /* Is this the first initialization? If so it's not really a rehashing * we just set the first hash table so that it can accept keys. */ if (d-&gt;ht[0].table == NULL) &#123; // 如果哈希表ht[0]没有被初始化，则使用ht[0] d-&gt;ht[0] = n; return DICT_OK; &#125; /* Prepare a second hash table for incremental rehashing */ // 否则，使用ht[1]用于rehash方式扩展 d-&gt;ht[1] = n; d-&gt;rehashidx = 0; return DICT_OK;&#125;static unsigned long _dictNextPower(unsigned long size)&#123; unsigned long i = DICT_HT_INITIAL_SIZE; if (size &gt;= LONG_MAX) return LONG_MAX; while(1) &#123; if (i &gt;= size) return i; i *= 2; // 2的指数被增大 &#125;&#125;","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"redis 字符串","slug":"redis-sds","date":"2016-12-03T04:24:54.000Z","updated":"2017-04-08T09:25:55.000Z","comments":true,"path":"2016/12/03/redis-sds/","link":"","permalink":"http://simyy.com/2016/12/03/redis-sds/","excerpt":"介绍和分析redis数据类型-sds。","text":"介绍和分析redis数据类型-sds。 基本结构在redis中，字符串的基本结构如下，其中，len表示字符串的长度，alloc表示字符串的最大容量，flags表示header的类型。123456struct __attribute__ ((__packed__)) sdshdr8 &#123; uint8_t len; /* \b已占用buf长度 */ uint8_t alloc; /* 申请的buf长度 */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[];&#125;; buf表示需要存储的字符数组，数组的长度为len+1（由于需要存储一个结束符’\\0’）。 具体结构如下， 类型除了上面提到的sdshdr8，还包含sdshdr5、sdshdr16、sdshdr32、sdshdr64。 在读取字符串时，首先需要获取当字符串的存储类型，12345678910111213// 类型 - flags取值#define SDS_TYPE_5 0#define SDS_TYPE_8 1#define SDS_TYPE_16 2#define SDS_TYPE_32 3#define SDS_TYPE_64 4// 用来获取字符串内容#define SDS_TYPE_MASK 7 #define SDS_TYPE_BITS 3#define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)));#define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T))))#define SDS_TYPE_5_LEN(f) ((f)&gt;&gt;SDS_TYPE_BITS) 其中，SDS_HDR和SDS_HDR_VAR可以从sds字符串中获取header的起始位置。 函数实现sds包含很多功能，12345678sds sdsnewlen(const void *init, size_t initlen);sds sdsnew(const char *init);sds sdsempty(void);sds sdsdup(const sds s);void sdsfree(sds s);sds sdsgrowzero(sds s, size_t len);sds sdscatlen(sds s, const void *t, size_t len);... 这里只详细介绍下初始化和追加函数，1234567891011121314151617181920212223242526272829303132333435sds sdsnewlen(const void *init, size_t initlen) &#123; void *sh; sds s; char type = sdsReqType(initlen); // 通过初始化长度获取数据类型 /* Empty strings are usually created in order to append. Use type 8 * since type 5 is not good at this. */ if (type == SDS_TYPE_5 &amp;&amp; initlen == 0) type = SDS_TYPE_8; int hdrlen = sdsHdrSize(type); unsigned char *fp; /* flags pointer. */ sh = s_malloc(hdrlen+initlen+1); // 申请内存空间：header长度+buf长度+1 if (!init) memset(sh, 0, hdrlen+initlen+1); // 全部设置为0 if (sh == NULL) return NULL; s = (char*)sh+hdrlen; // 获取buf指针位置 fp = ((unsigned char*)s)-1; // 获取类型flag字段 switch(type) &#123; case SDS_TYPE_5: &#123; *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS); break; &#125; case SDS_TYPE_8: &#123; SDS_HDR_VAR(8,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; ... &#125; if (initlen &amp;&amp; init) memcpy(s, init, initlen); // 拷贝数据 s[initlen] = '\\0'; // buf最后一位置为'\\0' return s;&#125; 为sds增加可用空间，1234567891011121314151617181920212223242526272829303132333435363738394041424344sds sdsMakeRoomFor(sds s, size_t addlen) &#123; void *sh, *newsh; size_t avail = sdsavail(s); size_t len, newlen; char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; // 获取数据类型 int hdrlen; /* Return ASAP if there is enough space left. */ if (avail &gt;= addlen) return s; // 如果可用空间大于请求的长度，则不需要增加空间 len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); if (newlen &lt; SDS_MAX_PREALLOC) // 2倍的新长度 newlen *= 2; else newlen += SDS_MAX_PREALLOC; type = sdsReqType(newlen); /* Don't use type 5: the user is appending to the string and type 5 is * not able to remember empty space, so sdsMakeRoomFor() must be called * at every appending operation. */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; hdrlen = sdsHdrSize(type); if (oldtype==type) &#123; newsh = s_realloc(sh, hdrlen+newlen+1); // 如果类型保持不变，则增加原有长度 if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; &#125; else &#123; /* Since the header size changes, need to move the string forward, * and can't use realloc */ newsh = s_malloc(hdrlen+newlen+1); if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); // 如果类型发生改变，则重新申请新类型数据，并拷贝buf数据 s_free(sh); s = (char*)newsh+hdrlen; s[-1] = type; sdssetlen(s, len); &#125; sdssetalloc(s, newlen); return s;&#125; sds与普通string的区别获取字符串长度复杂度string获取字符串长度的时间复杂度为O(N)，需要遍历整个字符串；而sds不再需要遍历字符串，通过len字段可以直接获取存储在buf内字符串的长度，时间复杂度为O(1)。 空间分配和释放对于C语言来说，每一次字符串长度的增加，都会造成一次内存分配的操作；每一次字符串长度的减少，都会造成一次内存释放操作。如果redis同样需要平凡的内存分配和释放，对性能会造成严重的影响。 sds通过alloc字段来记录预分配空间的大小，len字段来记录当前存储字符串的长度。当有资源需要释放时，sds只是减少len的大小；当需要增加空间时，只有当剩余的空位不足，才会重新申请新的空间，否则只需要增加len的大小。","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"tire tree算法","slug":"simWS-tire-tree","date":"2016-11-27T08:39:58.000Z","updated":"2017-04-08T09:33:44.000Z","comments":true,"path":"2016/11/27/simWS-tire-tree/","link":"","permalink":"http://simyy.com/2016/11/27/simWS-tire-tree/","excerpt":"tire tree称为查找树或字典树，插入和查询的复杂度为O(K)（K为被插入或查询的关键字长度）。","text":"tire tree称为查找树或字典树，插入和查询的复杂度为O(K)（K为被插入或查询的关键字长度）。 原理tire tree的基本性质如下， 根节点不包含任何字符 除根节点外每一个节点都仅包含一个字符 树的每一条路径的所有节点连接起来代表一\b个被添加的字符串 tire tree的核心思想是空间换时间，并利用字符串的公共前缀来降低查询时间的开销以及节约存储空间。 如图所示，“故事”和“故乡”拥有共同的前缀“故”，所以“故”只需要存储一次，达到了节约存储空间的目的。 应用统计词频对于哈希和堆来说，tire tree的公共前缀可以有效降低内存空间，当需要统计的数据越多时，这个压缩的效果越明显。 前缀匹配由于tire tree是按照前缀匹配来存储字符串的，那么查询字符串的复杂度只是O(K)，K为字符串的长度，因此，可以用来加载分词的词库。 实现这里使用go来实现简单的tire树，完整代码：https://github.com/yxd123/simWS/blob/master/tire/tire.go。 节点的定义树的节点需要包含节点值、\b子节点指针、节点状态。 节点值：当前节点存储的值 子节点指针：当前节点的子节点的指针（\b记录了当前节点的所有子节点） 节点状态：记录当前节点是否为代表字符串的结尾（用于判断以当前节点为结尾的字符串是否被添加过） 123456type Node struct &#123; value byte //当前节点存储的值 nextNodes []*Node // 子节点指针 nextNums int //子节点个数 flag bool //是否为结尾字符&#125; 节点的添加节点的添加需要考虑几种情况： 是否为公共前缀 是否为结尾字符 values代表需要添加的字符串，place代表当前需要处理的字符位置， 1234567891011121314151617181920212223242526272829303132func (t *Node) add(values []byte, place int) &#123; // 达到字符串的末尾 if place &gt;= len(values) &#123; t.flag = true return &#125; //如果没有子节点，需要添加新节点 if t.nextNodes == nil &#123; log.Println(&quot;new nextNodes&quot;) t.nextNodes = make([]*Node, 0, 26) t.nextNums = 0 &#125; if t.nextNums &gt; 0 &#123; for _, node := range t.nextNodes &#123; // 如果子节点中包含待添加字符，则递归添加下一节点 if node.value == values[place] &#123; log.Println(&quot;duplicate byte:&quot;, string(node.value)) node.add(values, place + 1) return &#125; &#125; &#125; // 子节点无待添加字符，则添加新的子节点 node := NewNode(values[place]) node.add(values, place + 1) t.nextNodes = append(t.nextNodes, node) t.nextNums += 1 log.Println(&quot;add new Node:&quot;, string(node.value), t.nextNums)&#125; 节点的查找节点的查找和添加类似，都是从树的根节点开始，递归到目的节点。 节点的查找需要注意结尾标记。 12345678910111213141516171819202122func (t *Node) find(values []byte, place int) bool &#123; //如果没有子节点，则未查到 if t == nil || t.nextNums == 0 &#123; log.Println(&quot;can&apos;t cmp&quot;, string(values[place]), &quot;nextNums:&quot;, string(t.value), t.nextNums) return false &#125; for _, node := range t.nextNodes &#123; log.Println(&quot;cmp:&quot;, string(node.value), string(values[place])) // 找到待查找字符 if node.value == values[place] &#123; //当前待查找字符为结尾，则找到目标字符串 if place == len(values) - 1 &#123; return node.flag &#125; // 非结尾字符串，递归查找下一字符 return node.find(values, place + 1) &#125; &#125; return false&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"},{"name":"中文分词","slug":"中文分词","permalink":"http://simyy.com/tags/中文分词/"}]},{"title":"编译原理（二）语法分析","slug":"compiling-2","date":"2016-11-14T14:36:09.000Z","updated":"2017-04-08T09:35:07.000Z","comments":true,"path":"2016/11/14/compiling-2/","link":"","permalink":"http://simyy.com/2016/11/14/compiling-2/","excerpt":"语法分析是编译过程的一个逻辑阶段。语法分析的任务是在词法分析的基础上将单词序列组合成各类语法短语。","text":"语法分析是编译过程的一个逻辑阶段。语法分析的任务是在词法分析的基础上将单词序列组合成各类语法短语。 语法分析目前语法分析常用的两类方法： 自顶向下 自底向上 本文采用自顶向下的分析方法。 分析原理分析的目标是构造一个计算机可以识别的模型，在这里，使用语法树作为计算器的识别结构。 其中，节点的结构如上一篇文章中的定义（包含优先级、类型）。 操作符定义首先，需要定义所有操作符对应的方法(这里仅提供加、减、乘、除四中运算法方法)， 12345678910111213141516class Opt(object): @staticmethod def add(self, children): return children[0].value + children[1].value @staticmethod def sub(self, children): return children[0].value - children[1].value @staticmethod def mux(self, children): return children[0].value * children[1].value @staticmethod def div(self, children): return children[0].value / children[1].value 在构建语法树的时候，需要把操作符作为根节点来处理，其中加法操作对应的语法树如下, 节点定义与计算语法树的节点可以是操作符，也可以是具体的值。但需要注意，所有的值应该是叶子节点，而操作符不可能为叶子节点。 节点定义包含：节点名称、节点优先级、节点类型、当前节点值以及子节点。 需要注意的是在计算当前节点的值时是需要递归计算子树下所有节点的结果的。 1234567891011121314151617181920212223242526272829303132333435363738394041class ASTNode(object): def __init__(self, token): self.token = token self.children = list() @property def token_name(self): return self.token.name @property def priority(self): return self.token.priority @property def is_opt(self): return self.token.type == TOKENTYPE.OPT @property def is_scp(self): return self.token.type == TOKENTYPE.SCP def add_child(self, child): self.children.append(child) def pop_child(self): return self.children.pop() @property def value(self): if self.is_opt: return calcuate(self) # 递归计算得到该操作符的操作结果 else: return self.token.value def __str__(self): if not self.is_opt: return &apos;(leaf [%s])&apos; % self.token.value else: return &apos;(%s %s, %s)&apos; % (self.token_name, str(self.children[0]), str(self.children[1])) __repr__ = __str__ 解析构造语法树可以使用递归的方式来处理，具体的思想如下： 从左侧出发，遍历代码片段 判断操作符类型：值、操作符、停止符（括号） 比较当前操作符和上一次操作符的优先级，调整语法树结构：优先级小于等于上一次操作符，则把当前操作符作为根节点，原有语法树作为左子树，剩余代码片段作为待处理的右子树；优先级大于上一次操作符，则当前操作符替换上一次值的位置，上一次值作为当前操作符的做节点，剩余代码片段作为待处理的右子树。 1234567891011121314151617181920212223def parse(tokens, stop_name=None): last_opt = None for token in tokens: if stop_name and token.name == stop_name: return last_opt node = ASTNode(token) if node.is_opt: next_node = ASTNode(tokens.next()) if next_node.is_scp: next_node = parse(tokens, stop_name=TOKEN_SCP_MATCH.get(next_node.token_name)) if node.priority &lt;= last_opt.priority: node.add_child(last_opt) node.add_child(next_node) last_opt = node else: node.add_child(last_opt.pop_child()) node.add_child(next_node) last_opt.add_child(node) else: last_opt = node print last_opt return last_opt","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"编译原理","slug":"编译原理","permalink":"http://simyy.com/tags/编译原理/"}]},{"title":"编译原理（一）词法分析","slug":"compiling-1","date":"2016-10-30T04:06:20.000Z","updated":"2017-04-12T14:42:39.000Z","comments":true,"path":"2016/10/30/compiling-1/","link":"","permalink":"http://simyy.com/2016/10/30/compiling-1/","excerpt":"词法分析（Lexical analysis）是完成编译程序的第一个阶段的工作。","text":"词法分析（Lexical analysis）是完成编译程序的第一个阶段的工作。词法分析是从左到右一个字符一个字符地读入源程序，即对构成源程序的字符流进行扫描然后根据构词规则识别单词(也称单词符号或符号)。词法分析程序实现这个任务。词法分析程序可以使用lex等工具自动生成。 关键字词法分析主要任务就是需要把由代码组成的字符串来识别出来，生成可以被识别的关键字序列。 本文只实现了5种关键字类型，其中IGN是可悲忽略的字符，1234567# 关键字类型class TOKENTYPE(object): IGN = 0 # ignore OPT = 1 # oprate SCP = 1 # reserved INT = 2 # int STR = 3 # string 对于不同的关键字需要对应的正则表达式来匹配该数据，并且需要预设关键字的优先级，例如，乘号的优先级高于加号，12345678# 关键字定义class TokenRegex(object): def __init__(self, regex, name,type=TOKENTYPE.OPT, priority=1): self.regex = regex self.name = name self.type = type self.priority = priority self.value = None 关键字定义如下，123456789101112131415161718_TokenRegexs = ( TokenRegex(r&apos;\\=&apos;, &apos;eq&apos;), TokenRegex(r&apos;\\+&apos;, &apos;add&apos;), TokenRegex(r&apos;\\-&apos;, &apos;sub&apos;), TokenRegex(r&apos;\\*&apos;, &apos;mux&apos;, priority=2), TokenRegex(r&apos;\\/&apos;, &apos;div&apos;, priority=2), TokenRegex(r&apos;\\(&apos;, &apos;(&apos;, type=TOKENTYPE.SCP, priority=3), TokenRegex(r&apos;\\)&apos;, &apos;)&apos;, type=TOKENTYPE.SCP, priority=3), TokenRegex(r&apos;\\&#123;&apos;, &apos;&#123;&apos;, type=TOKENTYPE.SCP, priority=3), TokenRegex(r&apos;\\&#125;&apos;, &apos;&#125;&apos;, type=TOKENTYPE.SCP, priority=3), TokenRegex(r&apos;if&apos;, &apos;_if&apos;), TokenRegex(r&apos;else&apos;, &apos;_else&apos;), TokenRegex(r&apos;;&apos;, &apos;end&apos;), TokenRegex(r&apos;[0-9]+&apos;, &apos;int&apos;, type=TOKENTYPE.INT), TokenRegex(r&apos;[a-zA-Z][a-zA-Z0-9_]*&apos;, &apos;str&apos;, type=TOKENTYPE.STR), TokenRegex(r&apos;\\n&apos;, &apos;enter&apos;, type=TOKENTYPE.IGN), TokenRegex(r&apos; &apos;, &apos;blank&apos;, type=TOKENTYPE.IGN),) 此外，还需要定义特殊关键子匹配规则，1234TOKEN_SCP_MATCH = &#123; &apos;(&apos;: &apos;)&apos;, &apos;&#123;&apos;: &apos;&#125;&apos;,&#125; 递归遍历输入字符，生成对应的关键字链表，12345678910111213141516def lex(characters): pos = 0 length = len(characters) while pos &lt; length: for token_regex in _TokenRegexs: regex = re.compile(token_regex.regex) r = regex.match(characters, pos) if r: if token_regex.type is not TOKENTYPE.IGN: yield Token(r.group(0), token_regex) pos = r.end(0) break else: print &apos;char:&apos;, characters[pos] sys.stderr.write(&apos;Illgal character: %s\\n&apos; % characters[pos]) sys.exit(1) 关键字识别关键字识别单元，用来存储每一个关键字的信息123456789101112class Token(object): def __init__(self, value, token_regex): self.value = int(value) if value.isdigit() else value self.name = token_regex.name self.type = token_regex.type self.priority = token_regex.priority def __repr__(self): return &apos;&lt;Token(\\&apos;%s\\&apos;) %s %s&gt;&apos; % (self.name, self.value, self.priority) def is_end(self): return True if self.name == &apos;end&apos; else False","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"编译原理","slug":"编译原理","permalink":"http://simyy.com/tags/编译原理/"}]},{"title":"SimHash算法","slug":"simhash","date":"2016-10-26T17:30:43.000Z","updated":"2017-04-08T09:33:52.000Z","comments":true,"path":"2016/10/27/simhash/","link":"","permalink":"http://simyy.com/2016/10/27/simhash/","excerpt":"Google采用这种算法来解决万亿级别的网页的去重任务。","text":"Google采用这种算法来解决万亿级别的网页的去重任务。 由于实验室和互联网基本没啥关系，也就从来没有关注过数据挖掘相关的东西。在实际工作中，第一次接触到匹配和聚类等工作，虽然用一些简单的匹配算法可以做小数据的聚类，但数据量达到一定的时候就束手无策了。 所以，趁着周末把这方面的东西看了看，做个笔记。 来历google的论文“detecting near-duplicates for web crawling”——–simhash。 基本思想simhash算法的主要思想是降维，将高维的特征向量映射成一个低维的特征向量，通过两个向量的Hamming Distance来确定文章是否重复或者高度近似。 算法步骤： 对于给定的一段语句，进行分词，得到有效的特征向量 为每一个特征向量设置一个权值 对每一个特征向量计算hash值，为01组成的n-bit签名 所有特征向量进行加权（1则为正，0则为负），然后累加 对于n-bit签名的累加结果，如果&gt;0置1，否则置0 得到该语句的simhash值 根据不同语句simhash的海明距离就来判断相似程度 具体如图， 问题simhash用于比较大文本，比如500字以上效果都还蛮好，距离小于3的基本都是相似，误判率也比较低。 这样的话，小文本呢？如何解决？ 该博客给出一个思路是，将短文本抽象出有序关键字，计算此有序字串的simhash值，寻找simhash相等的集合，缩小的搜索范围。还提到了并查集和bloom filter。 参考http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity.htmlhttp://www.cnblogs.com/zhengyun_ustc/archive/2012/06/12/sim.htmlhttp://blog.jobbole.com/21928/","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"软编码存储字典数据","slug":"soft-coding","date":"2016-10-26T16:47:01.000Z","updated":"2017-04-08T09:33:40.000Z","comments":true,"path":"2016/10/27/soft-coding/","link":"","permalink":"http://simyy.com/2016/10/27/soft-coding/","excerpt":"采用非固定的字段来存储结构化的json数据。","text":"采用非固定的字段来存储结构化的json数据。 问题树形非固定字段的结构化数据是无法直接存储在mysql中的，需要把数据序列化，然后以字符串的方式存储，但是，如果把字段名称硬编码在字段内，对后期字段修改后的兼容性造成影响。 可参考：saas 插件式配置 方法把字段名称映射为字段，按照0_1_2的方式来存储，代表第三层的第二个数据（它的根节点为第二层的第一个数据），字段名称这样就完全映射为了数字，所以还需要一个字段映射表来存储所有的字段与层级的映射关系。 此时，可以通过修改字段名称来实现字段名称的变化。 实例123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# encoding: utf-8field_dict = dict()def f(d, layer): if not isinstance(d, dict): return d r = dict() i = 0 for k, v in d.items(): layer_key = &apos;%s_%d&apos; % (layer, i) r[layer_key] = f(v, layer_key) i += 1 if layer_key not in field_dict: field_dict[layer_key] = k return rd = &#123; &quot;id&quot;: 1, &quot;name&quot;: &apos;y&apos;, &apos;contact&apos;: &#123; &apos;tel&apos;: &apos;13499991111&apos;, &apos;email&apos;: &apos;y@y.com&apos;, &#125;,&#125;import jsondd = f(d, &apos;0&apos;)print ddprint json.dumps(d)print json.dumps(dd)print field_dict","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"selenium问题","slug":"spider-selenium","date":"2016-10-24T14:24:34.000Z","updated":"2017-04-08T09:24:35.000Z","comments":true,"path":"2016/10/24/spider-selenium/","link":"","permalink":"http://simyy.com/2016/10/24/spider-selenium/","excerpt":"selenium可以用来动态解析页面数据，用于爬虫、测试等工作。","text":"selenium可以用来动态解析页面数据，用于爬虫、测试等工作。 install1pip install selenium issue12from selenium import webdriverdriver = webdriver.Firefox() 报错，1selenium.common.exceptions.WebDriverException: Message: &apos;geckodriver&apos; executable needs to be in PATH. 出现的原因：selenium 3.x需要firfox的驱动，而selenium 2.x是不需要该驱动的。 下载驱动，添加到环境变量中，123# download: https://github.com/mozilla/geckodriver/releasesmv geckodriver /usr/local/bin/ 同理，在使用chrome作为浏览器时，需要下载chromedriver，12from selenium import webdriverdriver = webdriver.Chrome()","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"},{"name":"spider","slug":"spider","permalink":"http://simyy.com/tags/spider/"}]},{"title":"spring注解annotion","slug":"spring-annotion","date":"2016-10-17T15:39:45.000Z","updated":"2017-04-08T09:24:14.000Z","comments":true,"path":"2016/10/17/spring-annotion/","link":"","permalink":"http://simyy.com/2016/10/17/spring-annotion/","excerpt":"spring 注解配置和使用说明","text":"spring 注解配置和使用说明 注解配置开启spring注解功能，1&lt;context:annotation-config/&gt; 它将隐式地向 Spring 容器注册AutowiredAnnotationBeanPostProcessor、CommonAnnotationBeanPostProcessor、PersistenceAnnotationBeanPostProcessor以及RequiredAnnotationBeanPostProcessor 这 4 个 BeanPostProcessor。 开启spring mvc注解功能，1&lt;mvc:annotation-driven /&gt; 它将隐式地向Spring容器注册DefaultAnnotationHandlerMapping和AnnotationMethodHandlerAdapter。 Required@Required注解用于检查被注解属性是否已经设置，但并不会检测属性是否为空。1234@Requiredpublic void setName(name) &#123; this.name = name;&#125; 123&lt;bean id=&quot;person&quot; class=&quot;Persoon&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jack&quot; /&gt;&lt;/bean&gt; 如果bean中没有设置name，则会抛出BeanInitializationException异常。 Autowired@Autowired注解用于对类进行自动装配工作（按照类型注入）。 在默认情况下使用 @Autowired 注释进行自动注入时，Spring 容器中匹配的候选 Bean 数目必须有且仅有一个。 当找不到一个匹配的 Bean 时，Spring 容器将抛出 BeanCreationException 异常，并指出必须至少拥有一个匹配的 Bean； 当不能确定 Spring 容器中一定拥有某个类的 Bean 时，可以在需要自动注入该类 Bean 的地方可以使用 @Autowired(required = false)，这等于告诉 Spring：在找不到匹配 Bean 时也不报错； 如果 Spring 容器中拥有多个候选 Bean，Spring 容器在启动时也会抛出 BeanCreationException 异常。 如果有多个候选的bean该如何处理？ @Qualifier可以用来指定注入的bean。@Qualifier是与@Autowired结合使用，这样就会把注入类型从按名称注入转变为按按名称注入。 @Autowired可以用标记成员变量、方法以及构造函数，而Qualifier只可以用来标记成员变量、方法入参和苟赞函数入参。1234567891011121314151617181920public class Student &#123; @Autowired @Qualifier(&quot;math&quot;) // 指定数学书 private Book book; private Pen pen; @Autowired public void setPen(@Qualifier(&quot;pencil&quot;)Pen pen) &#123; this.pen = pen; &#125;&#125;public class Student &#123; private Book book; @Autowired public Student(Book book) &#123; this.book = book; &#125;&#125; Resource@Resource注解是按照名称来装配注入的，只有找不到对应的名称才会按照类型来装配注入。@Resource注解是由J2EE提供，而@Autowired是由spring提供。12345@Resource(name=&quot;math&quot;)private Book book;@Reousrce(type=Book.class)private Book book; 一般情况下，无需使用type方式（反射可以获取类型）。 其他@Service用于标注业务层组件; @Controller用于标注控制层组件（如struts中的action）; @Repository用于标注数据访问组件，即DAO组件; @Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。","categories":[{"name":"web","slug":"web","permalink":"http://simyy.com/categories/web/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://simyy.com/tags/spring/"}]},{"title":"spring bean","slug":"spring-bean","date":"2016-10-10T13:44:29.000Z","updated":"2017-04-08T09:24:07.000Z","comments":true,"path":"2016/10/10/spring-bean/","link":"","permalink":"http://simyy.com/2016/10/10/spring-bean/","excerpt":"spring bean文件解析。","text":"spring bean文件解析。 beans1234&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; xmlns是XML NameSpace的缩写，用来区分xml文件，类似java中的package。xmlns:xsi是XML Schema Instance的缩写，是指具体用到的schema资源文件里定义的元素所遵守的规范。xsi:schemaLocation是指文档遵守的xml规范，schemaLocation属性用来引用schema模式文档，解析器可以在需要的情况下使用该文档对XML实例文档校验。该值成对出现，第一个表示命名空间，的哥标示描述其命名空间的模式文档的具体问题。 bean123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- 定义第一个Bean实例：bean1 --&gt; &lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot; /&gt; &lt;!-- 定义第二个Bean实例：bean2 --&gt; &lt;bean id=&quot;bean2&quot; class=&quot;com.Bean2&quot; /&gt; &lt;/bean&gt; &lt;bean&gt;..&lt;/bean&gt;代表一个bean实例，beans由多个bean实例组成。id是bean的唯一标识符，容器对bean的管理、访问以及依赖都需要该标志。class是bean的具体实现类，默认使用new关键字创建该bean实例。 bean作用域12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- 配置一个singleton Bean实例：默认 --&gt; &lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot; /&gt; &lt;!-- 配置一个prototype Bean实例 --&gt; &lt;bean id=&quot;bean2&quot; class=&quot;com.Bean2&quot; scope=&quot;prototype&quot;/&gt;&lt;/beans&gt; spring中bean共包含五种模式，默认的作用域为单例模式，使用scope来显示的设置作用域。singleton单例模式，在每一个spring容器中，一个bean定义只有一个对象实例；prototype原型模式，允许bean被多次实例，每次调用都创建一个实例；request模式，在每一次http请求中，每个bean定义对应一个实例，该作用域在基于web的spring mvc中才有效；session模式，在一个http session中，每个bean定义对应一个实例，该作用域在基于web的spring mvc中才有效；global-session模式，在一个全局http session中，每一个bean对应一个实例，该作用阈仅在Portlet Context才有效。 init-method&amp;destroy-methodinit-method属性指定一个方法，在实例化bean时，立即调用该方法。1&lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot; init-method=&quot;init&quot;&gt; 12345public class Bean1 &#123; public void init() &#123; ... &#125;&#125; destroy-method属性制定一个方法，从容器中移除bean之后，才调用该方法。1&lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot; destroy-method=&quot;init&quot;&gt; 12345public class Bean1 &#123; public void destrory() &#123; ... &#125;&#125; init-method和destroy-method是无参函数。 此外，也可以通过实现InitializingBean和DisposableBean来实现初始化和销毁操作。1234567891011public class Bean1 implements InitializingBean &#123; public void afterPropertiesSet() &#123; ... &#125;&#125;public class Bean1 implements DisposableBean &#123; public void destroy() &#123; .... &#125;&#125; 可以通过在beans属性中设置default-init-method和default-destroy-method来批量设置bean的初始化和销毁操作。 最后，还通过使用注解的方式来配置初始化和销毁操作，开启注解功能，1&lt;context:annotation-config /&gt; 使用@PostConstruct注解初始化方法，使用@PreDestroy注解销毁方法。123456789101112131415import javax.annotation.PostConstruct;import javax.annotation.PreDestroy;public class UserDAO &#123; @PreDestroy public void destory() &#123; System.out.println(&quot;销毁方法&quot;); &#125; @PostConstruct public void init() &#123; System.out.println(&quot;初始化方法&quot;); &#125;&#125; 依赖注入依赖注入就是把代码的依赖关系使用配置的方式组织起来，同时保证代码的独立性。依赖注入包含两种：基于构造函数的依赖注入和基于设值函数的依赖注入。 基于构造函数12345678910class Bean1 &#123; public Bean2 bean2; public Bean1(Bean2 bean2) &#123; this.bean2 = bean2; &#125;&#125;class Bean2 &#123; ...&#125; 使用代码硬编码依赖关系，12Bean2 bean2 = new Bean2();Bean1 bean1 = new Bean1(bean2); 使用依赖注入，1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot;&gt; &lt;constructor-arg ref=&quot;bean2&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;bean2&quot; class=&quot;com.Bean2&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 基于设值函数12345678910111213class Bean1 &#123; private Bean2 bean2; public setBean2(Bean2 bean2) &#123; this.bean2 = bean2; &#125; public getBean2() &#123; return bean2; &#125;&#125;class Bean2 &#123; ...&#125; 使用代码硬编码依赖关系，123Bean2 bean2 = new Bean2();Bean1 bean1 = new Bean1();bean1.setBean2(bean2); 使用依赖注入，12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot;&gt; &lt;property name=&quot;bean2&quot; ref=&quot;bean2&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;bean2&quot; class=&quot;com.Bean2&quot;/&gt;&lt;/beans&gt; p-namespace使用p-namespace来简化配置，增加xmlns:p=&quot;http://www.springframework.org/schema/p&quot;1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;bean1&quot; class=&quot;com.Bean1&quot; p:bean2-ref=&quot;bean2&quot;&gt; &lt;/bean&gt; &lt;bean id=&quot;bean2&quot; class=&quot;com.Bean2&quot;/&gt;&lt;/beans&gt; 注入集合类型参数目前spring提供了list，set，map和props。123456&lt;property name=&quot;listParams&quot;&gt; &lt;list&gt; &lt;value&gt;param1&lt;/value&gt; &lt;value&gt;param2&lt;/value&gt; &lt;/list&gt;&lt;/property&gt; 123456&lt;property name=&quot;setParams&quot;&gt; &lt;set&gt; &lt;value&gt;param1&lt;/value&gt; &lt;value&gt;param2&lt;/value&gt; &lt;/set&gt;&lt;/property&gt; 123456&lt;property name=&quot;mapParams&quot;&gt; &lt;map&gt; &lt;entry key=&quot;key1&quot; value=&quot;param1&quot;/&gt; &lt;entry key=&quot;key2&quot; value=&quot;param2&quot;/&gt; &lt;/map&gt;&lt;/property&gt; 123456&lt;property name=&quot;propsParams&quot;&gt; &lt;props&gt; &lt;prop key=&quot;key1&quot;&gt;param1&lt;/prop&gt; &lt;prop key=&quot;key2&quot;&gt;param2&lt;/prop&gt; &lt;/props&gt;&lt;/property&gt;","categories":[{"name":"web","slug":"web","permalink":"http://simyy.com/categories/web/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://simyy.com/tags/spring/"}]},{"title":"thrift 入门","slug":"rpc-thrift-install","date":"2016-08-21T17:09:18.000Z","updated":"2017-04-08T09:25:33.000Z","comments":true,"path":"2016/08/22/rpc-thrift-install/","link":"","permalink":"http://simyy.com/2016/08/22/rpc-thrift-install/","excerpt":"介绍thrift基本用法。","text":"介绍thrift基本用法。 install1brew install thrift hello examplemaven123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libthrift&lt;/artifactId&gt; &lt;version&gt;0.9.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; IDL编写thrift IDL文件hello.thrift，用于生成java代码12345namespace java demoservice HelloService &#123; string hi()&#125; generate执行名声，生成java代码1thrift -gen java hello.thrift 此时，会在当前目录下生成java-gen文件夹，其中包含所需要的thrift接口文件HelloService.java. impl上步中生成了相应的HelloServie.Iface接口，所以需要实现该接口文件，1234567package demo;public class HelloImpl implements HelloService.Iface &#123; public String hi() &#123; return &quot;Hi!!!&quot;; &#125;&#125; server123456789101112131415161718192021222324252627package demo;import org.apache.thrift.server.TServer;import org.apache.thrift.server.TSimpleServer;import org.apache.thrift.transport.TServerSocket;import org.apache.thrift.transport.TServerTransport;public class Server &#123; public static void main(String[] args) &#123; HelloImpl hello = new HelloImpl(); final HelloService.Processor&lt;HelloImpl&gt; p = new HelloService.Processor&lt;HelloImpl&gt;(hello); new Thread(new Runnable() &#123; public void run() &#123; try &#123; TServerTransport st = new TServerSocket(9000); TServer tServer = new TSimpleServer(new TServer.Args(st).processor(p)); System.out.println(&quot;Starting ...&quot;); tServer.serve(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; client注意server使用的端口9000，123456789101112131415161718192021package demo;import org.apache.thrift.TException;import org.apache.thrift.protocol.TBinaryProtocol;import org.apache.thrift.protocol.TProtocol;import org.apache.thrift.transport.TSocket;import org.apache.thrift.transport.TTransport;public class Client &#123; public static void main(String[] args) throws TException &#123; TTransport ts = new TSocket(&quot;localhost&quot;, 9000); ts.open(); TProtocol p = new TBinaryProtocol(ts); HelloService.Client client = new HelloService.Client(p); System.out.println(client.hi()); ts.close(); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"rpc","slug":"rpc","permalink":"http://simyy.com/tags/rpc/"}]},{"title":"mysql自增主键","slug":"mysql-auto-increment","date":"2016-08-21T17:05:45.000Z","updated":"2017-04-08T09:28:59.000Z","comments":true,"path":"2016/08/22/mysql-auto-increment/","link":"","permalink":"http://simyy.com/2016/08/22/mysql-auto-increment/","excerpt":"介绍mysql auto increment。","text":"介绍mysql auto increment。 auto incrementmysql的自增步长可以通过下面的命令查询， 1234567mysql&gt; SHOW VARIABLES LIKE &apos;auto_inc%&apos;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| auto_increment_increment | 1 || auto_increment_offset | 1 |+--------------------------+-------+ 其中，auto_increment_increment是自增的步长，value为1代表每次+1，auto_increment_offset是自增的偏移量，也就是自增开始，value为1代表从1开始增加。 InnoDB自增主键是通过本身的自增计数器获取，该方式会通过表锁机制完成。表锁只有在插入结束后才释放，也就是事务完成后。 为了解决自增主键锁表的问题，引入了innodb_autoinc_lock_mode，通过轻量级互斥量的增长机制来完成。123456mysql&gt; show variables like &apos;innodb_autoinc_lock_mode&apos;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| innodb_autoinc_lock_mode | 1 |+--------------------------+-------+ innodb_autoinc_lock_mode的取值有三种： 0，表锁 1，默认值，互斥量，会“预申请”多余的值，可能会出现不连续的情况 2，自增值不连续，性能好 参数innodb_autoinc_lock_mode = 1时，每次会预申请多余的id(handler.cc:compute_next_insert_id)，而insert执行完成后，会特别将这些预留的id空出，就是特意将预申请后的当前最大id回写到表中 最近就发现一次一个数据表由于频繁的insert on duplicate key update导致了表的不连续，具体原因是该数据会预分配id但如果插入失败执行更新操作，那么该id就被废弃了，而下一条插入操作会跳过该值。 应用在mysql主主同步时（两台机器互相同步数据），需要设置12auto_increment_increment = 2auto_increment_offset = 1 和 2 这样才能避免两台服务器同时做更新时自增字段的值之间的冲突。 其他http://www.cnblogs.com/zhoujinyi/p/3433823.htmlhttp://blog.csdn.net/yanzongshuai/article/details/46476151","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"python 迭代器和生成器","slug":"python-iter-gen","date":"2016-08-08T16:06:45.000Z","updated":"2017-04-08T09:27:51.000Z","comments":true,"path":"2016/08/09/python-iter-gen/","link":"","permalink":"http://simyy.com/2016/08/09/python-iter-gen/","excerpt":"python迭代器和生成器简单介绍。","text":"python迭代器和生成器简单介绍。 迭代器迭代器是一个实现了迭代器协议的对象，Python中的迭代器协议就是有next方法的对象会前进到下一结果，而在一系列结果的末尾是，则会引发StopIteration。 123456789101112&gt;&gt;&gt; l = range(2)&gt;&gt;&gt; i = iter(l)&gt;&gt;&gt; i&lt;listiterator object at 0x10a38d990&gt;&gt;&gt;&gt; i.next()0&gt;&gt;&gt; i.next()1&gt;&gt;&gt; i.next()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 在for循环中，Python将自动调用工厂函数iter()获得迭代器，自动调用next()获取元素，还完成了检查StopIteration异常的工作。 123# 当发生StopIteration异常，退出循环for a in range(2): print a 常用的几个内建数据结构tuple、list、set、dict都支持迭代器，字符串也可以使用迭代操作。 你也可以自己实现一个迭代器，如上所述，只需要在类的iter方法中返回一个对象，这个对象拥有一个next()方法，这个方法能在恰当的时候抛出StopIteration异常即可。但是需要自己实现迭代器的时候不多，即使需要，使用生成器会更轻松。 1234567891011121314151617#!/usr/bin/env python# coding=utf-8class test: def __init__(self, input_list): self.list = input_list self.i = 0 def __iter__(self): return self def next(self): if self.i == len(self.list): self.i = 0 raise StopIteration self.i += 1 return self.list[self.i - 1] 使用迭代器一个显而易见的好处就是：每次只从对象中读取一条数据，不会造成内存的过大开销。 例如： 1234567/* 把文件一次加载到内存中，然后逐行打印。当文件很大时，这个方法的内存开销就很大了 */for line in open(&quot;test.txt&quot;).readlines(): print line/* 这是最简单也是运行速度最快的写法，他并没显式的读取文件，而是利用迭代器每次读取下一行 */for line in open(&quot;test.txt&quot;): #use file iterators print line 生成器生成器的编写方法和函数定义类似，只是在return的地方改为yield。 生成器中可以有多个yield。当生成器遇到一个yield时，会暂停运行生成器，返回yield后面的值。当再次调用生成器的时候，会从刚才暂停的地方继续运行，直到下一个yield。 生成器自身又构成一个迭代器，每次迭代时使用一个yield返回的值。 1234567891011121314&gt;&gt;&gt; def gen():... yield 1... yield 2...&gt;&gt;&gt; a = gen()&gt;&gt;&gt; a.next()1&gt;&gt;&gt; a.next()2&gt;&gt;&gt; a.next()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 需要注意的是，生成器中不需要return语句，不需要指定返回值，在生成器中已经存在默认的返回语句 生成器表达式123(i for i in range(5))// 返回迭代器&lt;generator object &lt;genexpr&gt; at 0x7ff3e8f0d960&gt; 列表解析，返回list123[i for i in range(5)]// 返回list[0, 1, 2, 3, 4] 在这里存在一个问题，那就是range(5)会返回一个长度为5的数据，如果是range(1000)那么就会占用一个1000大小的数组空间；如果我们采用生成器，在需要的时候产生一个数字，那么空间的占用情况就会降低，这里我们可以使用xrange()函数来实现。 12345678910111213141516xrange 函数说明：用法与range完全相同，所不同的是生成的不是一个数组，而是一个生成器。xrange示例:&gt;&gt;&gt; xrange(5)xrange(5)&gt;&gt;&gt; list(xrange(5))[0, 1, 2, 3, 4]&gt;&gt;&gt; xrange(1,5)xrange(1, 5)&gt;&gt;&gt; list(xrange(1,5))[1, 2, 3, 4]&gt;&gt;&gt; xrange(0,6,2)xrange(0, 6, 2)&gt;&gt;&gt; list(xrange(0,6,2))[0, 2, 4] 所以xrange做循环的性能比range好，尤其是返回很大的时候，尽量用xrange吧，除非你是要返回一个列表。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"mysql中的null","slug":"mysql-null","date":"2016-08-08T14:14:20.000Z","updated":"2017-04-08T09:28:54.000Z","comments":true,"path":"2016/08/08/mysql-null/","link":"","permalink":"http://simyy.com/2016/08/08/mysql-null/","excerpt":"null一般用来代表某个字段还没有被赋值，而not null被认为某字段不允许为空，那么在使用中到底该注意哪些内容？","text":"null一般用来代表某个字段还没有被赋值，而not null被认为某字段不允许为空，那么在使用中到底该注意哪些内容？ query如果需要查询字段内容为null的数据，不可以直接用expr = null来查询，1select * from test where name = null; 上面的查询是错误的，因为expr = null 永远都是false，及时expr字段被设置为null。应该使用下面的方式expr is null来过滤，1select * from test where name is null; 在GROUP BY中，null会被认为是相同的；在ORDER BY ... ASC中, null会被放在最前面。 storage在MyISAM中，null需要增加一个额外的空间来表明该字段为null，在InnoDB中，null可以使变长字段不会占用存储空间，但需要一个标志位来表明该行为null。1234567REDUNDANT* null占用1/2 bytes* 如果字段是可变长的，data parts为空* 固定长度的字段，null 2 not null 不会造成索引页的碎片COMPACT* record header包含 performance在mysql中对于MyISAM、InnoDB、MEMORY，允许使用包含null的字段作为索引。但是，在《High Performance MySQL:Optimization》中指出： Mysql难以优化引用可空列查询，它会使索引、索引统计和值更加复杂。可空列需要更多的存储空间，还需要mysql内部进行特殊处理。可空列被索引后，每条记录都需要一个额外的字节，还能导致MyISAM中固定大小的索引变成可变大小的索引。 参考http://dev.mysql.com/doc/refman/5.5/en/problems-with-null.htmlhttp://stackoverflow.com/questions/229179/null-in-mysql-performance-storagehttps://dev.mysql.com/doc/refman/5.7/en/innodb-physical-record.html","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"ZeroCopy零拷贝","slug":"linux-zero-copy","date":"2016-07-11T16:02:53.000Z","updated":"2017-04-08T09:34:43.000Z","comments":true,"path":"2016/07/12/linux-zero-copy/","link":"","permalink":"http://simyy.com/2016/07/12/linux-zero-copy/","excerpt":"本文将简单介绍一下0拷贝的原理。","text":"本文将简单介绍一下0拷贝的原理。 传统的IO服务器在对静态文件进行处理时，需要从硬盘读取静态文件，然后再通过socket返回数据。在这个过程中，数据在内核和用户态之间切换，都会进行数据复制，影响数据传输的效率。 四次内存拷贝 调用read函数会产生一次系统调用sys_read，通过DMA将数据拷贝到内核模式;cpu将内核模式数据拷贝到用户模式的buffer;read函数返回后，write函数将用户模式下buffer数据拷贝到内核模式的socket buffer中;通过DMA将数据从socket buffer中拷贝到网卡缓存中. 三次上下文的切换 read会造成一次从用户态到内核态的上下文切换;read返回会造成一次从内核态到用户态的上下文切换;write会造成一次从用户态到内核态的上下文切换;write返回造成一次从内核态到用户态的上下文切换. zero copy零拷贝是通过避免不必要的拷贝和状态切换来提高性能的。 对于传统IO来说，在user和kernel之间的数据拷贝是非必须的，所以Linux提供了一种sendfile技术来提高性能。 在Linux中，提供了sendfile系统调用来避免两次数据拷贝，应用包括nginx和lighttpd，而在JAVA中，transferTo函数通过一次系统调用，减少了原来read和send的切换，数据直接从read buffer拷贝到socket buffer中，避免了两次copy操作，应用有kafka和netty。 同时，上下文切换也降低为两次，","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://simyy.com/tags/linux/"}]},{"title":"锁与mysql中的锁","slug":"db-lock","date":"2016-07-02T07:51:08.000Z","updated":"2017-04-08T09:31:08.000Z","comments":true,"path":"2016/07/02/db-lock/","link":"","permalink":"http://simyy.com/2016/07/02/db-lock/","excerpt":"锁是并发控制主要的技术手段。","text":"锁是并发控制主要的技术手段。 悲观锁Pessimistic Concurrency Control 悲观锁是假设数据冲突的可能性很大，每次更新数据都会锁定数据，直到更新操作完成。 在对数据进行更新之前，需要对数据添加排他锁，如果成功，执行更新操作，否则，抛出异常提示执行失败，表明该数据正在被修改。更新成功后，释放排他锁，该数据可用于其他更新操作。 乐观锁Optimistic Concurrency Control 乐观锁是假设数据一般不会造成冲突，只有在数据进行提交更新的时候，才会对数据的冲突与否进行验证，如果冲突返回错误信息，待用户进一步处理。 为数据增加一个新的属性版本标识，每当读取数据时，都会把数据的版本信息同时读取出来，每当需要更新数据时，都会数据对应的版本信息与上一次读取的版本信息做对比，只有版本信息相同（数据未被更改）时，才予以更新，否则认为数据已过期。 应用由于乐观锁的版本对比特性，每次更新都需要进行版本检查，造成了开销，所以它更适合多读得场景，也就是冲突较少的情况（避免过多的版本对比的开销）；反之，写操作较多，冲突较多的场景，更适合使用悲观锁，这样不需要重复的对比开销。 memcached使用CAS（compare and set）乐观锁来解决冲突问题。 redis使用乐观锁来实现事务操作（redis中的事务是把事务中的操作加入到一个执行队列中，直到exec命令开始执行）。 传统关系型数据库中的行锁、表锁都属于悲观锁。 locking in MySQLmysql中常见的锁：行锁、表锁、页锁。 行锁row-level locking 行级锁是mysql中锁的力度最小的一种，只会当前操作的行进行加锁，但它开销大，加锁慢。 Innodb支持行级锁，但它的实现是通过对索引加锁来实现的，因此只有通过索引来检索数据才会触发行级锁。 由于innodb中的行级锁是逐步获取的，那么有可能造成死锁。 表锁table-level locking 表级锁是对当前操作的整个数据表加锁，它的实现就比较简单，开销小。 它包括：排他锁和共享锁。 InnoDB和MyISAM都支持表级锁。 页级锁page-level locking 页级锁是介于行锁和表锁粒度之间的一种锁，BDB支持，但可能造成死锁。 共享锁、排他锁、意向锁共享锁属于行锁，又称为读锁，允许其他用户读取数据，但任何事务都不可以更改数据。1select * from student lock in share mode; 排他锁属于行锁，又称为写锁，不允许其他用户的操作（包括写和读）。1select * from student for update; 意向锁属于表锁，在InooDB中会自动加意向锁。意向锁分为两种，意向共享锁和意向排他锁。执行insert/update/delete语句时，InnoDB会自动给数据添加意向排他锁，而执行select语句不会加锁。","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"jetty简介","slug":"jetty","date":"2016-06-25T07:55:41.000Z","updated":"2017-04-08T09:30:22.000Z","comments":true,"path":"2016/06/25/jetty/","link":"","permalink":"http://simyy.com/2016/06/25/jetty/","excerpt":"jetty是一个开源的servlet容器。","text":"jetty是一个开源的servlet容器。 jetty &amp; tomcat jetty构架更简单 jetty基于Handler实现，tomcat基于容器设计 jetty适合处理长连接，tomcat适合处理短连接 jetty采用NIO，tomcat采用BIO（不适合处理静态资源） exampleIDE:IntelliJ Idea 15 配置SDK12Configure -&gt; Settings -&gt; Project StructureEdit Project SDK 创建maven项目intelliJ新建一个maven项目1234Create New Project -&gt; maven选择 maven-archetype-webapp填写 GroupId和ArtifactIdnext ... 添加jetty依赖添加jetty插件：org.mortbay.jetty123456789101112131415161718&lt;build&gt; &lt;finalName&gt;demo&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt; &lt;artifactId&gt;maven-jetty-plugin&lt;/artifactId&gt; &lt;version&gt;6.1.26&lt;/version&gt; &lt;configuration&gt; &lt;connectors&gt; &lt;connector implementation=&quot;org.mortbay.jetty.nio.SelectChannelConnector&quot;&gt; &lt;port&gt;8080&lt;/port&gt; &lt;/connector&gt; &lt;/connectors&gt; &lt;scanIntervalSeconds&gt;10&lt;/scanIntervalSeconds&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 编辑启动命令123Edit ConfigurationsCommand line: jetty:run启动后，访问:127.0.0.1:8080/demo spring mvc with jetty增加spring依赖1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;$&#123;servlet.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;$&#123;jstl.version&#125;&lt;/version&gt;&lt;/dependency&gt; 对应的版本信息 123456&lt;properties&gt; &lt;spring.version&gt;4.2.5.RELEASE&lt;/spring.version&gt; &lt;spring-data.version&gt;1.2.0.RELEASE&lt;/spring-data.version&gt; &lt;servlet.version&gt;2.5&lt;/servlet.version&gt; &lt;jstl.version&gt;1.2&lt;/jstl.version&gt;&lt;/properties&gt; 配置web.xml 12345678910&lt;servlet&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 配置对应的servlet在web.xml所在目录，新建mvc-dispatcher-servlet.xml 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd&quot;&gt; &lt;!--指定controller位置--&gt; &lt;context:component-scan base-package=&quot;controller&quot;/&gt; &lt;bean id=&quot;jspViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot;/&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 创建webapp/WEB-INF/pages/index.jsp 1234567891011121314151617181920212223242526272829303132&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;title&gt;SpringMVC Demo 首页&lt;/title&gt; &lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;&gt; &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt; &lt;!-- WARNING: Respond.js doesn&apos;t work if you view the page via file:// --&gt; &lt;!--[if lt IE 9]&gt; &lt;script src=&quot;//cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;//cdn.bootcss.com/respond.js/1.4.2/respond.min.js&quot;&gt;&lt;/script&gt; &lt;![endif]--&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;SpringMVC Demo&lt;/h1&gt;&lt;h3&gt;出现此页面，说明配置成功。&lt;/h3&gt;&lt;!-- jQuery文件。务必在bootstrap.min.js 之前引入 --&gt;&lt;script src=&quot;//cdn.bootcss.com/jquery/1.11.3/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt;&lt;script src=&quot;//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 创建src/main/java/controller/IndexController.java 12345678910111213package controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;@Controllerpublic class IndexController &#123; @RequestMapping(value = &quot;/&quot;, method = RequestMethod.GET) public String index() &#123; return &quot;index&quot;; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"jetty","slug":"jetty","permalink":"http://simyy.com/tags/jetty/"}]},{"title":"tcp常见问题","slug":"tcp-term","date":"2016-05-31T03:40:22.000Z","updated":"2017-04-08T09:32:52.000Z","comments":true,"path":"2016/05/31/tcp-term/","link":"","permalink":"http://simyy.com/2016/05/31/tcp-term/","excerpt":"记录tcp中常见的问题和答案。","text":"记录tcp中常见的问题和答案。 FINFIN是终止一个方向的连接请求，需要四次握手的原因是TCP的半关闭。 TIME_WAITTIME_WAIT状态也称为2MSL等待状态，MSL为最大报文段最大生存时间(Maximum Segment Lifetime)。 需要注意，处于TIME_WAIT状态的端口是不能被使用， 一般情况下，客户端主动断开连接，处于TIME_WAIT状态，该端口无法使用，但是，如果服务器主动断开连接，那么TIME_WAIT状态的服务器端口不能被立即重用（需要等待1~4分钟）。 为什么需要TIME_WAIT？ TCP是建立在不可靠网络的可靠协议。在主动关闭一方接收到被动关闭方发送的FIN后，回应ACK并进入TIME_WAIT状态，但是由于网络的不可靠，主动关闭回应的ACK可能会产生延迟，从而触发被动一方重传FIN。极端情况下，重传的往返时间就是2MSL，因此，TIME_WAIT状态是为了提高不可靠的网络中通信的可靠性。 解决大量TIME_WAIT？ 可配置参数，具体内容google： ip_contrack tcp_tw_recycle 回收 tcp_tw_reuse 复用 tcp_max_tw_buckets 总数设置 CLOSE_WAITCLOSE_WAIT状态是被动关闭方回应ACK之后的状态，也是在发送FIN给主动关闭方之前的状态。一般来所CLOSE_WAIT的持续时间较长，至少为2小时，该端口将一直被占用。 出现CLOSE_WAIT的原因？可能是在关闭连接之前有许多数据需要发送，也就是说被动关闭方自身存在问题。 解决CLOSE_WAIT？需要在业务层面判断连接是否已被对端关闭。 三次握手和四次握手的原因TCP建立连接时，客户端主动连接，服务器被动连接，因此，可以复用第二次握手来传递SYN/ACK， 但是，TCP断开连接时，断开连接的顺序不是固定的，客户端和服务器都有可能主动断开连接（同时断开连接），所以不可以复用握手。 此外，另外一种情况也回产生四次握手： 两个应用程序同时主动打开，发送SYN，那么两端接收后会变为SYN_RCVD状态，再分别发送SYN、ACK进行确认，而且只会建立一条连接。 请求不存在端口当一个数据报到达目的端口时，如果该端口未被使用， UDP返回ICMP不可达信息 TCP使用复位 Half-Open半打开连接是指一方关闭连接或异常终止连接而另外一方却不知情的情况。只要不打算在半打开连接上传输数据，仍处于连接状态的一方就不会检测另一方的异常状态。 backlogsocket listen参数中的backlog，是指TCP呼入连接请求队列的长度。请求队列是用来存储那些三次握手连接，但并没有被应用层接受的连接，如果请求队列满了，TCP不会对SYN做出反应，从而迫使客户端TCP重传。","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"http://simyy.com/tags/tcp/"}]},{"title":"spring mvc框架","slug":"spring-mvc","date":"2016-05-18T17:14:53.000Z","updated":"2017-04-08T09:23:58.000Z","comments":true,"path":"2016/05/19/spring-mvc/","link":"","permalink":"http://simyy.com/2016/05/19/spring-mvc/","excerpt":"spring web mvc 是一种基于JAVA实现的web框架，它根据MVC思想将web进行职责解耦。","text":"spring web mvc 是一种基于JAVA实现的web框架，它根据MVC思想将web进行职责解耦。 Componetspring mvc包括以下组件，1234Dispatcher Servlet - 前端控制器Handler Mapping - 处理器映射器Controller - 页面控制器View Resolver - 试图解析器 How to use in eclipse需要提前安装：1234javaeclipsetomcatmaven new dynamic web project新建动态web工程：1File -&gt; other -&gt; web -&gt; dynamic web project 设置：项目名称、服务器版本 maven project转换为maven工程：1properties -&gt; configure -&gt; convert to maven project add dependencies利用maven来管理依赖，123456789101112pom.xml -&gt; dependenciesadd -&gt; groupId - artifactId - version------------org.springframework - spring-context - 4.2.5.RELEASEorg.springframework - spring-aop - 4.2.5.RELEASEorg.springframework - spring-webmvc - 4.2.5.RELEASEorg.springframework - spring-web - 4.2.5.RELEASEjavax.servlet - jstl - 1.2commons-logging - commons-logging - 1.1.3------------ add hello-servlet.xml&lt;context:componet-scan&gt;用来设置需要加载的控制类所在的包，&lt;bean id=&quot;viewResolver&quot; ... &gt;用来解析view并自动添加前缀和后缀 12345678910111213141516171819&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.hello&quot; /&gt; &lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.UrlBasedViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot; /&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; add web.xml&lt;servlet-name&gt;设置servlet.xml文件的名称，如下对应的应该为hello-servlet.xml，1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; version=&quot;3.0&quot;&gt; &lt;display-name&gt;SpringMVC&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;servlet&gt; &lt;servlet-name&gt;hello&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;hello&lt;/servlet-name&gt; &lt;url-pattern&gt;/welcome.jsp&lt;/url-pattern&gt; &lt;url-pattern&gt;/welcome.html&lt;/url-pattern&gt; &lt;url-pattern&gt;*.html&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; add controller class添加包com.hello.controller以及Hello控制类，定义映射关系welcome，1234567891011121314package com.hello.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView; @Controllerpublic class Hello &#123; @RequestMapping(&quot;/welcome&quot;) public ModelAndView helloWorld() &#123; String message = &quot;Hello&quot;; return new ModelAndView(&quot;welcome&quot;, &quot;message&quot;, message); &#125;&#125; 其中，ModelAndView对象会解析view并把结果返回给请求端，message是传递给view的值。那么，根据hello-servlet.xml中的设置，welcome会自动补全为/WEB-INF/jsp/welcome.jsp。 add view添加默认页面index.jsp,123456789101112131415161718192021&lt;html&gt;&lt;head&gt;&lt;title&gt;Spring MVC Tutorial Series by Crunchify.com&lt;/title&gt;&lt;style type=&quot;text/css&quot;&gt;body &#123; background-image: url(&apos;http://crunchify.com/bg.png&apos;);&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;br&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;h2&gt; This is your Spring MCV&lt;br&gt; &lt;/h2&gt; &lt;h3&gt; &lt;a href=&quot;welcome.html&quot;&gt;Click here to See Welcome Message... &lt;/a&gt; &lt;/h3&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 添加welcome.jsp1&lt;h1&gt;$&#123;message&#125;&lt;/h1&gt; maven buildmaven编译，在target目录产生编译结果12345properties -&gt; run as -&gt; maven build ... Add Goals:----clean install----","categories":[{"name":"web","slug":"web","permalink":"http://simyy.com/categories/web/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"spring","slug":"spring","permalink":"http://simyy.com/tags/spring/"}]},{"title":"maven简介","slug":"maven","date":"2016-05-17T04:25:41.000Z","updated":"2017-04-08T09:29:38.000Z","comments":true,"path":"2016/05/17/maven/","link":"","permalink":"http://simyy.com/2016/05/17/maven/","excerpt":"maven是一个项目管理和自动化工具。","text":"maven是一个项目管理和自动化工具。 project123456$&#123;basedir&#125; # 子目录，存放有pom.xml$&#123;basedir&#125;/src/main/java # 项目源码$&#123;basedir&#125;/src/main/resource # 项目资源$&#123;basedir&#125;/src/test/java # 单元测试$&#123;basedir&#125;/src/test/resource # 测试资源$&#123;basedir&#125;/target # 编译结果 Downloadsite: http://maven.apache.org/download.cgi Configure123# ~/.bash_profile 或 ~/.zhsrcexport MAVEN_HOME=/path/to/mavenexport PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin 执行mvn -v，查看是否配置成功 Eclipse plugin通过eclipse的install new software来安装:1maven-plugin: http://download.eclipse.org/technology/m2e/releases Use maven with eclipsecreate maven project1234File -&gt; Other -&gt; Maven -&gt; Maven ProjectGroup Id: com.test.app # 项目的唯一标识符，实际对应包结构Artifact Id: quickstart # 具体的项目名称Version: 0.0.1 # 项目版本 此时，会生成一个名为quickstart。 add dependencies to quickstart对该项目添加json依赖，1234pom.xml -&gt; Depndencies -&gt; add(left)Group Id: com.google.code.gsonArtifact Id: gsonVersion: 2.3.1 use gson123456public class App &#123; public printJson(String s) &#123; Gson gson = new Gson(); System.out.println(gson.toJson(s)); &#125;&#125; maven build1quickstart -&gt; properties -&gt; run as -&gt; maven build use quickstart.jar12345678import com.test.app.App;public class Test &#123; public static void main( String[] args ) &#123; App app = new App(); app.printJson(&quot;hello world&quot;); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"http://simyy.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://simyy.com/tags/java/"},{"name":"maven","slug":"maven","permalink":"http://simyy.com/tags/maven/"}]},{"title":"同步、异步、阻塞、非阻塞","slug":"block-sync","date":"2016-04-25T16:01:00.000Z","updated":"2017-04-08T09:35:18.000Z","comments":true,"path":"2016/04/26/block-sync/","link":"","permalink":"http://simyy.com/2016/04/26/block-sync/","excerpt":"简单介绍同步、异步、阻塞等问题。","text":"简单介绍同步、异步、阻塞等问题。 阻塞和非阻塞 阻塞和非阻塞的区别之处在于程序等待调用结果后的状态（等待与否）。 阻塞 blocking调用结果返回前，程序会被挂起，直到调用结果返回。 非阻塞 non blocking程序不会等待调用结果的返回，不会导致当前程序的挂起。 socket read操作：阻塞情况：内核请求从缓冲区读取数据，程序挂起，直到缓冲区内数据准备好，数据从内核拷贝到read进程中非阻塞情况：内核请求从缓冲区读取数据，如果未准备好，不等待，直接返回，重复上面的过程 同步和异步 同步和异步的区别在于消息的通信机制。 同步 synchronous发起一个调用后，在没有得到结果之前，调用不会返回，直到返回调用结果。 异步 asynchronous发起一个调用后，直接返回，而调用结果通过其他方式来通知调用者（利用触发机制来通知处理消息者）。 可以参考node.js中的异步操作","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://simyy.com/tags/linux/"}]},{"title":"MyISAM与InnoDB对比","slug":"MyISAM-InnoDB","date":"2016-04-15T08:31:49.000Z","updated":"2017-04-08T09:29:03.000Z","comments":true,"path":"2016/04/15/MyISAM-InnoDB/","link":"","permalink":"http://simyy.com/2016/04/15/MyISAM-InnoDB/","excerpt":"总结myisam与innodb的区别。","text":"总结myisam与innodb的区别。 MyISAMMyISAM使用B+tree作为索引结构，叶节点存放的是数据地址。 MyISAM不支持事务和外键。 MyISAM是表锁，对数据库写操作时会锁住整个表，效率低。 MyISAM支持全文索引。 MyISAM设计结构简单，适合read密集的表。 MyISAM支持索引压缩，可以加载更多索引。 InnoDBInnoDB同样使用B+tree作为索引结构，但是叶节点存储的是完整的数据。 InnoDB支持事务和外键，在发生故障时可以通过事务日志来回复数据库。 InnoDB是行锁，只锁定一行数据，因此写操作很快。 InnoDB不支持全文索引。 InnoDB对于write和update密集的性能更好（由于行级锁的原因）。 参考：http://stackoverflow.com/questions/15678406/when-to-use-myisam-and-innodb 区别 叶节点数据存放的不同，指针和完整数据 是否支持事务和外键 设计的区别，适合读密集还是写密集 锁的区别，表锁和行锁(如果InnoDB不能确定扫描范围，则需要对全表锁定) 是否支持全文索引 select count(*)的区别，MyISAM保存行数(如果由where则扫描全表)，而InnoDB扫描全表","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"redis总结","slug":"redis-summary","date":"2016-04-07T14:23:31.000Z","updated":"2017-04-08T09:25:44.000Z","comments":true,"path":"2016/04/07/redis-summary/","link":"","permalink":"http://simyy.com/2016/04/07/redis-summary/","excerpt":"总结redis。","text":"总结redis。 redis与memcached redis支持更多的数据结构 redis支持数据持久化 redis支持两种存储方式：snapshot(快照)和aof(append only mode)，快照是定时将内存快照持久化到硬盘（crash会丢失数据），aof是在写入数据的同时将操作命令保存到日志（不会丢失数据，但操作日志管理维护成本高） redis单线程，memcached是多线程的 redis修改libevent实现小巧的epoll，memcached完全依赖libevent（性能影响） memcached使用CAS避免资源竞争修改，redis提供了事务功能 CAS是通过为每一个cacke key设置一个隐藏的cas token作为版本号，每次set操作都会检查并更新token 数据类型stringstring是key/value的存储结构（同memcached一样），支持的命令： 12345get/set/delincr/incrbydecr/descbyappend/strlengetrange/setrange 问题：计数 一般情况都需要额外使用锁来避免并发写的问题。 解决方法： 使用incr命令来实现原子递增，使用get/set来重置计数状态。 与memcached相同，可作为key/value数据库，例如session共享。 hashhash的value实际上是一个hashmap，可以直接操作value的各个feild的值，支持的命令： 1234hset/hget/hgetallhmset/hmgethlen/hexistshkeys/hvals hash两种实现方式， 当数据较少时，为了节省内存采用线性存储来节省空间 当数据较多时，才会使用hashmap来降低时间复杂度 问题：商品维度计数商品有各种计数（喜欢、评论、鉴定、浏览等）解决方法：使用hash的field字段来存储喜欢、评论等计数值。 同理，用户维度计数（动态、关注、粉丝、喜欢、发帖等）。 该存储结构适用于常用的商品、热门新闻动态等经常被大量访问的数据对象。 listlist通过双向链表实现，支持反向查找和遍历，支持的命令： 123lpush/rpushlpop/rpoplrange/lrem 利用push和pop操作可以实现消息队列，也可以实现关注列表、粉丝/在线好友列表等功能。 问题：显示最新的项目列表 1SELECT * FROM foo WHERE ... ORDER BY TIME DESC LIMIT 10; 数据库上的查询语句如上，随着数据的增多，排序会越来越慢。解决方法： 为每一条数据选择一个唯一的ID（可选自增ID） 缓存最新的N个数据，利用redis做数据缓存（list结构不断LPUSH最新的数据） 当查询内容超过redis缓存内容后，才会穿透缓存访问数据库 消息队列的实现： 用list数据结构作为channel 生产者lpush消息 消费者rpop消息 设定超时时间 上述消息队列与PUB/SUB相比，不会丢失数据，但也没有失败重传的机制（也就是没有消息状态） setset是一个不允许重复的数据组合，支持的命令： 123sadd/spopsismemberssinter/sunion/sdiff smembers可以用来实现好友列表中判断是否为已存在好友或关注好友，也可以通过集合操作来实现共同好友、共同兴趣、共同关注列表等。 问题：判断微博的共同好友、共同兴趣、是否为关注好友解决方法： 利用set集合存储用户的好友ID 通过sismembers判断用户是否存在好友集合中 通过sinter来判断两个好友集合的共同好友 通过sunion来获取两个集合的所有好友 通过sdiff来获取两个集合的非共同好友，用于好友推荐 sorted setsorted set是有序的set，通过提供一个优先级score来实现自动排序，支持的命令：12zadd/zremzrange/zcard score可以用来实现权重队列，也可以实现按时间、评分的自动排序列表。 问题：在线游戏实时排行榜在线游戏的排行榜都需要实时更新操作，不可能去频繁地更改关系型数据库，解决方法： 为用户设置唯一的ID，采用sorted set来存储用户的得分情况 利用有序集合来对用户得分进行排名，这里不可能存储全部的用户，可能只关心前100个用户，那么就需要在redis之外做限制 问题：新闻排序新闻排序是按照新闻的关注度（点击率）和时间做排序，1score = points / (time^alpha) 上面的公式可以说明，点击率越高，越可能获得更多的评分；时间越久远，也会降低新闻的评分。因此，需要有一个专门用于计算新闻评分的进程，实时地处理最新的N条数据解决方法： 拉取最新的N条数据 计算各个数据的评分 添加到有序集合中 Pub/Sub实现消息队列、实时消息系统。 Transactionsredis提供事务的支持， 事务可以一次执行多个命令，多个命令按照顺序执行，不会被其他客户单的命令所打断 事务是原子操作，要么全部执行，要么不执行 使用方法：123&gt; MULTI&gt; ...&gt; EXEC 错误处理： 在EXEC之前产生的错误，redis会自动放弃这个事务 在EXEC之后产生的错误，并没有特殊处理 其他应用 实时统计，而全量操作数据记录到log日志，利用Hadoop进行更全面的离线分析 web缓存，减轻数据库压力","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"Btree 索引","slug":"btree","date":"2016-03-22T15:55:34.000Z","updated":"2017-04-08T09:35:13.000Z","comments":true,"path":"2016/03/22/btree/","link":"","permalink":"http://simyy.com/2016/03/22/btree/","excerpt":"索引是帮助数据库高效获取数据的一种数据结构，通过提取句子主干，就可以得到索引的本质。","text":"索引是帮助数据库高效获取数据的一种数据结构，通过提取句子主干，就可以得到索引的本质。 m-way查找树如果想了解Btree，需要首先了解m-way数据结构。 m-way查找树是是一种树形的存储结构，主要特点如下， 每个节点存储的key数量小于m个 每个节点的度小于等于m 节点key按顺序排序 子树key值要完全小于、大于或介于父节点之间 例如，3-way如图，m为3，那么每个节点最多拥有为2个（m-1），12待索引元素列表为：[5, 7, 12, 6, 8, 3, 4] Btree查找树Btree是一种平衡的m-way查找树，它可以利用多个分支节点（子树节点）来减少查询数据时所经历的节点数，从而达到节省存取时间的目的。 主要特点如下， 每个节点的key数量小于m个（与m-way相同） 除根节点和叶子节点的其他节点存储key的个数必须大于等于m/2 所有叶子节点都处于同一层，也就是说所有叶节点具有相同的深度h（树的高度，也意味着树是平衡的） Btree的查找必须从根节点开始采用二分法查找，所以时间复杂度为O（logn）。 Btree的插入为了保证树的平衡，如果带插入节点的key数量小于m-1个，则直接插入（不会违反第一条特性），否则，需要将该节点分为两部分，再执行该操作。 详细插入操作可参考:http://www.cnblogs.com/yangecnu/p/introduce-b-tree-and-b-plus-tree.html B+tree查找树B+tree是基于Btree的变体，与Btree不同之处在于， 非叶子节点的key个数等于m 每个节点的下级指针为n个（n为关键字个数，而不是n+1个） 为所有叶子节点增加一个链指针（注意链上的数据是有序的） 所有key都存在叶子节点中 为什么使用Btree结构索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。（换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。） 为了达到降低磁盘I/O的目的 磁盘按需读取，要求每次都会预读的长度一般为页的整数倍， 数据库系统将一个节点的大小设为等于一个页，这样每个节点的元素数据只需要一次I/O就可以完全载入 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O 把B-tree中的m值设的非常大，就会让树的高度降低，有利于一次完全载入 红黑树 红黑树BST的时间复杂度是依赖于树的高度，但是，红黑树的高度与Btree相比，高度更大。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"TCP状态","slug":"tcp-status","date":"2016-01-18T13:15:43.000Z","updated":"2017-04-08T09:33:02.000Z","comments":true,"path":"2016/01/18/tcp-status/","link":"","permalink":"http://simyy.com/2016/01/18/tcp-status/","excerpt":"介绍TCP状态。","text":"介绍TCP状态。 TCP状态图 TCP连接中包含不同的状态，如何通过状态来判断程序问题尤为重要。 三次握手图中的connection部分为三次握手。 四次握手图中的close部分为四次握手。 CLOSE_WAIT服务器接受到FIN关闭请求后为CLOSE_WAIT状态。默认情况下，CLOSE_WAIT状态会至少维持2小时的状态。由于CLOSE_WAIT状态后应该发送ACK请求，状态会变为LAST_ACK，但如果由太多的CLOSE_WAIT状态，则服务器出现错误。 TIME_WAIT客户端在接受到服务器的FIN关闭请求后为TIME_WAIT状态，最后会自动切换到CLOSE状态。当服务器主动关闭链接时形成TIME_WAIT状态，会持续2个MSL(Max Segment Lifetime)，默认为4分钟。 TIME_WAIT状态下的socket不能被回收使用，具体现象是对于一个处理大量短连接的服务器，如果由于服务器主动关闭客户端的连接，将导致服务器产生大量处于TIME_WAIT状态的socket，严重影响服务器的处理能力。 AWK和NETSTAT命令12345678910111213# -n表示-netnetstat -n # -v用于定义参数 netstat -net | awk -v i=0,j=0 &apos;&#123;if ($6==&quot;ESTABLISHED&quot;) &#123;i++&#125;; if ($6 == &quot;CLOSE_WAIT&quot;) &#123;j++&#125;; print i, j, prit $6&#125;&apos;# 最佳用法如下:# 1. 使用正则/xxx/来匹配行# 2. $NF代表最后一列的数据，命令中定义了一个字典，最后一列数据作为key# 3. 每次匹配都会让value递增# 4. END表示结束# 5. 最后的&#123;&#125;用来打印key,valuenetstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for (a in S) print a, S[a]&#125;&apos; 那么，如何对结果进行排序呢？1234# 利用管道netstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for (a in S) print a, S[a] | &quot;sort -r -n -k2&quot;&#125;&apos;# 其中，sort为外部命令，-r代表从大到小（倒序），-n代表按照数字排序，-k2代表按照第2行排序。 此外，也可以使用asort来实现结果排序。","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"http://simyy.com/tags/tcp/"}]},{"title":"python-单元测试","slug":"python-单元测试","date":"2015-12-31T08:56:17.000Z","updated":"2017-04-08T09:28:10.000Z","comments":true,"path":"2015/12/31/python-单元测试/","link":"","permalink":"http://simyy.com/2015/12/31/python-单元测试/","excerpt":"The Hacker’s Guide To Python 单元测试","text":"The Hacker’s Guide To Python 单元测试 基本方式python中提供了非常简单的单元测试方式，利用nose包中的nosetests命令可以实现简单的批量测试。 安装nose包1sudo pip install nose 编辑测试文件123456# test_true.pydef test_true(): assert Truedef test_false(): assert False 执行测试1234567891011121314151617181920# 命令， nosetests命令会加载所有以test_开头的文件，并执行所有以test_开头的函数nosetests -v# 输出test_true.test_true ... oktest_true.test_false ... FAIL======================================================================FAIL: test_true.test_false----------------------------------------------------------------------Traceback (most recent call last): File &quot;/usr/local/lib/python2.7/site-packages/nose/case.py&quot;, line 197, in runTest self.test(*self.arg) File &quot;/xxxx/workspace/py/test/test_true.py&quot;, line 5, in test_false assert FalseAssertionError----------------------------------------------------------------------Ran 2 tests in 0.007sFAILED (failures=1 unittest是python提供了单元测试的标准库。 1234567891011# 为了兼容python 2.6和2.7try: import unittest2 as unittestexcept ImportError: import unittestclass TestKey(unittest.TestCase): def test_keyh(self): a = [&apos;a&apos;] b = [&apos;a&apos;, &apos;b&apos;] self.assertEqual(a, b) 输出如下，123456789101112131415161718192021test_keyh (test_true.TestKey) ... FAIL======================================================================FAIL: test_keyh (test_true.TestKey)----------------------------------------------------------------------Traceback (most recent call last): File &quot;/home/y/workspace/py/test/test_true.py&quot;, line 8, in test_keyh self.assertEqual(a, b)AssertionError: Lists differ: [&apos;a&apos;] != [&apos;a&apos;, &apos;b&apos;]Second list contains 1 additional elements.First extra element 1:b- [&apos;a&apos;]+ [&apos;a&apos;, &apos;b&apos;]----------------------------------------------------------------------Ran 1 test in 0.006sFAILED (failures=1) 此外，unittest.skipIf可以通过判断条件来选择是否进行测试，12345678class TestSkipped(unittest.TestCase): @unitttest.skip(&quot;Do not run this&quot;) def test_failt(self): self.fail(&quot;This should not be run&quot;) @unittest.skipIf(mylib is None, &quot;mylib is not available&quot;) def test_mylib(self): self.assertEqual(1, 1) 此外，自定义setUp和tearDown函数可以单元测试开始和结束时自动调用。 fixturesfixtures模块可以用来临时改变当前的测试环境。 1234567891011import fixturesimport osclass TestEnviron(fixtures.TestWithFixtures): def test_environ(self): fixture = self.useFixture( fixtures.EnvironmentVariable(&quot;FOOBAR&quot;, &quot;42&quot;)) # 临时增加一个环境变量FOOBAR self.assertEqual(os.environ.get(&quot;FOOBAR&quot;), &quot;42&quot;) def test_environ_no_fixture(self): self.assertEqual(os.environ.get(&quot;FOOBAR&quot;), None) # 上面增加的环境变量的操作对于其他函数无效 mockmock模块可以用来进行模拟测试，其主要功能就是模拟一个函数，类或实例的行为。 由于网络测试环境的特殊性，最常用的使用就是模拟网络请求，具体例子如下， 1234567891011121314151617181920212223242526272829303132333435363738394041# test_mock.pyimport requestsimport unittestimport mockclass WhereIsPythonError(Exception): passdef is_python(): try: r = requests.get(&quot;http://python.org&quot;) except IOError: pass else: if r.status_code == 200: return &apos;is python&apos; in r.content raise WhereIsPythonError(&apos;something happened&apos;)def get_fake_get(status_code, content): m = mock.Mock() m.status_code = status_code m.content = content def fake_get(url): return m return fake_getdef raise_get(url): raise IOError(&quot;unable to fetch url %s&quot; % url)class TestPython(unittest.TestCase): @mock.patch(&apos;requests.get&apos;, get_fake_get( 200, &apos;is python, hello&apos; )) def test_python_is(self): self.assertTrue(is_python()) @mock.patch(&apos;requests.get&apos;, get_fake_get( 200, &apos;is not python, hello&apos; )) def test_python_is_not(self): self.assertFalse(is_python()) 输出如下，12345678910# 命令nosetests --tests=test_mock -v# 结果test_python_is (test_mock.TestPython) ... oktest_python_is_not (test_mock.TestPython) ... ok----------------------------------------------------------------------Ran 2 tests in 0.001sOK","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python yield关键字","slug":"python-yield","date":"2015-11-24T03:05:40.000Z","updated":"2017-04-08T09:26:39.000Z","comments":true,"path":"2015/11/24/python-yield/","link":"","permalink":"http://simyy.com/2015/11/24/python-yield/","excerpt":"yield的功能类似于return，但是不同之处在于它返回的是生成器。","text":"yield的功能类似于return，但是不同之处在于它返回的是生成器。 生成器生成器是通过一个或多个yield表达式构成的函数，每一个生成器都是一个迭代器（但是迭代器不一定是生成器）。 如果一个函数包含yield关键字，这个函数就会变为一个生成器。 生成器并不会一次返回所有结果，而是每次遇到yield关键字后返回相应结果，并保留函数当前的运行状态，等待下一次的调用。 由于生成器也是一个迭代器，那么它就应该支持next方法来获取下一个值。 基本操作1234567# 通过`yield`来创建生成器def func(): for i in xrange(10); yield i# 通过列表来创建生成器[i for i in xrange(10)] 12345678910111213141516# 调用如下&gt;&gt;&gt; f = func()&gt;&gt;&gt; f # 此时生成器还没有运行&lt;generator object func at 0x7fe01a853820&gt;&gt;&gt;&gt; f.next() # 当i=0时，遇到yield关键字，直接返回0&gt;&gt;&gt; f.next() # 继续上一次执行的位置，进入下一层循环1...&gt;&gt;&gt; f.next()9&gt;&gt;&gt; f.next() # 当执行完最后一次循环后，结束yield语句，生成StopIteration异常Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 除了next函数，生成器还支持send函数。该函数可以向生成器传递参数。 12345678910111213&gt;&gt;&gt; def func():... n = 0... while 1:... n = yield n #可以通过send函数向n赋值... &gt;&gt;&gt; f = func()&gt;&gt;&gt; f.next() # 默认情况下n为00&gt;&gt;&gt; f.send(1) #n赋值11&gt;&gt;&gt; f.send(2)2&gt;&gt;&gt; 应用最经典的例子，生成无限序列。 常规的解决方法是，生成一个满足要求的很大的列表，这个列表需要保存在内存中，很明显内存限制了这个问题。 1234def get_primes(start): for element in magical_infinite_range(start): if is_prime(element): return element 如果使用生成器就不需要返回整个列表，每次都只是返回一个数据，避免了内存的限制问题。 12345def get_primes(number): while True: if is_prime(number): yield number number += 1 生成器源码分析生成器的源码在Objects/genobject.c。 调用栈在解释生成器之前，需要讲解一下Python虚拟机的调用原理。 Python虚拟机有一个栈帧的调用栈，其中栈帧的是PyFrameObject，位于Include/frameobject.h。 1234567891011121314151617181920212223242526272829303132333435typedef struct _frame &#123; PyObject_VAR_HEAD struct _frame *f_back; /* previous frame, or NULL */ PyCodeObject *f_code; /* code segment */ PyObject *f_builtins; /* builtin symbol table (PyDictObject) */ PyObject *f_globals; /* global symbol table (PyDictObject) */ PyObject *f_locals; /* local symbol table (any mapping) */ PyObject **f_valuestack; /* points after the last local */ /* Next free slot in f_valuestack. Frame creation sets to f_valuestack. Frame evaluation usually NULLs it, but a frame that yields sets it to the current stack top. */ PyObject **f_stacktop; PyObject *f_trace; /* Trace function */ /* If an exception is raised in this frame, the next three are used to * record the exception info (if any) originally in the thread state. See * comments before set_exc_info() -- it&apos;s not obvious. * Invariant: if _type is NULL, then so are _value and _traceback. * Desired invariant: all three are NULL, or all three are non-NULL. That * one isn&apos;t currently true, but &quot;should be&quot;. */ PyObject *f_exc_type, *f_exc_value, *f_exc_traceback; PyThreadState *f_tstate; int f_lasti; /* Last instruction if called */ /* Call PyFrame_GetLineNumber() instead of reading this field directly. As of 2.3 f_lineno is only valid when tracing is active (i.e. when f_trace is set). At other times we use PyCode_Addr2Line to calculate the line from the current bytecode index. */ int f_lineno; /* Current line number */ int f_iblock; /* index in f_blockstack */ PyTryBlock f_blockstack[CO_MAXBLOCKS]; /* for try and loop blocks */ PyObject *f_localsplus[1]; /* locals+stack, dynamically sized */&#125; PyFrameObject; 栈帧保存了给出代码的的信息和上下文，其中包含最后执行的指令，全局和局部命名空间，异常状态等信息。f_valueblock保存了数据，b_blockstack保存了异常和循环控制方法。 举一个例子来说明， 1234567def foo(): x = 1 def bar(y): z = y + 2 # &lt;--- (3) ... and the interpreter is here. return z return bar(x) # &lt;--- (2) ... which is returning a call to bar ...foo() # &lt;--- (1) We&apos;re in the middle of a call to foo ... 那么，相应的调用栈如下，一个py文件，一个类，一个函数都是一个代码块，对应者一个Frame，保存着上下文环境以及字节码指令。 12345678910c ---------------------------a | bar Frame | -&gt; block stack: []l | (newest) | -&gt; data stack: [1, 2]l --------------------------- | foo Frame | -&gt; block stack: []s | | -&gt; data stack: [&lt;Function foo.&lt;locals&gt;.bar at 0x10d389680&gt;, 1]t ---------------------------a | main (module) Frame | -&gt; block stack: []c | (oldest) | -&gt; data stack: [&lt;Function foo at 0x10d3540e0&gt;]k --------------------------- 每一个栈帧都拥有自己的数据栈和block栈，独立的数据栈和block栈使得解释器可以中断和恢复栈帧（生成器正式利用这点）。 Python代码首先被编译为字节码，再由Python虚拟机来执行。一般来说，一条Python语句对应着多条字节码（由于每条字节码对应着一条C语句，而不是一个机器指令，所以不能按照字节码的数量来判断代码性能）。 调用dis模块可以分析字节码， 123456789101112131415from dis import disdis(foo) 5 0 LOAD_CONST 1 (1) # 加载常量1 3 STORE_FAST 0 (x) # x赋值为1 6 6 LOAD_CONST 2 (&lt;code object bar at 0x7f3cdee3a030, file &quot;t1.py&quot;, line 6&gt;) # 加载常量2 9 MAKE_FUNCTION 0 # 创建函数 12 STORE_FAST 1 (bar) 9 15 LOAD_FAST 1 (bar) 18 LOAD_FAST 0 (x) 21 CALL_FUNCTION 1 # 调用函数 24 RETURN_VALUE 其中， 12345第一行为代码行号；第二行为偏移地址；第三行为字节码指令；第四行为指令参数；第五行为参数解释。 生成器源码分析由了上面对于调用栈的理解，就可以很容易的明白生成器的具体实现。 生成器的源码位于object/genobject.c。 生成器的创建12345678910111213141516PyObject *PyGen_New(PyFrameObject *f)&#123; PyGenObject *gen = PyObject_GC_New(PyGenObject, &amp;PyGen_Type); # 创建生成器对象 if (gen == NULL) &#123; Py_DECREF(f); return NULL; &#125; gen-&gt;gi_frame = f; # 赋予代码块 Py_INCREF(f-&gt;f_code); # 引用计数+1 gen-&gt;gi_code = (PyObject *)(f-&gt;f_code); gen-&gt;gi_running = 0; # 0表示为执行，也就是生成器的初始状态 gen-&gt;gi_weakreflist = NULL; _PyObject_GC_TRACK(gen); # GC跟踪 return (PyObject *)gen;&#125; send与nextnext与send函数，如下 123456789101112static PyObject *gen_iternext(PyGenObject *gen)&#123; return gen_send_ex(gen, NULL, 0);&#125;static PyObject *gen_send(PyGenObject *gen, PyObject *arg)&#123; return gen_send_ex(gen, arg, 0);&#125; 从上面的代码中可以看到，send和next都是调用的同一函数gen_send_ex，区别在于是否带有参数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970static PyObject *gen_send_ex(PyGenObject *gen, PyObject *arg, int exc)&#123; PyThreadState *tstate = PyThreadState_GET(); PyFrameObject *f = gen-&gt;gi_frame; PyObject *result; if (gen-&gt;gi_running) &#123; # 判断生成器是否已经运行 PyErr_SetString(PyExc_ValueError, &quot;generator already executing&quot;); return NULL; &#125; if (f==NULL || f-&gt;f_stacktop == NULL) &#123; # 如果代码块为空或调用栈为空，则抛出StopIteration异常 /* Only set exception if called from send() */ if (arg &amp;&amp; !exc) PyErr_SetNone(PyExc_StopIteration); return NULL; &#125; if (f-&gt;f_lasti == -1) &#123; # f_lasti=1 代表首次执行 if (arg &amp;&amp; arg != Py_None) &#123; # 首次执行不允许带有参数 PyErr_SetString(PyExc_TypeError, &quot;can&apos;t send non-None value to a &quot; &quot;just-started generator&quot;); return NULL; &#125; &#125; else &#123; /* Push arg onto the frame&apos;s value stack */ result = arg ? arg : Py_None; Py_INCREF(result); # 该参数引用计数+1 *(f-&gt;f_stacktop++) = result; # 参数压栈 &#125; /* Generators always return to their most recent caller, not * necessarily their creator. */ f-&gt;f_tstate = tstate; Py_XINCREF(tstate-&gt;frame); assert(f-&gt;f_back == NULL); f-&gt;f_back = tstate-&gt;frame; gen-&gt;gi_running = 1; # 修改生成器执行状态 result = PyEval_EvalFrameEx(f, exc); # 执行字节码 gen-&gt;gi_running = 0; # 恢复为未执行状态 /* Don&apos;t keep the reference to f_back any longer than necessary. It * may keep a chain of frames alive or it could create a reference * cycle. */ assert(f-&gt;f_back == tstate-&gt;frame); Py_CLEAR(f-&gt;f_back); /* Clear the borrowed reference to the thread state */ f-&gt;f_tstate = NULL; /* If the generator just returned (as opposed to yielding), signal * that the generator is exhausted. */ if (result == Py_None &amp;&amp; f-&gt;f_stacktop == NULL) &#123; Py_DECREF(result); result = NULL; /* Set exception if not called by gen_iternext() */ if (arg) PyErr_SetNone(PyExc_StopIteration); &#125; if (!result || f-&gt;f_stacktop == NULL) &#123; /* generator can&apos;t be rerun, so release the frame */ Py_DECREF(f); gen-&gt;gi_frame = NULL; &#125; return result;&#125; 字节码的执行PyEval_EvalFrameEx函数的功能为执行字节码并返回结果。 123456789101112131415161718# 主要流程如下，for (;;) &#123; switch(opcode) &#123; # opcode为操作码，对应着各种操作 case NOP: goto fast_next_opcode; ... ... case YIELD_VALUE: # 如果操作码是yield retval = POP(); f-&gt;f_stacktop = stack_pointer; why = WHY_YIELD; goto fast_yield; # 利用goto跳出循环 &#125;&#125;fast_yield: ... return vetval; # 返回结果 举一个例子，f_back上一个Frame，f_lasti上一次执行的指令的偏移量，12345678910111213141516171819import sysfrom dis import disdef func(): f = sys._getframe(0) print f.f_lasti print f.f_back yield 1 print f.f_lasti print f.f_back yield 2a = func()dis(func)a.next()a.next() 结果如下，其中第三行的英文为操作码，对应着上面的opcode，每次switch都是在不同的opcode之间进行选择。123456789101112131415161718192021222324252627282930313233343536373839 6 0 LOAD_GLOBAL 0 (sys) 3 LOAD_ATTR 1 (_getframe) 6 LOAD_CONST 1 (0) 9 CALL_FUNCTION 1 12 STORE_FAST 0 (f) 7 15 LOAD_FAST 0 (f) 18 LOAD_ATTR 2 (f_lasti) 21 PRINT_ITEM 22 PRINT_NEWLINE 8 23 LOAD_FAST 0 (f) 26 LOAD_ATTR 3 (f_back) 29 PRINT_ITEM 30 PRINT_NEWLINE 9 31 LOAD_CONST 2 (1) 34 YIELD_VALUE # 此时操作码为YIELD_VALUE，直接跳转上述goto语句，此时f_lasti为当前指令，f_back为当前frame 35 POP_TOP 11 36 LOAD_FAST 0 (f) 39 LOAD_ATTR 2 (f_lasti) 42 PRINT_ITEM 43 PRINT_NEWLINE 12 44 LOAD_FAST 0 (f) 47 LOAD_ATTR 3 (f_back) 50 PRINT_ITEM 51 PRINT_NEWLINE 13 52 LOAD_CONST 3 (2) 55 YIELD_VALUE 56 POP_TOP 57 LOAD_CONST 0 (None) 60 RETURN_VALUE 18&lt;frame object at 0x7fa75fcebc20&gt; #和下面的frame相同，属于同一个frame，也就是说在同一个函数（命名空间）内，frame是同一个。39&lt;frame object at 0x7fa75fcebc20&gt;","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"mysql存储过程","slug":"mysql-procedure","date":"2015-10-28T13:57:02.000Z","updated":"2017-04-08T09:28:48.000Z","comments":true,"path":"2015/10/28/mysql-procedure/","link":"","permalink":"http://simyy.com/2015/10/28/mysql-procedure/","excerpt":"存储过程是通过给定的语法格式编写自定义的数据库API，类似于给数据库编写可执行函数。","text":"存储过程是通过给定的语法格式编写自定义的数据库API，类似于给数据库编写可执行函数。 简介存储过程是一组为了完成特定功能的SQL语句集合，是经过编译后存储在数据库中。 存储过程增强了SQL语言的功能和灵活性，它可以使用流控制语句编写来完成复杂的判断和计算。 存储过程是把完成特定功能的SQL语句集合统一在数据库中进行处理，避免了多次网络IO请求造成的网络负载。 123456mysql&gt; DELIMITER //mysql&gt; CREATE PROCEDURE proc1(OUT s int) -&gt; BEGIN -&gt; SELECT COUNT(*) INTO s FROM person; -&gt; END //mysql&gt; DELIMITER; 基本语法基本结构 123456789101112DELIMITER //DROP PROCEDURE IF EXISTS some_func;CREATE PROCEDURE some_func( IN param1 INT, IN param2 VARCHAR(32), OUT res INT)BEGIN SQL-SCRIPTEND //DELIMITER; 调用语句如下， 1CALL some_func(params...); 变量 在存储过程中，函数参数包含三种变量， IN 输入参数，必须在调用存储过程时指定 OUT 输出参数，可在存储过程内部被改变，返回该结果 INOUT 输入输出参数，调用时指定并且可被改变和返回 在存储过程内部，参数的定义如下， 1DECLARE var_name [, var_name] var_type [default value]; 用户变量 1SET @mvar = 'Hello World'; 变量赋值 1SET var_name = 表达式; 条件语句 12345if var=0 then insert into person values ('f');else insert into person values ('ff');end if; case语句 12345678case varwhen 0 then insert into person values ('f');when 1 then insert into person values ('ff');else insert into person values ('fff');end case; 循环语句 WHILE语句，同C语言中的while语句一样。 1234while var &lt; N do insert into person values ('f'); set var = var + 1;end while; REPEAT语句，类似于C语言中的do while语句。 12345repeat insert into person values ('f'); set var = var + 1;until var &gt;= Nend repeat; LOOP语句，没有结束的判断语句，利用leave来跳出循环，类似于break。 12345678set @var = 0;loop_name:loop set @var = @var + 1; if @var &gt; 5 then leave loop_name; end if;end loop loop_name;select @var; 基本函数mysql内置了一些函数，这些函数可以极大地提高编写存储过程的效率。 字符串操作如下， 12345678910111213CHARSET(str) //获取字符集CONCAT(str1, str2, ...) //联接字符串INSTR(str, substr) //返回substr出现在str中的第一个位置LOCATE(substr, str, start_position) //返回substr在str的start_position开始第一次出现的位置LCASE(str) //将所有字符转换为小写LEFT(str, length) //返回str从左边开始的length个字符LENGTH(str) //返回str长度LOAD_FILE(file_name) //读取文见内容LPAD(str, length, pad) //重复在str的首部插入pad，直到str的长度达到lengthLTRIM(str) //去除str首部的空格RTRIM(str) //去除str尾部的空格STRCMP(str1, str2) //字符串比较SUBSTRING(str, start_position, length) //截取字符串，默认第一个字符下标为1 math相关操作如下， 12345678910111213ABS(i) //绝对值BIN(i) //十进制-&gt;二进制CEILING(i) //向上取整CONV(i, from, to) //进制转换FLOOR(i) //向下取整FORMAT(i, n) //保留小数位数HEX(i) //转十六进制LEAST(i0, i1, i2, ..) //求最小值MOD(i, demoninator) //求余POWER(I, POWER) //求指数RAND([seed]) //随机数ROUND(I, [, DECIMALS]) //四舍五入，decimals为保留小数位数SQRT(number2) //开平方 时间相关操作如下， 12345678910111213141516171819202122232425262728293031ADDTIME (date2 ,time_interval ) //将time_interval加到date2 CONVERT_TZ (datetime2 ,fromTZ ,toTZ ) //转换时区 CURRENT_DATE ( ) //当前日期 CURRENT_TIME ( ) //当前时间 CURRENT_TIMESTAMP ( ) //当前时间戳 DATE (datetime ) //返回datetime的日期部分 DATE_ADD (date2 , INTERVAL d_value d_type ) //在date2中加上日期或时间 DATE_FORMAT (datetime ,FormatCodes ) //使用formatcodes格式显示datetime DATE_SUB (date2 , INTERVAL d_value d_type ) //在date2上减去一个时间 DATEDIFF (date1 ,date2 ) //两个日期差 DAY (date ) //返回日期的天 DAYNAME (date ) //英文星期 DAYOFWEEK (date ) //星期(1-7) ,1为星期天 DAYOFYEAR (date ) //一年中的第几天 EXTRACT (interval_name FROM date ) //从date中提取日期的指定部分 MAKEDATE (year ,day ) //给出年及年中的第几天,生成日期串 MAKETIME (hour ,minute ,second ) //生成时间串 MONTHNAME (date ) //英文月份名 NOW ( ) //当前时间 SEC_TO_TIME (seconds ) //秒数转成时间 STR_TO_DATE (string ,format ) //字串转成时间,以format格式显示 TIMEDIFF (datetime1 ,datetime2 ) //两个时间差 TIME_TO_SEC (time ) //时间转秒数] WEEK (date_time [,start_of_week ]) //第几周 YEAR (datetime ) //年份 DAYOFMONTH(datetime) //月的第几天 HOUR(datetime) //小时 LAST_DAY(date) //date的月的最后日期 MICROSECOND(datetime) //微秒 MONTH(datetime) //月 MINUTE(datetime) //分返回符号,正负或0 参考http://xdj651897373-126-com.iteye.com/blog/1819924http://blog.tankywoo.com/2015/04/01/mysql-stored-procedure.htmlhttp://stackoverflow.com/questions/8549619/mysql-dynamically-build-query-string-in-a-stored-procedure-based-on-logic","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"golang反射","slug":"go-reflect","date":"2015-10-15T03:31:30.000Z","updated":"2017-04-08T09:30:50.000Z","comments":true,"path":"2015/10/15/go-reflect/","link":"","permalink":"http://simyy.com/2015/10/15/go-reflect/","excerpt":"go语言中reflect反射机制。详细原文：地址","text":"go语言中reflect反射机制。详细原文：地址在go中，反射是建立在类型基础之上，每一个变量都会有一个静态类型。 接口值到反射对象1234567891011package mainimport ( \"fmt\" \"reflect\")func main() &#123; var x int = 1 fmt.Println(\"type: \", reflect.TypeOf(x))&#125; 1type: int TypeOf函数的定义如下，参数为接口类型，返回值为类型1func TypeOf(i interface &#123;&#125;) Type ValueOf函数的定义如下，参数为接口类型，返回值为Value123var x int = 1fmt.Println(&quot;value: &quot;, reflect.ValueOf(x)) 1value: &lt;int Value&gt; 可以通过Kind函数来检查类型，12fmt.Println(\"Kind: \", reflect.ValueOf(x).Kind())fmt.Println(\"Kind is Int? \", reflect.ValueOf(x).Kind() == reflect.int) 12Kind: intKind is Int? true 反射对象到接口值通过Interface函数可以实现反射对象到接口值的转换，1func (v Value) Interface() interface &#123;&#125; 123// Interface 以 interface&#123;&#125; 返回 v 的值y := v.Interface().(float64)fmt.Println(y) 修改反射对象 修改反射对象的前提条件是其值必须是可设置的 123var x float64 = 3.4v := reflect.ValueOf(x)v.SetFloat(7.3) // Error: panic 为了避免这个问题，需要使用CanSet函数来检查该值的设置性， 123var x float64 = 3.4v := reflect.ValueOf(x)fmt.Println(&quot;settability of v: &quot;, v.CanSet()) 1settability of v: false 那么如何才能设置该值呢？这里需要考虑一个常见的问题，参数传递，传值还是传引用或地址？在上面的例子中，我们使用的是reflect.ValueOf(x)，这是一个值传递，传递的是x的值的一个副本，不是x本身，因此更新副本中的值是不允许的。如果使用reflect.ValueOf(&amp;x)来替换刚才的值传递，就可以实现值的修改。12345678var x float64 = 3.4p := reflect.ValueOf(&amp;x) // 获取x的地址fmt.Println(&quot;settability of p: &quot;, p.CanSet())v := p.Elem()fmt.Println(&quot;settability of v: &quot;, v.CanSet())v.SetFloat(7.1)fmt.Println(v.Interface())fmt.Println(x) 1234settability of p: falsesettability of v: true7.17.1 获取结构体标签首先介绍如何遍历结构体字段内容，假设结构体如下，123456type T struct &#123; A int B string&#125;t := T&#123;12, \"skidoo\"&#125; 从而，通过反射来遍历所有的字段内容123456s := reflect.ValueOf(&amp;t).Elem()typeOfT := s.Type()for i := 0; i &lt; s.NumField(); i++ &#123; f := s.Field(i) fmt.Printf(\"%d %s %s = %v\\n\", i, typeOfT.Field(i).Name, f.Type(), f.Interface())&#125; 120 A int = 231 B string = skidoo 接下来，如何获取结构体的标签内容?12345678910func main() &#123; type S struct &#123; F string `species:&quot;gopher&quot; color:&quot;blue&quot;` &#125; s := S&#123;&#125; st := reflect.TypeOf(s) field := st.Field(0) fmt.Println(field.Tag.Get(&quot;color&quot;), field.Tag.Get(&quot;species&quot;))&#125; interface{}到函数反射一般情况下，为了存储多个函数值，一般采用map来存储。其中key为函数名称，而value为相应的处理函数。在这里需要定义好函数类型，但是函数的参数以及返回类型就需要是统一的，如下12345678910111213package mainimport &quot;fmt&quot;func say(text string) &#123; fmt.Println(text)&#125;func main() &#123; var funcMap = make(map[string]func(string)) funcMap[&quot;say&quot;] = say funcMap[&quot;say&quot;](&quot;hello&quot;)&#125; 如果希望map可以存储任意类型的函数（参数不同，返回值不同），那么就需要用interface{}而不是func(param…)来定义value。 12345678910111213package mainimport &quot;fmt&quot;func say(text string) &#123; fmt.Println(text)&#125;func main() &#123; var funcMap = make(map[string]interface&#123;&#125;) funcMap[&quot;say&quot;] = say funcMap[&quot;say&quot;](&quot;hello&quot;)&#125; 1cannot call non-function funcMap[&quot;say&quot;] (type interface &#123;&#125;) 直接调用会报错，提示不能调用interface{}类型的函数。 这时，需要利用reflect把函数从interface转换到函数来使用，12345678910111213141516171819202122232425package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func say(text string) &#123; fmt.Println(text)&#125;func Call(m map[string]interface&#123;&#125;, name string, params ... interface&#123;&#125;) (result []reflect.Value) &#123; f := reflect.ValueOf(m[name]) // 把函数转化为reflect value in := make([]reflect.Value, len(params)) for k, param := range params &#123; in[k] = reflect.ValueOf(param) // 把参数转为为[] reflect value &#125; result = f.Call(in) return&#125;func main() &#123; var funcMap = make(map[string]interface&#123;&#125;) funcMap[&quot;say&quot;] = say Call(funcMap, &quot;say&quot;, &quot;hello&quot;) 其中，使用Call函数，调用函数v，参数为[]value1func (v Value) Call(in []Value) []Value","categories":[{"name":"go","slug":"go","permalink":"http://simyy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"http://simyy.com/tags/go/"}]},{"title":"模拟退火算法","slug":"simulate-anneal","date":"2015-09-22T03:44:58.000Z","updated":"2017-04-08T09:33:48.000Z","comments":true,"path":"2015/09/22/simulate-anneal/","link":"","permalink":"http://simyy.com/2015/09/22/simulate-anneal/","excerpt":"模拟退火算法","text":"模拟退火算法 搜索算法启发式搜索 按照预定的控制策略实行搜索，在搜索过程中获取的中间信息补用来改进控制策略，称为盲目搜索，反之，称为启发式搜索。 常见的深度优先和广度优先搜索算法都属于盲目搜索；而爬山法、模拟退火算法、遗传算法和蚁群算法等都属于启发式搜索。 模拟退火算法模拟退火算法属于启发式算法。 来源模拟退火算法是来源于固体退火原理，将固体加温至充分高，再让其嘘嘘冷却，加温时，固体内部粒子随温度升高变为无序状，内能增大，而徐徐冷却时粒子逐渐有序，每个温度都达到平衡状态，最后在常温时达到基态，内能减为最小。 根据Metropolis准则，固体内部粒子在温度T时趋于平衡的概率为exp(△E/(kT))，其中E为温度T时的内能，△E为其改变量，k为Boltzmann常数。 $$f(n)=\\begin{cases}1, \\quad if \\quad E(x_{new}) &lt; E(x_{old})\\\\exp(-\\frac{E(x_{new})-E(x_{old})}{T}), \\quad if \\quad E(x_{new}) &gt; E(x_{old}) \\end{cases}$$ 主要思想用固体退火模拟组合优化问题，可以将内能E模拟为目标函数值f，温度T演化成控制参数。 算法流程 随机产生一个初始解$ x_0 $, 令$ x_{best} = x_0 $， 并计算目标函数值$E(x_0)$ 设置初始温度$ T_0 $，迭代次数为N,i=0记录迭代次数 当i &lt; N, 并且不满足终止条件， 对当前最优解$ x_{best} $按照某一邻域函数产生一个新的解$ x_{new} $。计算新的目标函数值$ E(x_{new}) $，从而得到目标函数值的增量$△E = E(x_{new}) - E(x_{best})$ 应用Metropolis准则，当△E &lt; 0时，总是接受移动$x_{best}=x_{new}$，否则以一定概率接受移动$p=exp(-△E/T(i))$ i += 1 输出当前最优解，计算结束 伪代码 来自wikipedia 123456789s := 0; e := E(s)k := 0 ## //k为评估次数while k &lt; kmax and e &gt; emax //kmax最大评估次数，emax可接受的最大结果 sn := neighbour(s) //随机选取一个邻居状态 en := E(sn) //能量结果 if random() &lt; P(e, en, temp(k/kmax)) then s := sn; e := en //如果满足metropolis准则，则移动到该邻居节点 k += 1 //评估完成，次数+1return s 冷却进度这面两种方法都能够让模拟退火算法收敛于局部最小。 经典模拟退火算法的降温方式,从下面的公式中可以得出，该算法是一种指数的变化趋势 $T(t)=\\frac {T_0} {lg(1+t)}$ 快速模拟退火算法的降温方式，稳定变化 $T(t)=\\frac {T_0} {1 + t}$ 终止条件 设置终止的目标值 迭代次数限制 连续若干步保持不变 系统熵是否稳定 改进策略 增加升温或重升温策略，调整搜索进程中的状态，避免陷入局部最优 增加移动的记忆功能，避免遗失当前遇到过的最优解 结合其他搜索方法","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"MySQL实现嵌套集合模型(Nested Set Model)","slug":"Nested-Set-Model","date":"2015-09-21T16:32:51.000Z","updated":"2017-04-08T09:28:43.000Z","comments":true,"path":"2015/09/22/Nested-Set-Model/","link":"","permalink":"http://simyy.com/2015/09/22/Nested-Set-Model/","excerpt":"译文主要是介绍如何用MySQL来存储嵌套集合数据。在其中会增加一些自己的理解，也会删除掉一些自认为无用的废话。","text":"译文主要是介绍如何用MySQL来存储嵌套集合数据。在其中会增加一些自己的理解，也会删除掉一些自认为无用的废话。这篇文章主要讲的是嵌套集合模型，所以邻接表不是本文的重点，简单略过就好。 也许这是原文地址，因为我也不知道这是不是原文。 介绍什么是分层数据？类似于树形结构，除了根节点和叶子节点外，所有节点都有用一个父节点和多个子节点。 那么，在MySQL中如何处理分层数据呢？ 原文中介绍了两种分层结构模型：邻接表模型和嵌套集合模型。 邻接表模型(The Adjacency List Model)首先，建立测试表，导入测试数据，12345678910111213141516171819202122232425262728293031323334CREATE TABLE category( category_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(20) NOT NULL, parent INT DEFAULT NULL);INSERT INTO category VALUES (1,&apos;ELECTRONICS&apos;,NULL), (2,&apos;TELEVISIONS&apos;,1), (3,&apos;TUBE&apos;,2), (4,&apos;LCD&apos;,2), (5,&apos;PLASMA&apos;,2), (6,&apos;PORTABLE ELECTRONICS&apos;,1), (7,&apos;MP3 PLAYERS&apos;,6), (8,&apos;FLASH&apos;,7), (9,&apos;CD PLAYERS&apos;,6), (10,&apos;2 WAY RADIOS&apos;,6);SELECT * FROM category ORDER BY category_id;+-------------+----------------------+--------+| category_id | name | parent |+-------------+----------------------+--------+| 1 | ELECTRONICS | NULL || 2 | TELEVISIONS | 1 || 3 | TUBE | 2 || 4 | LCD | 2 || 5 | PLASMA | 2 || 6 | PORTABLE ELECTRONICS | 1 || 7 | MP3 PLAYERS | 6 || 8 | FLASH | 7 || 9 | CD PLAYERS | 6 || 10 | 2 WAY RADIOS | 6 |+-------------+----------------------+--------+10 rows in set (0.00 sec) 在邻接表中，所有的数据均拥有一个Parent字段，用来存储它的父节点。当前节点为根节点的话，它的父节点则为NULL。那么在遍历的时候，可以使用递归来实现查询整棵树，从根节点开始，不断寻找子节点（父节点-&gt;子节点-&gt;父节点-&gt;子节点）。 检索分层路径一般需要获取一个分层结构的路径问题，那么 123456789101112131415161718SELECT t1.name AS lev1, t2.name as lev2, t3.name as lev3, t4.name as lev4FROM category AS t1LEFT JOIN category AS t2 ON t2.parent = t1.category_idLEFT JOIN category AS t3 ON t3.parent = t2.category_idLEFT JOIN category AS t4 ON t4.parent = t3.category_idWHERE t1.name = &apos;ELECTRONICS&apos;;+-------------+----------------------+--------------+-------+| lev1 | lev2 | lev3 | lev4 |+-------------+----------------------+--------------+-------+| ELECTRONICS | TELEVISIONS | TUBE | NULL || ELECTRONICS | TELEVISIONS | LCD | NULL || ELECTRONICS | TELEVISIONS | PLASMA | NULL || ELECTRONICS | PORTABLE ELECTRONICS | MP3 PLAYERS | FLASH || ELECTRONICS | PORTABLE ELECTRONICS | CD PLAYERS | NULL || ELECTRONICS | PORTABLE ELECTRONICS | 2 WAY RADIOS | NULL |+-------------+----------------------+--------------+-------+6 rows in set (0.00 sec) #检索叶子节点123456789101112131415SELECT t1.name FROMcategory AS t1 LEFT JOIN category as t2ON t1.category_id = t2.parentWHERE t2.category_id IS NULL;+--------------+| name |+--------------+| TUBE || LCD || PLASMA || FLASH || CD PLAYERS || 2 WAY RADIOS |+--------------+ #检索指定路径12345678910111213SELECT t1.name AS lev1, t2.name as lev2, t3.name as lev3, t4.name as lev4FROM category AS t1LEFT JOIN category AS t2 ON t2.parent = t1.category_idLEFT JOIN category AS t3 ON t3.parent = t2.category_idLEFT JOIN category AS t4 ON t4.parent = t3.category_idWHERE t1.name = &apos;ELECTRONICS&apos; AND t4.name = &apos;FLASH&apos;;+-------------+----------------------+-------------+-------+| lev1 | lev2 | lev3 | lev4 |+-------------+----------------------+-------------+-------+| ELECTRONICS | PORTABLE ELECTRONICS | MP3 PLAYERS | FLASH |+-------------+----------------------+-------------+-------+1 row in set (0.01 sec) 邻接表的缺点在检索路径的过程中，除了本层外，每一层都会对应一个LEFT JOIN，那么如果层数不定怎么办？或者层数过多？在删除中间层的节点时，需要同时删除该节点下的所有节点，否则会出现孤立节点。 嵌套集合模型Nested Set Model原文中主要的目的是介绍嵌套集合模型，如下 通过集合的包含关系，嵌套结合模型可以表示分层结构，每一个分层可以用一个Set来表示（一个圈），父节点所在的圈包含所有子节点所在的圈。 为了用MySQL来表示集合关系，需要定义连个字段left和right（表示一个集合的范围）。 1234567891011121314151617181920212223242526272829303132333435CREATE TABLE nested_category ( category_id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(20) NOT NULL, lft INT NOT NULL, rgt INT NOT NULL);INSERT INTO nested_category VALUES (1,&apos;ELECTRONICS&apos;,1,20), (2,&apos;TELEVISIONS&apos;,2,9), (3,&apos;TUBE&apos;,3,4), (4,&apos;LCD&apos;,5,6), (5,&apos;PLASMA&apos;,7,8), (6,&apos;PORTABLE ELECTRONICS&apos;,10,19), (7,&apos;MP3 PLAYERS&apos;,11,14), (8,&apos;FLASH&apos;,12,13), (9,&apos;CD PLAYERS&apos;,15,16), (10,&apos;2 WAY RADIOS&apos;,17,18);SELECT * FROM nested_category ORDER BY category_id;+-------------+----------------------+-----+-----+| category_id | name | lft | rgt |+-------------+----------------------+-----+-----+| 1 | ELECTRONICS | 1 | 20 || 2 | TELEVISIONS | 2 | 9 || 3 | TUBE | 3 | 4 || 4 | LCD | 5 | 6 || 5 | PLASMA | 7 | 8 || 6 | PORTABLE ELECTRONICS | 10 | 19 || 7 | MP3 PLAYERS | 11 | 14 || 8 | FLASH | 12 | 13 || 9 | CD PLAYERS | 15 | 16 || 10 | 2 WAY RADIOS | 17 | 18 |+-------------+----------------------+-----+-----+ 由于left和right是MySQL的保留字，因此，字段名称用lft和rgt代替。每一个集合都是从lft开始到rgt结束，也就是集合的两个边界。 在树中也同样适用， 当为树状结构编号时，我们从左到右，一次一层，赋值按照从左到右的顺序遍历其子节点，这种方法称为先序遍历算法。 检索分层路径由于子节点的lft值总在父节点的lft和rgt值之间，所以可以通过父节点连接到子节点上来检索整棵树。 123456789101112131415161718192021SELECT node.nameFROM nested_category AS node, nested_category AS parentWHERE node.lft BETWEEN parent.lft AND parent.rgt AND parent.name = &apos;ELECTRONICS&apos;ORDER BY node.lft;+----------------------+| name |+----------------------+| ELECTRONICS || TELEVISIONS || TUBE || LCD || PLASMA || PORTABLE ELECTRONICS || MP3 PLAYERS || FLASH || CD PLAYERS || 2 WAY RADIOS |+----------------------+&lt;/pre&gt; 这个方法并不需要考虑层数，而且不需要考虑节点的rgt。 检索所有叶子节点由于每一个叶子节点的rgt=lft+1，那么只需要这一个条件即可。 1234567891011121314SELECT nameFROM nested_categoryWHERE rgt = lft + 1;+--------------+| name |+--------------+| TUBE || LCD || PLASMA || FLASH || CD PLAYERS || 2 WAY RADIOS |+--------------+ 检索节点路径不再需要多个join连接操作。 123456789101112131415SELECT parent.nameFROM nested_category AS node, nested_category AS parentWHERE node.lft BETWEEN parent.lft AND parent.rgt AND node.name = &apos;FLASH&apos;ORDER BY node.lft;+----------------------+| name |+----------------------+| ELECTRONICS || PORTABLE ELECTRONICS || MP3 PLAYERS || FLASH |+----------------------+ 检索节点深度通过COUNT和GROUP BY函数来获取父节点的个数。 123456789101112131415161718192021SELECT node.name, (COUNT(parent.name) - 1) AS depthFROM nested_category AS node, nested_category AS parentWHERE node.lft BETWEEN parent.lft AND parent.rgtGROUP BY node.nameORDER BY node.lft;+----------------------+-------+| name | depth |+----------------------+-------+| ELECTRONICS | 0 || TELEVISIONS | 1 || TUBE | 2 || LCD | 2 || PLASMA | 2 || PORTABLE ELECTRONICS | 1 || MP3 PLAYERS | 2 || FLASH | 3 || CD PLAYERS | 2 || 2 WAY RADIOS | 2 |+----------------------+-------+ 甚至可以得到分层的缩进结果， 123456789101112131415161718192021SELECT CONCAT( REPEAT(&apos; &apos;, COUNT(parent.name) - 1), node.name) AS nameFROM nested_category AS node, nested_category AS parentWHERE node.lft BETWEEN parent.lft AND parent.rgtGROUP BY node.nameORDER BY node.lft;+-----------------------+| name |+-----------------------+| ELECTRONICS || TELEVISIONS || TUBE || LCD || PLASMA || PORTABLE ELECTRONICS || MP3 PLAYERS || FLASH || CD PLAYERS || 2 WAY RADIOS |+-----------------------+ 检索子树的深度考虑到检索中需要自连接的node或parent，因此需要增加一个额外的连接来作为子查询来限制子树。 12345678910111213141516171819202122232425262728SELECT node.name, (COUNT(parent.name) - (sub_tree.depth + 1)) AS depthFROM nested_category AS node, nested_category AS parent, nested_category AS sub_parent, ( SELECT node.name, (COUNT(parent.name) - 1) AS depth FROM nested_category AS node, nested_category AS parent WHERE node.lft BETWEEN parent.lft AND parent.rgt AND node.name = &apos;PORTABLE ELECTRONICS&apos; GROUP BY node.name ORDER BY node.lft )AS sub_treeWHERE node.lft BETWEEN parent.lft AND parent.rgt AND node.lft BETWEEN sub_parent.lft AND sub_parent.rgt AND sub_parent.name = sub_tree.nameGROUP BY node.nameORDER BY node.lft;+----------------------+-------+| name | depth |+----------------------+-------+| PORTABLE ELECTRONICS | 0 || MP3 PLAYERS | 1 || FLASH | 2 || CD PLAYERS | 1 || 2 WAY RADIOS | 1 |+----------------------+-------+ 检索节点的直接子节点假设一个场景，当用户点击网站上电子产品的一个分类时，将呈现该分类下的产品，同时需要列出所有子分类，并不是全部分类。为了限制显示分类的层数，需要使用HAVING字句， 12345678910111213141516171819202122232425262728SELECT node.name, (COUNT(parent.name) - (sub_tree.depth + 1)) AS depthFROM nested_category AS node, nested_category AS parent, nested_category AS sub_parent, ( SELECT node.name, (COUNT(parent.name) - 1) AS depth FROM nested_category AS node, nested_category AS parent WHERE node.lft BETWEEN parent.lft AND parent.rgt AND node.name = &apos;PORTABLE ELECTRONICS&apos; GROUP BY node.name ORDER BY node.lft )AS sub_treeWHERE node.lft BETWEEN parent.lft AND parent.rgt AND node.lft BETWEEN sub_parent.lft AND sub_parent.rgt AND sub_parent.name = sub_tree.nameGROUP BY node.nameHAVING depth &amp;lt;= 1ORDER BY node.lft;+----------------------+-------+| name | depth |+----------------------+-------+| PORTABLE ELECTRONICS | 0 || MP3 PLAYERS | 1 || CD PLAYERS | 1 || 2 WAY RADIOS | 1 |+----------------------+-------+ 增加新节点上面已经介绍了如何检索结果，那么如何才能增加新的节点呢？ 如果希望在TELEVISIONS和PROTABLE ELECTRONICS节点之间增加一个新的节点，那么新节点的lft和rgt的值应该是10和11，那么所有大于10的节点（新节点右侧的节点）的lft和rgt都应该加2，如上图所示。 1234567891011LOCK TABLE nested_category WRITE;SELECT @myRight := rgt FROM nested_categoryWHERE name = &apos;TELEVISIONS&apos;;UPDATE nested_category SET rgt = rgt + 2 WHERE rgt &amp;gt; @myRight;UPDATE nested_category SET lft = lft + 2 WHERE lft &amp;gt; @myRight;INSERT INTO nested_category(name, lft, rgt) VALUES(&apos;GAME CONSOLES&apos;, @myRight + 1, @myRight + 2);UNLOCK TABLES 如果希望在叶子节点下增加节点，需要修改下查询语句， 123456789101112LOCK TABLE nested_category WRITE;SELECT @myLeft := lft FROM nested_categoryWHERE name = &apos;2 WAY RADIOS&apos;;UPDATE nested_category SET rgt = rgt + 2 WHERE rgt &amp;gt; @myLeft;UPDATE nested_category SET lft = lft + 2 WHERE lft &amp;gt; @myLeft;INSERT INTO nested_category(name, lft, rgt) VALUES(&apos;FRS&apos;, @myLeft + 1, @myLeft + 2);UNLOCK TABLES; 删除节点删除叶子节点比较容易，只需要删除自己，而删除一个中间层节点就需要删除其所有子节点。在这个模型中，所有子节点的节点正好在lft和rgt之间。 123456789101112LOCK TABLE nested_category WRITE;SELECT @myLeft := lft, @myRight := rgt, @myWidth := rgt - lft + 1FROM nested_categoryWHERE name = &apos;GAME CONSOLES&apos;;DELETE FROM nested_category WHERE lft BETWEEN @myLeft AND @myRight;UPDATE nested_category SET rgt = rgt - @myWidth WHERE rgt &amp;gt; @myRight;UPDATE nested_category SET lft = lft - @myWidth WHERE lft &amp;gt; @myRight;UNLOCK TABLES; 在某些情况下，只需要删除某个节点，但是并不希望删除该节点下的子节点数据。通过把右侧所有节点的左右值-2，当前节点的子节点左右值-112345678910111213LOCK TABLE nested_category WRITE;SELECT @myLeft := lft, @myRight := rgt, @myWidth := rgt - lft + 1FROM nested_categoryWHERE name = &apos;PORTABLE ELECTRONICS&apos;;DELETE FROM nested_category WHERE lft = @myLeft;UPDATE nested_category SET rgt = rgt - 1, lft = lft - 1 WHERE lft BETWEEN @myLeft AND @myRight;UPDATE nested_category SET rgt = rgt - 2 WHERE rgt &amp;gt; @myRight;UPDATE nested_category SET lft = lft - 2 WHERE lft &amp;gt; @myRight;UNLOCK TABLES; 最后的思考原作者推荐了一本名为《Joe Celko’s Trees and Hierarchies in SQL for Smarties》的书籍，该书的作者是SQL领域的大神Joe Celko（嵌套几何模型的创造者）。这本书涵盖了本文中未涉及到的一些高级话题。","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://simyy.com/tags/mysql/"}]},{"title":"Go项目结构和模块导入","slug":"golang-project-import","date":"2015-09-17T08:46:17.000Z","updated":"2017-04-08T09:30:47.000Z","comments":true,"path":"2015/09/17/golang-project-import/","link":"","permalink":"http://simyy.com/2015/09/17/golang-project-import/","excerpt":"golang项目结构与其他语言类似，但是仍然有一些需要注意的地方。","text":"golang项目结构与其他语言类似，但是仍然有一些需要注意的地方。 项目结构环境配置go 命令依赖一个重要的环境变量：$GOPATH，它表示GO项目的路径，如下设置 1export GOPATH=/home/t/gospace 对于GOPATH来说，允许多个项目目录（Unix中为“：”，Windows中为“；”）。 项目目录在项目目录中，一般包含三个文件夹，分别为src，pkg和 bin。各个文件夹功能如下， src 存放golang源码 pkg 存放编译后的文件 bin 存放编译后可执行的文件 模块导入在golang中，模块导入包括两种导入方式：相对路径和绝对路径。 相对导入当前文件同一目录的model目录，但是不建议这种方式来import 1234import ( \"./test1\" \"../test2\") 绝对导入前提条件需要把该项目加入到golang的GOPATH中， 1234import ( \"project/module1\" \"project/module2/t\") import的其他操作点操作点操作的含义就是这个包导入之后在你调用这个包的函数时，你可以省略前缀的包名, 12345import . \"fmt\"func test() &#123; Println(\"test\")&#125; 别名操作别名操作就是把包命名成另一个名字 12345import f \"fmt\"func test() &#123; f.Println(\"test\")&#125; _操作操作其实是引入该包，而不直接使用包里面的函数，而是调用了该包里面的init函数 文件1： module/module1.go1234567package module1import \"fmt\"func init() &#123; fmt.Println(\"this is module1\")&#125; 文件2： main.go12345678910package mainimport ( \"fmt\" _ \"module\")func main() &#123; fmt.Println(\"this is a test\")&#125; output:12this is module1this is a test","categories":[{"name":"go","slug":"go","permalink":"http://simyy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"http://simyy.com/tags/go/"}]},{"title":"sqlalchemy(二)高级用法","slug":"sqlalchemy2","date":"2015-08-19T05:48:34.000Z","updated":"2017-04-08T09:33:11.000Z","comments":true,"path":"2015/08/19/sqlalchemy2/","link":"","permalink":"http://simyy.com/2015/08/19/sqlalchemy2/","excerpt":"本文将介绍sqlalchemy的高级用法。","text":"本文将介绍sqlalchemy的高级用法。 外键以及relationship首先创建数据库，在这里一个user对应多个address，因此需要在address上增加user_id这个外键（一对多）。 12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env python# encoding: utf-8from sqlalchemy import create_enginefrom sqlalchemy import Columnfrom sqlalchemy import Integerfrom sqlalchemy import Stringfrom sqlalchemy import ForeignKeyfrom sqlalchemy.orm import backreffrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.orm import relationship, backreffrom sqlalchemy.ext.declarative import declarative_baseBase = declarative_base()class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) name = Column(String(32)) addresses = relationship(\"Address\", order_by=\"Address.id\", backref=\"user\")class Address(Base): __tablename__ = 'addresses' id = Column(Integer, primary_key=True) email_address = Column(String(32), nullable=False) user_id = Column(Integer, ForeignKey('users.id')) #user = relationship(\"User\", backref=backref('addresses', order_by=id))engine = create_engine('mysql://root:root@localhost:3306/test', echo=True)#Base.metadata.create_all(engine) 接下来，调用user和address来添加数据， 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; jack = User(name='jack')&gt;&gt;&gt; jack.addressTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'User' object has no attribute 'address'&gt;&gt;&gt; jack.addresses[]&gt;&gt;&gt; jack.addresses = [Address(email_address='test@test.com'), Address(email_address='test1@test1.com')]&gt;&gt;&gt; jack.addresses[&lt;demo.Address object at 0x7f2536564f90&gt;, &lt;demo.Address object at 0x7f2535dc71d0&gt;]&gt;&gt;&gt; session.add(jack)&gt;&gt;&gt; session.commit()2015-08-19 13:45:36,237 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'2015-08-19 13:45:36,237 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,238 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()2015-08-19 13:45:36,238 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,239 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8' and `Collation` = 'utf8_bin'2015-08-19 13:45:36,239 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,239 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_12015-08-19 13:45:36,239 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,240 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_12015-08-19 13:45:36,240 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,240 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_12015-08-19 13:45:36,240 INFO sqlalchemy.engine.base.Engine ()2015-08-19 13:45:36,241 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)2015-08-19 13:45:36,242 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name) VALUES (%s)2015-08-19 13:45:36,242 INFO sqlalchemy.engine.base.Engine ('jack',)2015-08-19 13:45:36,243 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (%s, %s)2015-08-19 13:45:36,243 INFO sqlalchemy.engine.base.Engine ('test@test.com', 1L)2015-08-19 13:45:36,243 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (%s, %s)2015-08-19 13:45:36,243 INFO sqlalchemy.engine.base.Engine ('test1@test1.com', 1L)2015-08-19 13:45:36,244 INFO sqlalchemy.engine.base.Engine COMMIT&gt;&gt;&gt; 此时，查看数据库，可以得到刚才插入的数据， 12345678910111213141516mysql&gt; select * from users;+----+------+| id | name |+----+------+| 1 | jack |+----+------+1 row in set (0.00 sec)mysql&gt; select * from addresses;+----+-----------------+---------+| id | email_address | user_id |+----+-----------------+---------+| 1 | test@test.com | 1 || 2 | test1@test1.com | 1 |+----+-----------------+---------+2 rows in set (0.00 sec) join查询如果不使用join的话，可以直接联表查询， 123456&gt;&gt;&gt; session.query(User.name, Address.email_address).filter(User.id==Address.user_id).filter(Address.email_address==&apos;test@test.com&apos;).all()2015-08-19 14:02:02,877 INFO sqlalchemy.engine.base.Engine SELECT users.name AS users_name, addresses.email_address AS addresses_email_address FROM users, addresses WHERE users.id = addresses.user_id AND addresses.email_address = %s2015-08-19 14:02:02,878 INFO sqlalchemy.engine.base.Engine (&apos;test@test.com&apos;,)[(&apos;jack&apos;, &apos;test@test.com&apos;)] 在sqlalchemy中提供了Queqy.join()函数， 1234567891011121314151617181920212223242526&gt;&gt;&gt; session.query(User).join(Address).filter(Address.email_address==&apos;test@test.com&apos;).first()2015-08-19 14:06:56,624 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name FROM users INNER JOIN addresses ON users.id = addresses.user_id WHERE addresses.email_address = %s LIMIT %s2015-08-19 14:06:56,624 INFO sqlalchemy.engine.base.Engine (&apos;test@test.com&apos;, 1)&lt;demo.User object at 0x7f9a74139a10&gt;&gt;&gt;&gt; session.query(User).join(Address).filter(Address.email_address==&apos;test@test.com&apos;).first().name2015-08-19 14:07:04,224 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name FROM users INNER JOIN addresses ON users.id = addresses.user_id WHERE addresses.email_address = %s LIMIT %s2015-08-19 14:07:04,224 INFO sqlalchemy.engine.base.Engine (&apos;test@test.com&apos;, 1)&apos;jack&apos;&gt;&gt;&gt; session.query(User).join(Address).filter(Address.email_address==&apos;test@test.com&apos;).first().addresses2015-08-19 14:07:06,534 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name FROM users INNER JOIN addresses ON users.id = addresses.user_id WHERE addresses.email_address = %s LIMIT %s2015-08-19 14:07:06,534 INFO sqlalchemy.engine.base.Engine (&apos;test@test.com&apos;, 1)2015-08-19 14:07:06,535 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id FROM addresses WHERE %s = addresses.user_id ORDER BY addresses.id2015-08-19 14:07:06,535 INFO sqlalchemy.engine.base.Engine (1L,)[&lt;demo.Address object at 0x7f9a74139350&gt;, &lt;demo.Address object at 0x7f9a741390d0&gt;]&gt;&gt;&gt; 注意，上面的用法的前提是存在外键的情况下，如果没有外键，那么可以使用, 1234query.join(Address, User.id==Address.user_id) # explicit conditionquery.join(User.addresses) # specify relationship from left to rightquery.join(Address, User.addresses) # same, with explicit targetquery.join(&apos;addresses&apos;) 表的别名12&gt;&gt;&gt; from sqlalchemy.orm import aliased&gt;&gt;&gt; adalias1 = aliased(Address) 子查询假设我们需要这样一个查询， 12345678910mysql&gt; SELECT users.*, adr_count.address_count FROM users LEFT OUTER JOIN -&gt; (SELECT user_id, count(*) AS address_count -&gt; FROM addresses GROUP BY user_id) AS adr_count -&gt; ON users.id=adr_count.user_id;+----+------+---------------+| id | name | address_count |+----+------+---------------+| 1 | jack | 2 |+----+------+---------------+1 row in set (0.00 sec) 1234567891011# 生成子句，等同于（select user_id ... group_by user_id）&gt;&gt;&gt; sbq = session.query(Address.user_id, func.count(&apos;*&apos;).label(&apos;address_count&apos;)).group_by(Address.user_id).subquery()# 联接子句，注意子句中需要使用c来调用字段内容&gt;&gt;&gt; session.query(User.name, sbq.c.address_count).outerjoin(sbq, User.id==sbq.c.user_id).all()2015-08-19 14:42:53,425 INFO sqlalchemy.engine.base.Engine SELECT users.name AS users_name, anon_1.address_count AS anon_1_address_countFROM users LEFT OUTER JOIN (SELECT addresses.user_id AS user_id, count(%s) AS address_countFROM addresses GROUP BY addresses.user_id) AS anon_1 ON users.id = anon_1.user_id2015-08-19 14:42:53,425 INFO sqlalchemy.engine.base.Engine (&apos;*&apos;,)[(&apos;jack&apos;, 2L)]&gt;&gt;&gt; 包含contains1query.filter(User.addresses.contains(someaddress)) 数据删除delete123&gt;&gt;&gt; session.delete(jack)&gt;&gt;&gt; session.query(User).filter_by(name=&apos;jack&apos;).count()0 外键配置在上面的例子中，删除了user－jack，但是address中的数据并没有删除。 cascade字段用来 12addresses = relationship(&quot;Address&quot;, backref=&apos;user&apos;, cascade=&quot;all, delete, delete-orphan&quot;)","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"},{"name":"orm","slug":"orm","permalink":"http://simyy.com/tags/orm/"}]},{"title":"sqlalchemy(一)基本操作","slug":"sqlalchemy","date":"2015-08-18T14:10:21.000Z","updated":"2017-04-08T09:33:13.000Z","comments":true,"path":"2015/08/18/sqlalchemy/","link":"","permalink":"http://simyy.com/2015/08/18/sqlalchemy/","excerpt":"sqlalchemy采用简单的Python语言，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型。","text":"sqlalchemy采用简单的Python语言，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型。 安装 需要安装MySQLdb pip install sqlalchemy 安装完成后，执行 12&gt;&gt;&gt;import sqlalchemy&gt;&gt;&gt;sqlalchemy.__version__ 连接数据库在sqlalchemy中，session用于创建程序与数据库之间的会话。所有对象的载入和保存都需要通过session对象。 12345678from sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmakerengine = create_engine('mysql://user:passwd@ip:port/db', echo=True)Session = sessionmaker(bind=engine)session = Session()session.execute('show databases') 其中，echo为True代表打开logging。 创建一个映射一个映射对应着一个Python类，用来表示一个表的结构。下面创建一个person表，包括id和name两个字段。 123456789101112from sqlalchemy.ext.declarative import declarative_baseBase = declarative_base()class Person(Base): __tablename__ = 'person' id = Column(Integer, primary_key=True) name = Column(String(32)) def __repr__(self): return \"&lt;Person(name='%s')&gt;\" % self.name 添加数据123456#创建一个person对象person = Person(name='jack')#添加person对象，但是仍然没有commit到数据库session.add(person)#commit操作session.commit() 如何获取id的？12345678910111213141516&gt;&gt;&gt; person = Person(name='ilis')&gt;&gt;&gt; person.id #此时还没有commit到mysql,因此无id&gt;&gt;&gt; session.add(person)&gt;&gt;&gt; person.id #同上&gt;&gt;&gt; session.commit()2015-08-18 23:08:23,530 INFO sqlalchemy.engine.base.Engine INSERT INTO person (name) VALUES (%s)2015-08-18 23:08:23,531 INFO sqlalchemy.engine.base.Engine ('ilis',)2015-08-18 23:08:23,532 INFO sqlalchemy.engine.base.Engine COMMIT&gt;&gt;&gt; person.id #commit后，可以获取该对象的id2015-08-18 23:08:27,556 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)2015-08-18 23:08:27,557 INFO sqlalchemy.engine.base.Engine SELECT person.id AS person_id, person.name AS person_nameFROM personWHERE person.id = %s2015-08-18 23:08:27,557 INFO sqlalchemy.engine.base.Engine (5L,)5L&gt;&gt;&gt; 添加多个数据12345session.add_all([ Person(name='jack'), Person(name='mike')])session.commit() 回滚123456789101112131415161718192021&gt;&gt;&gt; person = Person(name='test')&gt;&gt;&gt; session.add(person)&gt;&gt;&gt; session.query(person).filter(name=='test')&gt;&gt;&gt; session.query(Person).filter(Person.name=='test').all()2015-08-18 23:13:23,265 INFO sqlalchemy.engine.base.Engine INSERT INTO person (name) VALUES (%s)2015-08-18 23:13:23,265 INFO sqlalchemy.engine.base.Engine ('test',)2015-08-18 23:13:23,267 INFO sqlalchemy.engine.base.Engine SELECT person.id AS person_id, person.name AS person_nameFROM personWHERE person.name = %s2015-08-18 23:13:23,267 INFO sqlalchemy.engine.base.Engine ('test',)[&lt;demo.Person object at 0x7f4e37730510&gt;]&gt;&gt;&gt; session.rollback()2015-08-18 23:13:37,496 INFO sqlalchemy.engine.base.Engine ROLLBACK&gt;&gt;&gt; session.query(Person).filter(Person.name=='test').all()2015-08-18 23:13:38,690 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)2015-08-18 23:13:38,691 INFO sqlalchemy.engine.base.Engine SELECT person.id AS person_id, person.name AS person_nameFROM personWHERE person.name = %s2015-08-18 23:13:38,692 INFO sqlalchemy.engine.base.Engine ('test',)[]&gt;&gt;&gt; 数据查询使用Session的query()方法。 12345678910111213141516171819202122232425262728293031323334#获取所有数据session.query(Person).all()#获取某一列数据,类似于django的get,如果返回数据为多个则报错session.query(Person).filter(Person.name=='jack').one()#获取返回数据的第一行session.query(Person).first()#过滤数据session.query(Person.name).filter(Person.id&gt;1).all()#limitsession.query(Person).all()[1:3]#order bysession.query(Person).ordre_by(-Person.id)#equal/like/inquery = session.query(Person)query.filter(Person.id==1).all()query.filter(Person.id!=1).all()query.filter(Person.name.like('%ac%')).all()query.filter(Person.id.in_([1,2,3])).all()query.filter(~Person.id.in_([1,2,3])).all()query.filter(Person.name==None).all()#and orfrom sqlalchemy import and_query.filter(and_(Person.id==1, Person.name=='jack')).all()query.filter(Person.id==1, Person.name=='jack').all()query.filter(Person.id==1).filter(Person.name=='jack').all()from sqlalchemy import or_query.filter(or_(Person.id==1, Person.id==2)).all() 使用text12345678from sqlalchemy import textquery.filter(text(\"id&gt;1\")).all()query.filter(Person.id&gt;1).all() #同上query.filter(text(\"id&gt;:id\")).params(id=1).all() #使用:，params来传参query.from_statement( text(\"select * from person where name=:name\")).\\ params(name='jack').all() 计数Query使用count()函数来实现查询计数。 1query.filter(Person.id&gt;1).count() group by的用法12from sqlalchemy import funcsession.query(func.count(Person.name), Person.name),group_by(Person.name).all() 实现count(*)来查询表内行数12session.query(func.count('*')).select_from(Person).scalar()session.query(func.count(Person.id)).scalar()","categories":[{"name":"db","slug":"db","permalink":"http://simyy.com/categories/db/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"},{"name":"orm","slug":"orm","permalink":"http://simyy.com/tags/orm/"}]},{"title":"K-均值聚类算法","slug":"kmeans","date":"2015-07-13T14:08:11.000Z","updated":"2017-04-08T09:34:49.000Z","comments":true,"path":"2015/07/13/kmeans/","link":"","permalink":"http://simyy.com/2015/07/13/kmeans/","excerpt":"聚类是一种无监督的学习算法，它将相似的数据归纳到同一簇中。K-均值是因为它可以按照k个不同的簇来分类，并且不同的簇中心采用簇中所含的均值计算而成。","text":"聚类是一种无监督的学习算法，它将相似的数据归纳到同一簇中。K-均值是因为它可以按照k个不同的簇来分类，并且不同的簇中心采用簇中所含的均值计算而成。 K-均值算法算法思想K-均值是把数据集按照k个簇分类，其中k是用户给定的，其中每个簇是通过质心来计算簇的中心点。 主要步骤： 随机确定k个初始点作为质心 对数据集中的每个数据点找到距离最近的簇 对于每一个簇，计算簇中所有点的均值并将均值作为质心 重复步骤2，直到任意一个点的簇分配结果不变具体实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071from numpy import *import matplotlibimport matplotlib.pyplot as pltdef loadDataSet(fileName): #general function to parse tab -delimited floats dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = map(float,curLine) #map all elements to float() dataMat.append(fltLine) return dataMatdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)def randCent(dataSet, k): n = shape(dataSet)[1] centroids = mat(zeros((k,n)))#create centroid mat for j in range(n):#create random cluster centers, within bounds of each dimension minJ = min(dataSet[:,j]) rangeJ = float(max(dataSet[:,j]) - minJ) centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1)) return centroids def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2)))#create mat to assign data points #to a centroid, also holds SE of each point centroids = createCent(dataSet, k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m):#for each data point assign it to the closest centroid minDist = inf; minIndex = -1 for j in range(k): distJI = distMeas(centroids[j,:],dataSet[i,:]) if distJI &lt; minDist: minDist = distJI; minIndex = j if clusterAssment[i,0] != minIndex: clusterChanged = True clusterAssment[i,:] = minIndex,minDist**2 for cent in range(k):#recalculate centroids ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean print ptsInClust print mean(ptsInClust, axis=0) return return centroids, clusterAssmentdef clusterClubs(numClust=5): datList = [] for line in open('places.txt').readlines(): lineArr = line.split('\\t') datList.append([float(lineArr[4]), float(lineArr[3])]) datMat = mat(datList) myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC) fig = plt.figure() rect=[0.1,0.1,0.8,0.8] scatterMarkers=['s', 'o', '^', '8', 'p', \\ 'd', 'v', 'h', '&gt;', '&lt;'] axprops = dict(xticks=[], yticks=[]) ax0=fig.add_axes(rect, label='ax0', **axprops) imgP = plt.imread('Portland.png') ax0.imshow(imgP) ax1=fig.add_axes(rect, label='ax1', frameon=False) for i in range(numClust): ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:] markerStyle = scatterMarkers[i % len(scatterMarkers)] ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90) ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=300) plt.show() 结果 算法收敛设目标函数为 $$J(c, \\mu) = \\sum _{i=1}^m (x_i - \\mu_{c_{(i)}})^2$$ Kmeans算法是将J调整到最小，每次调整质心，J值也会减小，同时c和$\\mu$也会收敛。由于该函数是一个非凸函数，所以不能保证得到全局最优，智能确保局部最优解。 二分K均值算法为了克服K均值算法收敛于局部最小值的问题，提出了二分K均值算法。 算法思想该算法首先将所有点作为一个簇，然后将该簇一分为2，之后选择其中一个簇继续进行划分，划分规则是按照最大化SSE（目标函数）的值。 主要步骤： 将所有点看成一个簇 计算每一个簇的总误差 在给定的簇上进行K均值聚类，计算将簇一分为二的总误差 选择使得误差最小的那个簇进行再次划分 重复步骤2，直到簇的个数满足要求 具体实现1234567891011121314151617181920212223242526272829303132def biKMeans(dataSet, k, distMeans=distEclud): m, n = shape(dataSet) clusterAssment = mat(zeros((m, 2))) # init all data for index 0 centroid = mean(dataSet, axis=0).tolist() centList = [centroid] for i in range(m): clusterAssment[i, 1] = distMeans(mat(centroid), dataSet[i, :]) ** 2 while len(centList) &lt; k: lowestSSE = inf for i in range(len(centList)): cluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :] # get the clust data of i centroidMat, splitCluster = kMeans(cluster, 2, distMeans) sseSplit = sum(splitCluster[:, 1]) #all sse sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1]) # error sse #print sseSplit, sseNotSplit if sseSplit + sseNotSplit &lt; lowestSSE: bestCentToSplit = i bestNewCent = centroidMat bestClust = splitCluster.copy() lowerSEE = sseSplit + sseNotSplit print bestClust bestClust[nonzero(bestClust[:, 0].A == 1)[0], 0] = len(centList) bestClust[nonzero(bestClust[:, 0].A == 0)[0], 0] = bestCentToSplit print bestClust print 'the bestCentToSplit is: ',bestCentToSplit print 'the len of bestClustAss is: ', len(bestClust) centList[bestCentToSplit] = bestNewCent[0, :].tolist()[0] centList.append(bestNewCent[1, :].tolist()[0]) print clusterAssment clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClust print clusterAssment return mat(centList), clusterAssment 结果","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://simyy.com/tags/机器学习/"}]},{"title":"回归","slug":"regression","date":"2015-06-25T15:14:10.000Z","updated":"2017-04-08T09:33:59.000Z","comments":true,"path":"2015/06/25/regression/","link":"","permalink":"http://simyy.com/2015/06/25/regression/","excerpt":"回归的目的是预测数值型目标值。类似于$y = w_1 \\cdot x_1 + w_2 \\cdot x_2$，其中w称为回归系数，只要可以确定w，就可以通过输入x得到预测值。","text":"回归的目的是预测数值型目标值。类似于$y = w_1 \\cdot x_1 + w_2 \\cdot x_2$，其中w称为回归系数，只要可以确定w，就可以通过输入x得到预测值。 平方误差确定回归系数假设输入为x，输出为y，则平方误差可以表示为： $$\\sum_{i=1}^m (y_i - x_i^T w)^2)$$ 为了让平方误差最小，令导数为0求得最佳回归系数，则 $$w = (X^T X)^{-1}X^Ty$$算法实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243from numpy import *import matplotlib.pyplot as pltdef loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t')) - 1 dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr =[] curLine = line.strip().split('\\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMatdef standRegres(xArr,yArr): xMat = mat(xArr); yMat = mat(yArr).T xTx = xMat.T*xMat if linalg.det(xTx) == 0.0: print \"This matrix is singular, cannot do inverse\" return ws = xTx.I * (xMat.T*yMat) return ws def regression1(): xArr, yArr = loadDataSet(\"Ch08/ex0.txt\") xMat = mat(xArr) yMat = mat(yArr) ws = standRegres(xArr, yArr) fig = plt.figure() ax = fig.add_subplot(111) print xMat[:, 1].flatten() print yMat.T[:, 0].flatten() ax.scatter(xMat[:, 1].flatten(), yMat.T[:, 0].flatten().A[0]) xCopy = xMat.copy() xCopy.sort(0) yHat = xCopy * ws ax.plot(xCopy[:, 1], yHat) plt.show()if __name__ == \"__main__\": regression1() 结果如下： 局部加权线性回归最简单的线性回归(locally weighted linear regression)具有最小均方误差的无偏估计，因此会出现欠拟合现象。通过局部加权线性回归就可以优化预测结果，局部加权的回归系数w如下： $$w = (X^T WX)^{-1}X^TWy$$ 其中，W是类似于“核”来对调整不同权值的权重。最常用的核是高斯核，如下： $$w(i, j) = exp (\\frac {|x^{(i)} - x|} {-2k^2})$$ 其中，k会对权重产生影响，k越小，权重变化越快。 算法实现通过核函数来调整权值的权重，可以让附近的点的赋予更高的权值。 1234567891011121314151617181920212223242526272829303132def lwlr(testPoint,xArr,yArr,k=1.0): xMat = mat(xArr); yMat = mat(yArr).T m = shape(xMat)[0] weights = mat(eye((m))) for j in range(m): #next 2 lines create weights matrix diffMat = testPoint - xMat[j,:] # weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2)) xTx = xMat.T * (weights * xMat) if linalg.det(xTx) == 0.0: print \"This matrix is singular, cannot do inverse\" return ws = xTx.I * (xMat.T * (weights * yMat)) return testPoint * wsdef lwlrTest(testArr,xArr,yArr,k=1.0): #loops over all the data points and applies lwlr to each one m = shape(testArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHatdef regression2(): xArr, yArr = loadDataSet(\"Ch08/ex0.txt\") yhat = lwlrTest(xArr, xArr, yArr, 0.01) fig = plt.figure() ax = fig.add_subplot(111) xMat = mat(xArr) srtInd = xMat[:, 1].argsort(0) xSort = xMat[srtInd][:, 0, :] ax.plot(xSort[:, 1], yhat[srtInd]) ax.scatter(xMat[:, -1].flatten(), mat(yArr).T.flatten().A[0], s=2, c=\"red\") plt.show() 结果如下： 因此，k值如果越小会考虑太多的噪声影响，选择适合的k值可以得到最优的结果。 岭回归 ridge regression预测精度对于普通的线性回归模型，样本数量n和特征数量p会影响预测精度： $n \\le p$，最小二乘回归会有较小的方差 $n \\approx p$，容易产生过拟合 $n \\ge p$，最小二乘回归得不到有意义的结果 岭回归算法岭回归是在平方误差的基础之上增加正则项， $$\\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_i x_{ij})^2 + \\lambda \\sum_{j=0}^p w_j^2, \\quad \\lambda &gt; 0$$ 通过确定$\\lambda$的值可以使方差和偏差之间达到一定的平衡。 从而对w求导，得 $$2 X^T (Y - XW) - 2 \\lambda W$$ 令导数为0，则 $$w = (X^TX + \\lambda I)^{-1}X^TY$$","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://simyy.com/tags/机器学习/"}]},{"title":"logistic回归","slug":"logistic","date":"2015-06-24T13:10:30.000Z","updated":"2017-04-08T09:34:39.000Z","comments":true,"path":"2015/06/24/logistic/","link":"","permalink":"http://simyy.com/2015/06/24/logistic/","excerpt":"学习logistic回归。","text":"学习logistic回归。 回归就是对已知公式的未知参数进行估计。比如已知公式是$y = a*x + b$，未知参数是a和b，利用多真实的(x,y)训练数据对a和b的取值去自动估计。估计的方法是在给定训练样本点和已知的公式后，对于一个或多个未知参数，机器会自动枚举参数的所有可能取值，直到找到那个最符合样本点分布的参数（或参数组合）。 logistic分布设X是连续随机变量，X服从logistic分布是指X具有下列分布函数和密度函数： $$F(x)=P(x \\le x)=\\frac 1 {1+e^{-(x-\\mu)/\\gamma}}\\\\f(x)=F^{‘}(x)=\\frac {e^{-(x-\\mu)/\\gamma}} {\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}$$ 其中，$\\mu$为位置参数，$\\gamma$为形状参数。 $f(x)$与$F(x)$图像如下，其中分布函数是以$(\\mu, \\frac 1 2)$为中心对阵，$\\gamma$越小曲线变化越快。 logistic回归模型二项logistic回归模型如下： $$P(Y=1|x)=\\frac {exp(w \\cdot x + b)} {1 + exp(w \\cdot x + b)} \\\\P(Y=0|x)=\\frac {1} {1 + exp(w \\cdot x + b)}$$ 其中，$x \\in R^n$是输入，$Y \\in {0,1}$是输出，w称为权值向量，b称为偏置，$w \\cdot x$为w和x的内积。 参数估计假设： $$P(Y=1|x)=\\pi (x), \\quad P(Y=0|x)=1-\\pi (x)$$ 则似然函数为: $$\\prod_{i=1}^N [\\pi (x_i)]^{y_i} [1 - \\pi(x_i)]^{1-y_i}$$ 求对数似然函数： $$L(w) = \\sum_{i=1}^N [y_i \\log{\\pi(x_i)} + (1-y_i) \\log{(1 - \\pi(x_i)})]\\\\=\\sum_{i=1}^N [y_i \\log{\\frac {\\pi (x_i)} {1 - \\pi(x_i)}} + \\log{(1 - \\pi(x_i)})]$$ 从而对$L(w)$求极大值，得到$w$的估计值。 求极值的方法可以是梯度下降法，梯度上升法等。 梯度上升确定回归系数logistic回归的sigmoid函数： $$\\sigma (z) = \\frac 1 {1 + e^{-z}}$$ 假设logstic的函数为: $$y = w_0 + w_1 x_1 + w_2 x_2 + … + w_n x_n$$ 可简写为: $$y = w_0 + w^T x$$ 梯度上升算法是按照上升最快的方向不断移动，每次都增加$\\alpha \\nabla_w f(w)$， $$w = w + \\alpha \\nabla_w f(w) $$ 其中，$\\nabla_w f(w)$为函数导数，$\\alpha$为增长的步长。 训练算法本算法的主要思想根据logistic回归的sigmoid函数来将函数值映射到有限的空间内，sigmoid函数的取值范围是0~1，从而可以把数据按照0和1分为两类。在算法中，首先要初始化所有的w权值为1，每次计算一次误差并根据误差调整w权值的大小。 每个回归系数都初始化为1 重复N次 计算整个数据集合的梯度 使用$\\alpha \\cdot \\nabla f(x)$来更新w向量 返回回归系数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#!/usr/bin/env python# encoding:utf-8import mathimport numpyimport timeimport matplotlib.pyplot as pltdef sigmoid(x): return 1.0 / (1 + numpy.exp(-x))def loadData(): dataMat = [] laberMat = [] with open(\"test.txt\", 'r') as f: for line in f.readlines(): arry = line.strip().split() dataMat.append([1.0, float(arry[0]), float(arry[1])]) laberMat.append(float(arry[2])) return numpy.mat(dataMat), numpy.mat(laberMat).transpose()def gradAscent(dataMat, laberMat, alpha=0.001, maxCycle=500): \"\"\"general gradscent\"\"\" start_time = time.time() m, n = numpy.shape(dataMat) weights = numpy.ones((n, 1)) for i in range(maxCycle): h = sigmoid(dataMat * weights) error = laberMat - h weights += alpha * dataMat.transpose() * error duration = time.time() - start_time print \"duration of time:\", duration return weightsdef stocGradAscent(dataMat, laberMat, alpha=0.01): start_time = time.time() m, n = numpy.shape(dataMat) weights = numpy.ones((n, 1)) for i in range(m): h = sigmoid(dataMat[i] * weights) error = laberMat[i] - h weights += alpha * dataMat[i].transpose() * error duration = time.time() - start_time print \"duration of time:\", duration return weightsdef betterStocGradAscent(dataMat, laberMat, alpha=0.01, numIter=150): \"\"\"better one, use a dynamic alpha\"\"\" start_time = time.time() m, n = numpy.shape(dataMat) weights = numpy.ones((n, 1)) for j in range(numIter): for i in range(m): alpha = 4 / (1 + j + i) + 0.01 h = sigmoid(dataMat[i] * weights) error = laberMat[i] - h weights += alpha * dataMat[i].transpose() * error duration = time.time() - start_time print \"duration of time:\", duration return weights start_time = time.time()def show(dataMat, laberMat, weights): m, n = numpy.shape(dataMat) min_x = min(dataMat[:, 1])[0, 0] max_x = max(dataMat[:, 1])[0, 0] xcoord1 = []; ycoord1 = [] xcoord2 = []; ycoord2 = [] for i in range(m): if int(laberMat[i, 0]) == 0: xcoord1.append(dataMat[i, 1]); ycoord1.append(dataMat[i, 2]) elif int(laberMat[i, 0]) == 1: xcoord2.append(dataMat[i, 1]); ycoord2.append(dataMat[i, 2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcoord1, ycoord1, s=30, c=\"red\", marker=\"s\") ax.scatter(xcoord2, ycoord2, s=30, c=\"green\") x = numpy.arange(min_x, max_x, 0.1) y = (-weights[0] - weights[1]*x) / weights[2] ax.plot(x, y) plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.show() if __name__ == \"__main__\": dataMat, laberMat = loadData() #weights = gradAscent(dataMat, laberMat, maxCycle=500) #weights = stocGradAscent(dataMat, laberMat) weights = betterStocGradAscent(dataMat, laberMat, numIter=80) show(dataMat, laberMat, weights) 未优化的程序结果如下， 随机梯度上升算法（降低了迭代的次数，算法较快，但结果不够准确）结果如下， 对$\\alpha$进行优化，动态调整步长（同样降低了迭代次数，但是由于代码采用动态调整alpha，提高了结果的准确性），结果如下","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://simyy.com/tags/机器学习/"}]},{"title":"SVM-非线性支持向量机及SMO算法","slug":"ml-svm-2","date":"2015-06-18T14:44:44.000Z","updated":"2017-04-08T09:34:31.000Z","comments":true,"path":"2015/06/18/ml-svm-2/","link":"","permalink":"http://simyy.com/2015/06/18/ml-svm-2/","excerpt":"学习支持向量机。","text":"学习支持向量机。 线性不可分情况线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，为了满足函数间隔大于1的约束条件，可以对每个样本$(x_i, y_i)$引进一个松弛变量$\\xi_i \\ge 0$，使函数间隔加上松弛变量大于等于1,， $$y_i (w \\cdot x_i + b) \\ge 1 - \\xi_i$$ 目标函数变为 $$\\frac 1 2 {||w||^2} + C \\sum_{j=1}^N \\xi_i$$ 其中，C&gt;0称为惩罚参数，值越大对误分类的惩罚越大，值越小对误分类的惩罚越小。 因此，最小化目标函数也就是使$\\frac 1 2 {||w||^2}$尽量小（间隔尽量大），同时使误分类点的个数尽量小。 线性不可分的线性支持向量机的学习问题变成如下凸二次规划问题： $$ \\min_{w,b,\\xi}\\frac 1 2 {||w||}^2 + C \\sum_{i=1}^N \\xi_i \\\\s.t. \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\xi_i, \\quad i=1,2,…,N, \\xi_i \\ge 0, i=1,2,…,N$$ 线性支持向量学习算法 选择惩罚参数C&gt;0，构造并求解凸二次规划问题 $$ \\min_\\alpha \\frac 1 2 \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha_i\\\\s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i = 0 \\\\0 \\le \\alpha_i \\le C, i=1,2,…,N$$ 求得最优解$\\alpha^*=(\\alpha_1^*, \\alpha_2^*, … , \\alpha_N^*)^T$ 计算$w^*=\\sum_{i=1}^N \\alpha_i^* y_i x_i$ 选择$\\alpha^*$的一个分量$\\alpha_j^*$适合条件$0&lt;\\alpha_j^*&lt;C$，计算 $$b^*=y_i - \\sum_{i=1}^N y_i \\alpha_i^*(x_i \\cdot x_j)$$ 求得分离超平面 $$w^* \\cdot x + b^* = 0$$ 分类决策函数： $$f(x) = sign(w^* \\cdot x + b^*)$$ 核函数用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。 核技巧应用在支持向量机的基本思想：通过一个非线性变换将输入空间（欧式空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间H），使得在输入空间$R^n$中的超曲面模型对应于特征空间H中的超平面模型（支持向量机）。 非线性支持向量分类机非线性支持向量机从非线性分类训练集，通过核函数与间隔最大化或凸二次规划，学习得到的分类决策函数： $$f(x)=sign(\\sum_{i=1}^N \\alpha_i^*y_i K(x,x_i) + b^*)$$ 称为非线性支持向量，$K(x,z)$是正定核函数。 学习算法 选择适当的核函数$K(x,z)$和适当的参数C，构造并求解最优化问题 $$\\min_\\alpha \\frac 1 2 \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^N \\alpha_i\\\\s.t. \\quad \\sum_{i=1}^N \\alpha_i y_i = 0, 0&lt;\\alpha_i&lt;C,i=1,2,…,N$$ 求解最优解$\\alpha^*=(\\alpha_1^*, \\alpha_2^*,…,\\alpha_N^*)$ 选择$\\alpha^*$的第一个正分量$0&lt;\\alpha_j^*&lt;C$，计算 $$b^*=y_i - \\sum_{i=1}^N \\alpha_i^* y_i K(x_i \\cdot x_j)$$ 构造决策函数 $$f(x)=sign(\\sum_{i=1}^N \\alpha_i^* y_i K(x \\cdot x_i) + b^*)$$ 序列最小优化算法 SMO算法是一种启发式算法。如果所有变量都满足KKT条件，那么这个最优化问题就解决了（KKT问题是该最优化问题的充要条件），否则，选择两个变量，固定其他变量，针对这两个变量构造二次规划问题。该方法会使原始二次规划问题的目标函数变小，不断分解自问题并对子问题求解进而达到求解原问题的目的。 由于 $$\\sum_{i=1}^N \\alpha_i y_i = 0$$ 所以 $$\\alpha_i = - \\frac 1 {y_i} \\sum_{i=2}^N \\alpha_i y_i$$ 两个变量的二次规划求解假设选择两个变量$\\alpha_1，\\alpha_2$， $$\\min_{\\alpha_1\\alpha_2} \\quad = \\frac 1 2 K_{11} \\alpha_1^2 + \\frac 1 2 K_{22} \\alpha_2^2 + y_1 y_2 K_{12} \\alpha_1 \\alpha_2 \\\\\\quad (\\alpha_1 + \\alpha_2) + y_1 \\alpha_1 \\sum_{i=3}^N y_i \\alpha_i K_{i1} + y_2\\alpha_2\\sum_{i=3}^N y_i \\alpha_i K_{12} \\\\s.t. \\quad \\alpha_1 y_1 + \\alpha_2 y_2 = - \\sum_{i=3}^N y_i \\alpha_i = \\xi \\\\0 \\le \\alpha_i \\le C, i=1,2$$ 由于只有两个变量$(\\alpha_i,\\alpha_j)$，因此根据两变量的符号情况约束条件可用二位空间中的图表示（参考$\\alpha_1 y_1 + \\alpha_2 y_2 = \\xi(常数)$）， L和H是$\\alpha$取值的最小和最大值，如果$y_i != y_j$，则 $$L=\\max(0,\\alpha_2 - \\alpha_1), H=\\min(C,C+\\alpha_2-\\alpha_1)$$ 如果$y_i = y_j$，则 $$L=\\max(0,\\alpha_2 + \\alpha_1 + C), H=\\min(C,\\alpha_2+\\alpha_1)$$ 令 $$g(x) = \\sum_{i=1}^N \\alpha_i y_i K(x_i, x) + b$$ 得到误差值： $$E_i = g(x_i) - y_i = ( \\sum_{i=1}^N \\alpha_i y_i K(x_i, x) + b) - y_i$, \\quad i = 1,2$$ 此最优问题的解是： $$\\alpha_2^{new} = \\alpha_2^{old} + y_2 \\frac {(E_1 - E_2)} \\eta$$ 其中， $$\\eta = K_{11} + K_{22} - 2K_{12} = {||\\phi(x_1) - \\phi(x_2)||}^2$$ $\\phi(x)$为输入空间到特征空间的映射，经过剪辑后是 $$f(n)=\\begin{cases}H,\\quad \\alpha_2^{new} &gt; H \\\\\\alpha_2^{new}, \\quad L \\le \\alpha_2^{new} \\le H \\\\L,\\quad \\alpha_2^{new} &lt; L \\end{cases}$$ 则$\\alpha_1^{new}$为 $$\\alpha_1^{new} = \\alpha_1^{old} + y_1 y_2 (\\alpha_2^{old} - \\alpha_2^{new})$$ 变量的选择方法SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。 1.第1个变量的选择 SMO算法在外层循环中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量，KKT条件如下 $$\\alpha_i = 0 &lt;=&gt; y_i g(x_i) \\ge 1 \\\\0 &lt; \\alpha_i &lt; C &lt;=&gt; y_i g(x_i)=1 \\\\\\alpha_i = C &lt;=&gt; y_i g(x_i) \\le 1$$ 其中，$g(x_i) = \\sum_{j=1}^N \\alpha_j y_j K(x_i,x_j)+b$。 该检验在$\\epsilon$范围内进行的，在校验过程中，外层循环首先遍历所有满足条件$0&lt;\\alpha_i&lt;C$的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否满足KKT条件。 2.第2个变量的选择 SMO算法在内层循环，假设在外层循环中已经找到第一个变量$\\alpha_1$，现在要在内层循环中找到第2个变量$\\alpha_2$，第2个变量选择的标准是希望能使$\\alpha_2$有足够的变化。根据上一节可知，$\\alpha_2^{new}$是依赖$|E_1 - E_2|$的，为了加快计算速度，最简单的做法是选择$|E_1 - E_2|$最大的（如果$E_1$为负值，则选择最大的$E_i$作为$E_2$，否则选择最小的$E_i$为$E_2$，需要保存所有的$E_i$）。 3.计算阈值b和差值$E_i$ 在每次完成两个变量优化后，都要重新计算阈值b。 由KKT条件得 $$\\sum_{i=1}^N \\alpha_i y_i K_{i1} + b = y_i$$ 从而 $$b_1^{new} = y_1 - \\sum_{i=3}^N \\alpha_i y_i K_{i1} - \\alpha_1^{new} y_1 K_{11} - \\alpha_2^{new} y_2 K_{21}$$ 由于$E_i = g(x_i) - y_i = ( \\sum_{i=1}^N \\alpha_i y_i K(x_i, x) + b) - y_i$, \\quad i = 1,2$，则 $$E_1 = g(x_1) - y_1 = \\sum_{i=3}^N \\alpha_i y_i K_{i1} + \\alpha_1^{old} y_1 K_{11} + \\alpha_2^{old} y_2 K_{21} + b^{old} - y_1$$ 将上式中的$y_i - \\sum_{i=3}^N \\alpha_i y_i K_{i1} $代入$b_1^{new}$的公式中，得到 $$b_1^{new} = -E_1 - y_1 K_{11} (\\alpha_1^{new} - \\alpha_1^{old} ) - y_2 K_{21} (\\alpha_2^{new} - \\alpha_2^{old} ) + b^old$$ 对于b的取值： $$b^{new}=\\begin{cases}b_1^{new}=b_2^{new}, \\quad 0 &lt; \\alpha_i^{new} &lt; C, i =1,2 \\\\\\frac {b_1^{new} + b_2^{new}} 2,\\quad \\alpha_i^{new} == 0 or C，满足KKT条件\\end{cases}$$","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://simyy.com/tags/机器学习/"}]},{"title":"SVM-线性可分支持向量机","slug":"ml-svm-1","date":"2015-06-17T14:57:29.000Z","updated":"2017-04-08T09:36:56.000Z","comments":true,"path":"2015/06/17/ml-svm-1/","link":"","permalink":"http://simyy.com/2015/06/17/ml-svm-1/","excerpt":"学习支持向量机。","text":"学习支持向量机。 函数间隔和几何间隔给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为 $$w^* \\cdot x+b^* =0 $$ 以及相应的分类决策函数 $$f(x) = sign (w^* \\cdot x + b^*)$$ 称为线性可分支持向量机。 对于给定训练集合T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x _i,y_i)$的函数间隔为 $$\\hat\\gamma_i = y_i(w \\cdot x_i + b)$$ 定义超平面$(w,b)$关于训练数据集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔之最小值， $$\\hat\\gamma = \\min_{i=1,…,N}\\hat\\gamma_i$$ 对于给定的训练数据集和超平面$(w,b)$，定义超平面$(w,b)关于$样本$(x_i,y_i)$的几何间隔为 $$\\hat\\gamma_i = y_i(\\frac w{||w||} \\cdot x_i + \\frac b{||w||})$$ 定义超平面$(w,b)$关于训练数据集T的几何间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的几何间隔之最小值 $$\\gamma = \\min_{i=1,…,N}\\gamma_i$$ 从而得到几何间隔和函数间隔的关系: $$\\gamma = \\frac{\\hat\\gamma_i}{||w||}$$ 间隔最大化对数据集合找到几何间隔最大的超平面意味着以充分大的确信度来对训练数据进行分类。 最大化超平面可表示为： $$\\max_{w,b} \\quad {\\gamma}\\\\s.t.\\quad y_i(\\frac w{||w||} \\cdot x_i + \\frac b{||w||}) \\ge \\gamma,\\quad i=1,…,N$$ 即最大化超平面$(w,b)$关于训练结合的间隔$\\gamma$，约束条件表示的超平面$(w,b)$关于每个训练样本点的几何间隔至少为$\\gamma$。 而函数间隔对于上述公式并没有影响，假设按比例改变为$\\lambda w$和$\\lambda b$，那么函数间隔改变为$\\lambda \\hat\\gamma$ 改变为相应的函数距离，如下 $$\\max_{w,b} \\quad \\frac{\\hat\\gamma}{||w||} \\\\s.t.\\quad y_i(w \\cdot x_i + b) \\ge \\hat\\gamma,\\quad i=1,…,N$$ 由于分母和分子同时拥有$\\lambda$，因此成比例改变并不会对函数间隔产生影响，从而对目标函数的优化也没有影响。 令$\\hat\\gamma$=1，代入上式，最大化$\\frac1{||w||}$等价于最小化$\\frac12||w||$，从而得到线性可分支持向量机学习的最优化问题 $$min_{w,b}\\quad \\frac12{||w||}^2 \\\\s.t.\\quad y_i(w\\cdot x_i + b) - 1 \\ge 0, \\quad i=1,2,…,N$$ 这是一个凸二次规划问题。 支持向量在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector），即 $$y_i(w\\cdot x_i+b) =1$$ 对于y=+1的正例来说，支持向量在超平面 $$H_1:w\\cdot x + b= 1$$ 对于y=-1的负例来说，支持向量在超平面 $$H_2:w\\cdot x + b = -1$$ 如图中， H1和H2平行，之间形成一条长带，其宽度为$\\frac 2 {||w||}$。在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，如果移动支持向量改变所求的解，但是如果在间隔边界（H1和H2）以外移动其他实例点，解都不会发生改变。 对偶算法为了求解线性可分支持向量机的最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到最优解。 定义拉格朗日函数： $$L(w,b,\\alpha) = \\frac 1 2 {||w||}^2 - \\sum_{i=0}^n \\alpha_i y_i(w\\cdot x_i + b) + \\sum_{i=1}^N\\alpha_i$$ 其中，$\\alpha = (\\alpha_1, \\alpha_2,…,\\alpha_N)^T$为拉格朗日乘子向量。 根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题需要先求$L(w,b,\\alpha)$对(w,b)求极小，再对$\\alpha$求极大： $$\\max_\\alpha \\min_{w,b} L(w,b,\\alpha)$$ $\\min_{w,b} L(w,b,\\alpha)$ 分别对$w,b,\\alpha$求偏导数，并令其等于0，将结果带入原公式中即得 $$\\min_{w,b} L(w,b,\\alpha) = -\\frac 1 2 \\sum_{i-=1}^N \\sum_{j-=1}^N \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i=1}^N \\alpha_i$$ 求$\\min_{w,b} L(w, b, \\alpha)$对$\\alpha$的极大 $$\\max_\\alpha -\\frac 1 2 \\sum_{i-=1}^N \\sum_{j-=1}^N \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i=1}^N \\alpha_i \\\\s.t.\\quad \\sum_{i=1}^N\\alpha_i y_i=0, \\quad \\alpha_i &gt; 0,i=1,2,…,N$$ 等价于： $$\\min_\\alpha \\frac 1 2 \\sum_{i-=1}^N \\sum_{j-=1}^N \\alpha_i \\alpha_j y_i y_j(x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha_i \\\\s.t.\\quad \\sum_{i=1}^N\\alpha_i y_i=0, \\quad \\alpha_i &gt; 0, i=1,2,…,N$$ 线性可分支持向量机学习算法(1)构造并求解约束最优化问题 $$\\min_\\alpha \\frac 1 2 \\sum_{i-=1}^N \\sum_{j-=1}^N \\alpha_i \\alpha_j y_i y_j(x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha_i\\\\s.t.\\quad \\sum_{i=1}^N\\alpha_i y_i=0, \\quad \\alpha_i &gt; 0, i=1,2,…,N$$ (2)计算 $$w^* = \\sum_{i=1}^N \\alpha_i^* y_i x_i$$ 并选择$\\alpha^*$的一个正分量$\\alpha_j^*$，计算 $$b^* = y_i - \\sum_{i=1}^N \\alpha_i^*y_i(x_i \\cdot x_j)$$ (3)求得分离超平面 $$w^* \\cdot x + b^* = 0$$ 分类决策函数 $$f(x) = sign(w^* \\cdot x + b^*)$$","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://simyy.com/tags/机器学习/"}]},{"title":"python-多线程","slug":"python-thread","date":"2015-06-02T17:03:01.000Z","updated":"2017-04-08T09:27:11.000Z","comments":true,"path":"2015/06/03/python-thread/","link":"","permalink":"http://simyy.com/2015/06/03/python-thread/","excerpt":"介绍python中的多线程。","text":"介绍python中的多线程。 真正的多线程吗？对于多核处理器，在同一时间确实可以多个线程独立运行，但在Python中确不是这样的了。原因在于，python虚拟机中引入了GIL这一概念。GIL（Global Interpreter Lock）全局解析器锁是用来解决共享资源访问的互斥问题，导致在python虚拟机中同一时间只能有一个线程访问python所提供的API。 那么python是如何支持多线程的呢？ 在操作系统中系统通过时钟中断进行进程的调度，而python正是参考这个原理。在python内部维护了一个内部的时钟，来记录每个线程每个时钟周期执行命令的数量。 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getcheckinterval() #获取一个始终周期内执行指令数100 当一个线程获得了python虚拟机的GIL后可以按顺序执行100条指令，然后挂起当前进程，切换下一个等待执行的线程。 那么python如何选择下一个需要执行的线程呢？ python并没有去实现一个线程优先级调度算法，而是将线程选择问题交给了底层的操作系统，也就是说python借用了底层操作系统所提供的线程调度机制来决定下一个执行的线程。 因此，python使用的就是操作系统原生的线程，只是python在其基础之上提供了一套统一的抽象机制。 线程切换在操作系统中，进程之间的切换需要不断保存和恢复进程之间的上下文环境，保证每一个进行都能在其对应的上下文环境中运行。python正是参考操作系统的切换机制，为每一个线程创建一个保存线程状态信息的PyFrameObject对象。在python中有有一个全局变量PyThreadState *_PyThreadState_Current用来保存当前活动线程的线程状态对象。 下一线程切换需要的线程状态如何获取？ 在python中通过一个单项链表来管理所有的python线程对象(保护线程的状态信息和线程信息，例如线程id)，当需要寻找一个线程对应的状态对象时，就遍历这个链表，搜索其对应的状态对象。 这个状态对象链表并不会受到GIL的保护，而是有其专用的锁。 需要注意 当前活动的python线程不一定是获得了GIL的线程，例如“主线程获得了GIL，子线程还没有申请到GIL时也没有挂起，而且主线程和子线程都是操作系统原生的线程，操作系统可能在主线程和子线程之间进行切换（操作系统的线程切换是不受python虚拟机控制的，属于操作系统自身行为）”。python虚拟机的调度是一定是获得GIL基础之上的，而操作系统级的就不一定获得GIL了。 虽有操作系统会把未获得GIL的线程切换为活动线程，但是该线程发现自身并没有获得GIL会自动挂起。 只有当所有线程都完成了初始化操作，操作系统的线程调度和Python线程调度才会一致。那时，python的线程调度会迫使当前活动线程释放GIL，导致触发GIL中维护的Event内核对象，从而触发操作系统的线程调度。（在初始化完成之前，python线程调度和操作系统调度之间没有因果关系） 阻塞调度和线程销毁在python中如果有raw_input等待输入的操作时将自身阻塞后，并将等待GIL线程唤醒，这种情况成为阻塞调度。 在线程通过阻塞调度切换时，python内部的时钟周期技术_Py_Ticker依然会被保持，不会被重置。 python的主线程销毁和子线程销毁是不同的，子线程只需要维护引用计数，而主线程还需要销毁运行环境。 用户级互斥和同步上面讨论的GIL属于python内合计互斥，实现了保护内存的共享资源。而用户级互斥保护了用户程序中的共享资源。 python中提供了lock机制来实现线程之间的互斥。当线程通过lock.acquire获得lock之后，子线程会因为等待lock而挂起，直到主线程释放lock之后才会被Python的线程调度机制唤醒。 在线程执行过程中如果出现需要等待另一个lock资源的时候，需要将GIL转交给其他等待GIL的线程以避免死锁。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python virtualenv","slug":"python-virtualenv","date":"2015-05-28T16:07:37.000Z","updated":"2017-04-08T09:27:06.000Z","comments":true,"path":"2015/05/29/python-virtualenv/","link":"","permalink":"http://simyy.com/2015/05/29/python-virtualenv/","excerpt":"virtualenv 是一个创建隔离的Python环境的工具，创建独立的python运行环境。","text":"virtualenv 是一个创建隔离的Python环境的工具，创建独立的python运行环境。 安装任选一种安装方式： 12sudo apt-get install python-virtualenvsudo pip install virtualenv 操作创建和删除创建虚拟环境时可以添加--python来指定虚拟环境的python版本1234virtualenv mysitevirtualenv mysite --python=python3.4rmvirtualenv mysite 运行命令后会在当前文件夹下生成mysite文件夹，其中包含着最基本的开发库。 运行虚拟环境12cd mysitesource ./bin/activate 启动虚拟环境后，可以尝试在虚拟环境内安装一个bottle， 1234(mysite)y@ubuntu:~/mysite$ pip install bottle ...#进入python命令行&gt;&gt;&gt; import bottle 成功安装，import模块并没有报错，注意当前是在虚拟环境下安装和运行的结果。 退出虚拟环境，同样在python的命令行下执行import bottle，会发现bottle模块并不存在。这种情况就是virtualenv所谓的隔离效果。 退出虚拟环境1deactivate 总结virtualenv提供了一种隔离不同Python版本。 virtualenv创建的虚拟环境不会对除自己之外的其他环境造成影响，从而可以保证一个纯净的开发测试环境。 virtualenv创建的虚拟环境可以直接打包移动到其他机器（需要安装有同样的python版本）上运行，其所运行所需要的开发库（除python基本开发库）均包含在虚拟环境内。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 模块加载","slug":"python-module-init","date":"2015-05-27T14:10:16.000Z","updated":"2017-04-08T09:27:42.000Z","comments":true,"path":"2015/05/27/python-module-init/","link":"","permalink":"http://simyy.com/2015/05/27/python-module-init/","excerpt":"本文主要介绍python模块加载的过程。","text":"本文主要介绍python模块加载的过程。 module的组成所有的module都是由对象和对象之间的关系组成。 #type和objectpython中所有的东西都是对象，分为三类：类型type、类class和实例instance。 三种对象之间的两种关系： is kind of，基类和子类的关系 is instance of，类和对象的关系 类和对象的关系可以通过内建方法type来辨别。 python中，任何一个类都是直接或间接继承自object，而每一个对象都会拥有自己的type类型，可以通过__class__属性获得。 运行环境初始化python首先需要加载多个基础的module，例如__builtin__,sys等，同时也会完成python类型系统的初始化和异常系统的初始化。 系统module初始化Python内部维护了一个维护加载到内存的module集合，需要现在集合中查找module是否存在。如果存在直接返回该module对象，否则创建该module对象，并插入到module集合中。创建module后，需要设置module的属性。 由于python的module集合是一个PyDictObject对象，而PyDictObject对象在Python中是一个可变对象，所以其中维护的元素有可能在运行时被删除。对于Python的扩展module，例如sys，为了避免再一次初始化同样的module，python将所有的扩展module通过一个全局的PyDictObject对象来进行备份维护。 启动虚拟机python包括两种运行方式：命令行和脚本文件。 python中的run_mode函数基于AST抽象语法树 (AST, Abstract Syntax Tree)完成了字节码的编译工作，并创建PyCodeObject对象。 python中所有的线程都是共享同样的builtin名字空间。 模块的动态加载import功能包括： python运行时的全局module缓存的维护和搜索； 解析和搜索module路径的树形结构； 对不同文件格式的module动态加载机制。 py文件中的import不会影响上一层的命名空间，只会影响各自的命名空间，也就是影响各自module自身维护的那个dict对象。但所有的import操作都会影响全局的module集合，这样的话只要再一次import该module，python虚拟机只需要将全局module集合中缓存的对象返回。 在module的基础之上，python提供了package机制（逻辑相关联的module需要聚合到同一package中）。也就是说通过package机制来管理module，通过module来管理class。 文件件中存在__init__.py文件才能成为package（多个py文件组成的文件夹）。 在加载package下的module时，例如a.b.c，python内部将这个module视为一个树形结构，c是b的子节点，b是a的子节点。python虚拟机在动态加载时，需要将这个树形结构分解，然后从左到右依次去sys.modules中查找每一个符号是否存在。如果已经存在，假设存在a，那么在a对应的PyModuleObject对象中保持着__path__路径信息，此时就可以在a.__path__路径中搜索b和c了。 del删除模块只是把模块从当前命名空间中删除，但该module依然存在于module缓存中。 module缓存python中的全局module集合sys.modules被称为modules缓存，保证了module的唯一性，每当有import操作都会在该sys.modules查找，如果不存在就会将该module加入到sys.modules中。 如果已经加载的模块发生改变，那么需要调用reload函数来重新加载该模块。需要注意reload函数并不会重新创建该对象，而在在原有对象的基础上做修改。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 名字、作用域和命名空间","slug":"python-name-space","date":"2015-05-27T14:02:16.000Z","updated":"2017-04-08T09:27:36.000Z","comments":true,"path":"2015/05/27/python-name-space/","link":"","permalink":"http://simyy.com/2015/05/27/python-name-space/","excerpt":"在python中包括三个独立的命名空间：local、global和builtin，所有信息保存在PyFrameObject中。","text":"在python中包括三个独立的命名空间：local、global和builtin，所有信息保存在PyFrameObject中。 访问PyFrameObjectPyFrameObject对象是一个用于python虚拟机实现的内部对象，对应于可执行文件在执行时的栈帧，但python提供了getframe方法来访问该对象。 12345678import sysframe = sys._getframe()print frame.f_code.co_name # 函数名称print frame.f_locals #局部变量print frame.f_globals #全局变量 module与命名空间py文件可以视为一个module，它的作用不仅是为了代码复用，而且可以为整个系统划分命名空间。 在python中，赋值语句会影响命名空间。由于命名空间和变量函数的约束关系，python中采用PyDictObject对象来存储该约束关系。 在一个module的内部，可能存在多个命名空间，每一个命名空间都与一个作用域对应。作用域是指一段程序的正文区域，在这个区域里可能存在多个约束条件，但除了这个区域约束条件就不起作用了。在python中，一个约束条件是否起作用，是由该约束在文本中的位置决定的，而不是动态决定，因此，python是具有静态作用域。 作用域在python中，一个函数定义了一个Local作用域；一个module定义了一个global作用域；python自身定义的作用域builtin作用域（内建函数dir,open,range等）。 名字引用是指在命名空间中查找一个名字所引用的对象，不允许越过module边界。属性引用是指到对象的命名空间查找名字。 嵌套函数的命名空间12345def compare(base, value): return value &gt; basecompare(10, 5)compare(10, 20) 上面的例子中，会使用两次10来进行比较，python中提供嵌套的方法只需要设置一次10即可。 1234567def get_compare(base): def real_compare(value): return value &gt; basecompare_with_10 = get_compare(10)print compare_with_10(5) #Falseprint compare_with_10(20) #True 上面的例子形成了一个闭包，是将名字空间和函数捆绑后的结果。 12345678def get_compare(base): def real_compare(value, base=base): return value &gt; basecompare_with_10 = get_compare(10)print compare_with_10(5) #Falseprint compare_with_10(20) #Trueprint compare_with_10(5, 1) #True 线程与module在python中，所有module是全局共享的。 pyton虚拟机是对CPU的抽象，利用不同线程轮流使用虚拟机来实现多线程。 python中使用GIL（Global Interpreter Lock）来实现线程同步。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 对象","slug":"python-object","date":"2015-05-24T14:59:59.000Z","updated":"2017-04-08T09:27:31.000Z","comments":true,"path":"2015/05/24/python-object/","link":"","permalink":"http://simyy.com/2015/05/24/python-object/","excerpt":"在python中，对象就是为C中的结构体在堆上申请的一块内存，一般来说，对象是不能被静态初始化的，并且不能再栈空间上生存。本文主要对Python的基本数据类型做简单的介绍。","text":"在python中，对象就是为C中的结构体在堆上申请的一块内存，一般来说，对象是不能被静态初始化的，并且不能再栈空间上生存。本文主要对Python的基本数据类型做简单的介绍。 PyObject在python中，所有东西都是对象，而所欲的对象都拥有一些共性（object.h/PyObject）。PyObject是整个python对象机制的核心。 123typedef struct _object &#123; PyObject_HEAD&#125; PyObject; 在release模式下编译python时，PyObject如下：1234typedef struct _object &#123; int ob_refcnt; //引用计数 struct _typeobject *ob_type; //类型&#125; PyObject; 其中，ob_refcnt为int整型，实现了基于引用计数的垃圾收集机制。对于某一对象A，当有一个新的PyObject引用该对象时，A的引用计数应该增加；而当这个PyObject被删除时，A的引用计数应该减少；当A的引用计数减少到0时，A就可以从堆上被删除，以释放出内存供别的对象使用。 引用计数PyObject中的ob_refcnt是一个32位的整形变量，这实际蕴含着Python所有的一个假设，既对一个对象的引用不会超过一个整形变量的最大值。 整数对象小整数对象在python中，所有对象都存活在堆上，python重复地使用malloc申请空间，大大降低了运行效率，造成大量内存碎片，影响整体性能。因此，在python中，对小整数对象使用了对象池技术，用来缓存所有的小整形对象（对重复的对象不需要重复的malloc）。 NSMALLPOSINTS和NSMALLNEGINTS来修改小整数对象池的范围（默认分别为257和-5）。 大整数对象对于小整数，使用对象池技术完全缓存其PyIntObject对象，而对于其他整数，python提供了一种PyIntBlock结构供大整形对象使用。PyIntBlock是通过维护一块内存（Block）来供大整数使用，并通过单向列表block_list来维护。 Python的设计者为了提高代码执行效率放弃了类型安全使用了宏来代替函数。 当一个PyIntObject对象被销毁时，它所占用的内存并不会被释放，而是继续被python保留着，加入到free_list所维护的自有内存链表，为其他需要创建对象的内存使用。 字符串对象在python中，PyStringObject是字符串对象的实现，它是一个拥有可变长度内存的对象。 123456trpdef struct &#123; PyObject_VAR_HEAD //ob_size字符串长度 long ob_shash; //字符串hash值 int ob_sstate; char ob_sval[1];&#125; PyStringObject; 在PyStringObject中，还使用了intern机制和缓存池技术。 ###intern机制和缓存池 intern机制的目的：保证被intern之后的字符串在python整个运行期间只对应唯一的一个PyStringObject对象。 intern机制的关键是在系统中有一个（key,value）映射的集合，记录所有被intern机制处理过的PyStringObject对象。当python在创建一个字符串时，会首先在interned中检查是否已经有该字符串对应的PyStringObject对象了，如果有，则不用创建新的，这样可以节省内存空间。其实，Python始终会为字符串创建PyStringObject对象，intern机制是在创建之后才会生效的，通常python在运行时创建一个PyStrhingObject对象temp后，基本就会销毁该对象（引用计数减1）。 类似小整形的缓存池，python为PyStringObject中的一个字节的字符对应的PyStringObject对象设计对象池characters。 Python中的处理顺序： 判断是否为一个字符 创建PyStringObject对象进行intern操作 将intern结果缓存到字符缓冲池中 ###PyStringObject效率问题 在Python中”+”操作符进行字符串连接的方法效率极其低下，其根源在于python中的PyStringObject对象是一个不可变对象。这就意味着当进行字符串连接时，实际上必须要创建一个新的PyStringObject对象。因此，如果需要连接N个PyStringObject对象，就必须进行N-1次内存申请及内存搬运工作，严重影响python的执行效率。 官方推荐利用PyStringObject对象的join操作来对存储在list和tuple中一组PyStringObject对象进行连接操作，只需要分配一次内存，执行效率大大提高。 join操作会统计出list中所有PyStringObject对象的字符串长度，然后申请内存。 ##列表对象 PyListObject是Python提供的对列表的抽象。它类似于C++中的vector，而不是list。 12345typedef struct &#123; PyObject_VAR_HEAD PyObject **ob_item; //元素首地址 int allocated; //实际申请内存的个数（部分没有被使用，类似于vector）&#125; PyListObject; 其中，ob_item为指向元素列表的指针。 python所采用的内存管理策略和c++中vector采取的内存管理策略一样，并不是寸多少数据就要申请对应大小的空间，这样内存管理的效率较低，因此，在每一次申请内存时PyListObject总会申请一大块内存，该内存的大小就记录在allocated中，而实际被使用的内存数量记录在ob_size中。 ###对象缓存池 在创建一个新的list时，首先创建PyListObject对象，然后创建PyListObject对象所维护的元素列表；在销毁一个List时，首先销毁PyListObject所维护的列表，然后释放掉PyListObject本身，但是在释放之前python会检查缓冲池free_lists，查看其中缓存的PyListObject的数量是佛已经满了，如果没有，就将该数据对象加入到穿存池中，否则删除。 ##Dict对象 与map不同，PyDictObject采用散列表(hash table)。在最优情况下，散列表能提供O（1）复杂度的搜索效率。 ###散列表 散列表的基本思想是通过一个函数将需搜索的键值映射为一个整数，将这个整数视为索引值访问某片连续性的内存区域。 在使用散列表的过程中，不同的对象经过散列函数的作用，可能被映射为相同的散列值。随着需要存储的数据的增多，这样的冲突就会发生得越来越频繁。散列冲突是散列表不可避免的问题，当散列表的装载率（已使用空间和总空间的比值）大于2/3时，散列冲突发生的概率就会大大增加。 在STL库的hash table采用开链法来解决冲突，而python中采用开放地址法。当产生冲突时，python会通过一个二次探测函数f计算下一个候选位置，如果该候选位置可用，则可将待插入元素放到该位置，否则再次调用探测函数f，寻找可用位置。 ###关联容器entry 12345typedef struct &#123; Py_ssize me_hash; //散列值 PyObject *me_key; PyObject *me_value;&#125; 在PyDictObject对象生存变化的过程中，其中entry会在不同的状态间转换：Unused、Active和Dummy。 Unused：当一个entry的me_key和me_value都是NULL； Active：当entry中存储了一个(key，value)对时； dummy：当entry中存储的（key,value）被删除后，entry的状态不能直接从Active转到unused（伪删除），否则会出现冲突探测链中断。","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"RabbitMQ 远程连接","slug":"rabbitmq-remote-access","date":"2015-04-26T13:25:10.000Z","updated":"2017-04-08T09:26:29.000Z","comments":true,"path":"2015/04/26/rabbitmq-remote-access/","link":"","permalink":"http://simyy.com/2015/04/26/rabbitmq-remote-access/","excerpt":"解决rabbitmq远程连接的问题。","text":"解决rabbitmq远程连接的问题。默认情况下，rabbitmq使用guest来连接本地（localhost）的server，当需要远程连接时，就会失效。 &quot;guest&quot; user can only connect via localhost 官方文档：http://www.rabbitmq.com/access-control.html 如果必须使用guest用户来进行远程登录，需要修改配置 1[&#123;rabbit, [&#123;loopback_users, []&#125;]&#125;]. （1）那么首先需要创建并添加一个用户test，让其具有管理员权限 rabbitmqctl add_user rootroot rabbitmqctl set_user_tags rootadministrator rabbitmqctl set_permissions -p / root”.“ “.“ “.*” （2）修改配置文件1[&#123;rabbit, [&#123;loopback_users, [&quot;root&quot;]&#125;]&#125;]. （3）重启rabbitmq-server 1/etc/init.d/rabbitmq-server restart （4）修改host 修改远程客户端机器上的/etc/hosts，添加rabbit-server的IP 1xx.xx.xx.xx rabbit-server （5）认证 原文：https://pika.readthedocs.org/en/0.9.14/modules/parameters.htmlpika提供了两种认证方式：ConnectinParameters和URLParameters。 ConnectionParameters 12345678import pika# Set the connection parameters to connect to rabbit-server1 on port 5672# on the / virtual host using the username \"guest\" and password \"guest\"credentials = pika.PlainCredentials('root', 'root')parameters = pika.ConnectionParameters('rabbit-server1', 5672, '/', credentials) URLParameters 1234import pika# Set the connection parameters to connect to rabbit-server1 on port 5672# on the / virtual host using the username \"guest\" and password \"guest\"parameters = pika.URLParameters('amqp://guest:guest@rabbit-server1:5672/%2F') 例子import pika i = 1 def callback(ch, method, properties, body): global i #print 'receive %r'%body print 'receive %s'%i i += 1 f = open('%s'%i, 'w+') f.write(body) f.close() #第一种方法 #credentials = pika.PlainCredentials('mtest', 'root') #connection = pika.BlockingConnection(pika.ConnectionParameters('rabbit-server', 5672, '/', credentials)) #第二种方法 parameters = pika.URLParameters('amqp://mtest:root@rabbit-server:5672/%2F') connection = pika.BlockingConnection(parameters) channel = connection.channel() channel.queue_declare(queue='hello') channel.basic_consume(callback, queue='hello1', no_ack=True) channel.start_consuming()","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"memcache 内存管理","slug":"memcache-memory-management","date":"2015-04-26T10:46:10.000Z","updated":"2017-04-08T09:29:25.000Z","comments":true,"path":"2015/04/26/memcache-memory-management/","link":"","permalink":"http://simyy.com/2015/04/26/memcache-memory-management/","excerpt":"memcached使用预申请的方式来管理内存的分配，从而避免内存碎片化的问题。如果采用mallo和free来动态的申请和销毁内存，必然会产生大量的内存碎片。","text":"memcached使用预申请的方式来管理内存的分配，从而避免内存碎片化的问题。如果采用mallo和free来动态的申请和销毁内存，必然会产生大量的内存碎片。 基本知识slab：内存块是memcached一次申请内存的最小单元，在memcached中一个slab的默认大小为1M； slabclass：特定大小的chunk的组。 chunk：缓存的内存空间，一个slab被划分为若干个chunk； item：存储数据的最小单元，每一个chunk都会包含一个item； factor:增长因子，默认为1.25，相邻slab中的item大小与factor成比例关系； 基本原理memcached使用预分配方法，避免频繁的调用malloc和free； memcached通过不同的slab来管理不同chunk大小的内存块，从而满足存储不同大小的数据。 slab的申请是通过在使用item时申请slab大小的内存空间，然后再把内存切割为大小相同的item，挂在到slab的未使用链表上。 过期和被删除item并不会被free掉，memcached并不会删除已经分配的内存； Memcached会优先使用已超时的记录空间，通过LRU算法； memcached使用lazy expiration来判断元素是否过期，所以过期监视上不会占用cpu时间。 源码分析下面主要分析memcached的内存申请和存储相关代码。 itemitem是key/value的存储单元。 12345678910111213141516171819typedef struct _stritem &#123; struct _stritem *next; /* 前后指针用于在链表slab-&gt;slots中连接前后数据 */ struct _stritem *prev; struct _stritem *h_next; /* hash chain next */ rel_time_t time; /* 最后一次访问时间 */ rel_time_t exptime; /* 过期时间 */ int nbytes; /* 数据大小 */ unsigned short refcount; /* 引用次数 */ uint8_t nsuffix; /* suffix长度 */ uint8_t it_flags; /* ITEM_* above */ uint8_t slabs_clsid;/* 所有slab的id */ uint8_t nkey; /* key长度 */ /* this odd type prevents type-punning issues when we do * the little shuffle to save space when not using CAS. */ union &#123; uint64_t cas; char end; &#125; data[]; /* cas|key|suffix|value */&#125; item; slab初始化12345678910111213141516171819202122232425262728void slabs_init(const size_t limit, const double factor, const bool prealloc) &#123; int i = POWER_SMALLEST - 1; unsigned int size = sizeof(item) + settings.chunk_size; /* 得到每一个item的大小 */ mem_limit = limit; if (prealloc) &#123; /* 预分配一块内存 */ ... &#125; memset(slabclass, 0, sizeof(slabclass)); /* 把slabclass置为0，slabclass是一个slab数组，存储所有slab的信息 */ while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) &#123; /* 循环初始化每一个slab的内容,保证slab中item的size小于max_size/factor */ /* Make sure items are always n-byte aligned */ if (size % CHUNK_ALIGN_BYTES) /* 用于内存对齐 */ size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES); slabclass[i].size = size; /* 初始化slabclass中item的大小 */ slabclass[i].perslab = settings.item_size_max / slabclass[i].size; /* 初始化每个slab中item的数量 */ size *= factor; /* item的大小随factor逐渐增大 */ ... &#125; /* 初始化最后一个slab,大小为最大的max_size，只有一个item */ power_largest = i; slabclass[power_largest].size = settings.item_size_max; slabclass[power_largest].perslab = 1; ...&#125; 从源码中，可以看出来同一个slab中所有的item的大小都是固定的， 申请slab内存1234567891011121314151617181920212223242526272829static void *do_slabs_alloc(const size_t size, unsigned int id) &#123; slabclass_t *p; void *ret = NULL; item *it = NULL; if (id &lt; POWER_SMALLEST || id &gt; power_largest) &#123; /* 判断id是否合法 */ MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0); return NULL; &#125; p = &amp;slabclass[id]; /* 获取slab */ assert(p-&gt;sl_curr == 0 || ((item *)p-&gt;slots)-&gt;slabs_clsid == 0); /* fail unless we have space at the end of a recently allocated page, we have something on our freelist, or we could allocate a new page */ if (! (p-&gt;sl_curr != 0 || do_slabs_newslab(id) != 0)) &#123; /*如果sl_curr为0，没有剩余的item，那么就执行do_slabs_newslab申请内存空间*/ /* We don't have more memory available */ ret = NULL; &#125; else if (p-&gt;sl_curr != 0) &#123; /* 如果有未使用的空间，则获取该item，并从slots链表中删除该item */ /* return off our freelist */ it = (item *)p-&gt;slots; p-&gt;slots = it-&gt;next; if (it-&gt;next) it-&gt;next-&gt;prev = 0; p-&gt;sl_curr--; ret = (void *)it; &#125; ... return ret;&#125; sl_curr来判断是否存在未使用的内容空间，如果不存在需要调用do_slabs_newslab来申请slab空间。 1234567891011121314151617181920212223242526static int do_slabs_newslab(const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; int len = settings.slab_reassign ? settings.item_size_max : p-&gt;size * p-&gt;perslab; char *ptr; /* 1. 判断是否超过内存限制 2. 判断是否申请过内存空间 3. 如果没有申请过，则申请slab-&gt;size*slab-&gt;perslab大小的整块内存 4.如果申请过，调用grow_slab_list来扩大slab大小 */ if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) || (grow_slab_list(id) == 0) || ((ptr = memory_allocate((size_t)len)) == 0)) &#123; MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id); return 0; &#125; memset(ptr, 0, (size_t)len); split_slab_page_into_freelist(ptr, id); /* 把申请的内存分配到slots链表中 */ p-&gt;slab_list[p-&gt;slabs++] = ptr; mem_malloced += len; MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id); return 1;&#125; 申请空间后，需要通过split_slab_page_into_freelist函数把申请的内存空间分配到未使用的链表中。 1234567891011121314151617181920212223242526static void split_slab_page_into_freelist(char *ptr, const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; int x; for (x = 0; x &lt; p-&gt;perslab; x++) &#123; /* 循环分配内存 */ do_slabs_free(ptr, 0, id); ptr += p-&gt;size; &#125;&#125;static void do_slabs_free(void *ptr, const size_t size, unsigned int id) &#123; slabclass_t *p; item *it; ... p = &amp;slabclass[id]; /* 获取内存指针，把item块挂在到slots链表中，增加sl_curr */ it = (item *)ptr; it-&gt;it_flags |= ITEM_SLABBED; it-&gt;prev = 0; it-&gt;next = p-&gt;slots; if (it-&gt;next) it-&gt;next-&gt;prev = it; p-&gt;slots = it; p-&gt;sl_curr++; p-&gt;requested -= size; return;&#125; 获取适当大小的item在do_item_alloc中，调用了slabs_clsid来获取适合存储当前元素的slab id。 12345678910unsigned int slabs_clsid(const size_t size) &#123; int res = POWER_SMALLEST; if (size == 0) return 0; while (size &gt; slabclass[res].size) /* 遍历slabclass来找到适合size的item */ if (res++ == power_largest) /* won't fit in the biggest slab */ return 0; return res;&#125; 优缺点内存预分配可以避免内存碎片以及避免动态分配造成的开销。 内存分配是由冗余的，当一个slab不能被它所拥有的chunk大小整除时，slab尾部剩余的空间就会被丢弃。 由于分配的是特定长度的内存，因此无法有效地利用所有分配的内存，例如如果将100字节的数据存储在128字节的chunk中，会造成28字节的浪费。","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"memcache","slug":"memcache","permalink":"http://simyy.com/tags/memcache/"}]},{"title":"Python实现web框架simfish","slug":"simfish-framework","date":"2015-04-26T04:29:12.000Z","updated":"2017-04-08T09:25:12.000Z","comments":true,"path":"2015/04/26/simfish-framework/","link":"","permalink":"http://simyy.com/2015/04/26/simfish-framework/","excerpt":"本文主要记录本人利用Python实现web框架simfish的过程。源码github地址：simfish","text":"本文主要记录本人利用Python实现web框架simfish的过程。源码github地址：simfish WSGI HTTP Serverwsgi模块提供了简单的simple_server， 1wsgiref.simple_server.make_server(host, port, app, server_class=WSGIServer, handler_class=WSGIRequestHandler) 官方提供的例子， 12345678910from wsgiref.simple_server import make_server, demo_apphttpd = make_server(&apos;&apos;, 8000, demo_app)print &quot;Serving HTTP on port 8000...&quot;# Respond to requests until process is killedhttpd.serve_forever()# Alternative: serve one request, then exithttpd.handle_request() 访问http://127.0.0.1:8000来检查是否正常运行。 因此，有了wsgi的帮助，我们只需要实现我们自己的demo_app了。 demo_appdemo_app接受两个参数environ和start_response，其中environ包含包含所有cgi和wsgi的参数变量，startresponse是一个函数，参数为status和headers，返回结果为列表或\\_iter__的可迭代实例。 12345678910def demo_app(environ,start_response): from StringIO import StringIO stdout = StringIO() print &gt;&gt;stdout, \"Hello world!\" print &gt;&gt;stdout h = environ.items(); h.sort() for k,v in h: print &gt;&gt;stdout, k,'=', repr(v) start_response(\"200 OK\", [('Content-Type','text/plain')]) return [stdout.getvalue()] 实现自己的hello_app，替换demo_app即可。 12345def hello_app(environ, start_response): status = '200 OK' response_headers = [('Content-type', 'text/plain')] start_response(status, response_headers) return ['Hello world!\\n'] 更多的基础细节请参考：http://www.tuicool.com/articles/aYBRBz simfish的实现web框架其实并不复杂，仅仅负责请求的分发和处理，让web后台开发变得简单、规范的一种方法。 本框架主要参考了bottle和webpy的源码。 路由使用过web框架的人对路由并不陌生，路由就是用来记录url和callback function的映射关系。 在simfish中实现了三种添加路由的方法（装饰器、一次加载、随时添加），源码如下（具体内容参考注释）： 12345678910111213141516171819202122232425262728293031323334# Routeclass Routes: \"\"\"FrameWork Routes\"\"\" ROUTES = &#123;&#125; #存储所有的url到callback function的映射 @classmethod def add(cls, url, handler): \"\"\"add route and handler to ROUTES\"\"\" if not url.startswith('/'): url = '/' + url if re.match(r'^/(\\w+/)*\\w*$', url): #这里需要使用re模块来获取正确的url cls.ROUTES[url] = handler @classmethod def match(cls, url): #用来寻找url对象的处理函数 \"\"\"match url in ROUTES\"\"\" if not url: return None url = url.strip() route = cls.ROUTES.get(url,None) #从ROUTES中查找结果 return route @classmethod def load_urls(cls, urls): #用于类似webpy中urls加载的方式 for item in urls: cls.add(item[0], item[1])def route(url, **kargs): #这是一个装饰器，@route('/') \"\"\"Decorator for request handler. Same as Routes.route(url, handler).\"\"\" def wrapper(handler): Routes.add(url, handler, **kargs) return handler return wrapper 具体使用方法参考：routing 封装request在demo_app中的参数environ（它是一个字典啊！！！）中包含了request中需要的所有信息，那么我们需要把environ添加到request类中， 12345class Request(threading.local): \"\"\"Represents a single request using thread-local namespace\"\"\" def bind(self, environ): \"\"\"Bind the enviroment\"\"\" self._environ = environ 添加获取请求方方法的方法，使用@propery让method方法可以直接调用，注意保持方法的大写（GET/POST） 1234@propertydef method(self): \"\"\"Returns the request method (GET,POST,PUT,DELETE,...)\"\"\" return self._environ.get('REQUEST_METHOD', 'GET').upper() 如果获取请求参数呢？在django中使用如下的方法， 12request.GET.get('param', '')request.POST.get('param', '') 那么，我们需要把get和post的参数全部添加到一个字典中，在environ中”QUERY_STRING”包含了get的所有参数，而post的参数需要通过”wsgi.input”获取。 123456789101112@propertydef GET(self): \"\"\"Returns a dict with GET parameters.\"\"\" if self._GET is None: raw_dict = parse_qs(self.query_string, keep_blank_values=1) self._GET = &#123;&#125; for key, value in raw_dict.items(): if len(value) == 1: self._GET[key] = value[0] else: self._GET[key] = value return self._GET 其中，parse_qs是解析get参数的，推荐使用urlparse.parse_qs 和 urlparse.parse_qsl，目前cgi的已经废弃但保留是为了向后兼容。 与get请求不同之处，在post请求中需要调用cgi模块的FieldStorage来解析post请求参数。 1raw_data = cgi.FieldStorage(fp=self._environ['wsgi.input'], environ=self._environ) 具体参考源码：simfish.py 封装response在这里主要实现了response-header和response-status的封装。 1234567class Response(threading.local): \"\"\"Represents a single response using thread-local namespace.\"\"\" def bind(self): \"\"\"Clears old data and creates a brand new Response object\"\"\" self.status = 200 self.header = HeaderDict() #继承dict的header类 self.header['Content-type'] = 'text/plain' 继承自threading.local可以保证每个每个线程拥有自己的request和response，并不会相互影响。 实现template这里使用bottle默认的模板，使用方法参考：template 发送文件文件的发送与普通的string返回并不相同。首先，需要判断文件的权限， 123456789if not filename.startswith(root): response.status = 401 return \"Access denied.\" if not os.path.exists(filename) or not os.path.isfile(filename): response.status = 404 return \"File does not exist.\" if not os.access(filename, os.R_OK): response.status = 401 return \"You do not have permission to access this file.\" 获取文件的类型，这里需要使用mimetypes模块， 123456if mimetype: response.header['Content-type'] = mimetypeelif guessmime: guess = mimetypes.guess_type(filename)[0] if guess: response.header['Content-type'] = guess 最后返回文件对象 12345678if mimetype == 'application/octet-stream' and \"Content-Disposition\" not in response.header: response.header[\"Content-Disposition\"] = \"attachment;filename=%s\"%nameelif 'Last-Modified' not in response.header: ts = time.gmtime(stats.st_mtime) ts = time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", ts) response.header[\"Content-Length\"] = stats.st_size response.header['Last-Modified'] = tsreturn open(filename, 'r') 跳转123456# Redirect to another urldef redirect(url, code=307): \"\"\" Aborts execution and causes a 307 redirect \"\"\" response.status = code response.header['Location'] = url raise SimFishException(\"\") 异常处理123456789101112131415161718# Exceptionsclass SimFishException(Exception): \"\"\"A base class for exception\"\"\" passclass HTTPError(SimFishException): \"\"\"Jump out to error handler\"\"\" def __init__(self, status, text): self.output = text self.http_status = status def __str__(self): return self.outputclass BreakSimFish(SimFishException): \"\"\"Jump out of execution\"\"\" def __init__(self, text): self.output = text 路由分发在上面已经有了如何添加路由，接下来就要实现路由的分发。 1234567891011121314151617181920212223242526272829303132333435class Simfish: def __init__(self, environ, start_response): self.environ = environ self.start = start_response request.bind(environ) #绑定request和response response.bind() def __iter__(self): path = request.path handler = Routes.match(path) #Routes类中的match方法，获取处理的callback function result = \"\" if not handler: response.status = 404 result = \"not Found\" else: try: result = handler(request) except SimFishException,output: #捕获异常情况 result = output if isinstance(result, tuple) and len(result) == 2: #返回(response_string, mimetype)，自定义返回类型 response.header['Content-type'] = result[1] result = result[0] status = '%d %s' % (response.status, HTTP_CODES[response.status]) #获取返回status self.start(status, list(response.header.items())) #调用start_response if hasattr(result, 'read'): #用于返回文件 if 'wsgi.file_wrapper' in self.environ: return self.environ['wsgi.file_wrapper'](result) else: return iter(lambda: result.read(8192), '') return iter(lambda: result.read(8192), '') elif isinstance(result, basestring): return iter([result]) else: return iter(result) 实例1234567891011#!/usr/bin/env python# encoding:utf8from simfish import application, route@route('/')def hello(request): return \"hello world\"app = application(port=8086)app.run() 参考http://www.tuicool.com/articles/aYBRBzhttp://www.bottlepy.org/docs/dev/index.htmlhttp://webpy.org/","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"framework","slug":"framework","permalink":"http://simyy.com/tags/framework/"},{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python sort","slug":"python-sort","date":"2015-04-18T05:01:00.000Z","updated":"2017-04-08T09:27:16.000Z","comments":true,"path":"2015/04/18/python-sort/","link":"","permalink":"http://simyy.com/2015/04/18/python-sort/","excerpt":"python不仅提供了list.sort()方法来实现列表的排序，而且提供了内建sorted()函数来实现对复杂列表的排序以及按照字典的key和value进行排序。","text":"python不仅提供了list.sort()方法来实现列表的排序，而且提供了内建sorted()函数来实现对复杂列表的排序以及按照字典的key和value进行排序。 sorted函数原型1234sorted(data, cmp=None, key=None, reverse=False) #data为数据#cmp和key均为比较函数#reverse为排序方向，True为倒序，False为正序 基本用法对于列表直接进行排序 1234567&gt;&gt;&gt; sorted([5, 2, 3, 1, 4])[1, 2, 3, 4, 5]&gt;&gt;&gt; a = [5, 2, 3, 1, 4]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[1, 2, 3, 4, 5] 对于字典只对key进行排序 123456789sorted(&#123;1: &apos;D&apos;, 2: &apos;B&apos;, 3: &apos;B&apos;, 4: &apos;E&apos;, 5: &apos;A&apos;&#125;)[1, 2, 3, 4, 5]``` ### key函数key函数应该接受一个参数并返回一个用于排序的key值。由于该函数只需要调用一次，因而排序速度较快。复杂列表 student_tuples = [ (‘john’, ‘A’, 15), (‘jane’, ‘B’, 12), (‘dave’, ‘B’, 10),]sorted(student_tuples, key=lambda student: student[2]) # sort by age[(‘dave’, ‘B’, 10), (‘jane’, ‘B’, 12), (‘john’, ‘A’, 15)]12如果列表内容是类的话， class Student: def init(self, name, grade, age): self.name = name self.grade = grade self.age = age def repr(self): return repr((self.name, self.grade, self.age))student_objects = [ Student(‘john’, ‘A’, 15), Student(‘jane’, ‘B’, 12), Student(‘dave’, ‘B’, 10),]sorted(student_objects, key=lambda student: student.age) # sort by age[(‘dave’, ‘B’, 10), (‘jane’, ‘B’, 12), (‘john’, ‘A’, 15)]12字典 student = [ {“name”:”xiaoming”, “score”:60}, {“name”:”daxiong”, “score”:20}, {“name”:”maodou”, “score”:30}, ]student[{‘score’: 60, ‘name’: ‘xiaoming’}, {‘score’: 20, ‘name’: ‘daxiong’}, {‘score’: 30, ‘name’: ‘maodou’}]sorted(student, key=lambda d:d[“score”])[{‘score’: 20, ‘name’: ‘daxiong’}, {‘score’: 30, ‘name’: ‘maodou’}, {‘score’: 60, ‘name’: ‘xiaoming’}]12此外，Python提供了operator.itemgetter和attrgetter提高执行速度。 from operator import itemgetter, attrgetterstudent = [ (“xiaoming”,60), (“daxiong”, 20), (“maodou”, 30}]sorted(student, key=lambda d:d[1])[(‘daxiong’, 20), (‘maodou’, 30), (‘xiaoming’, 60)]sorted(student, key=itemgetter(1))[(‘daxiong’, 20), (‘maodou’, 30), (‘xiaoming’, 60)] 12operator提供了多个字段的复杂排序。 sorted(student, key=itemgetter(0,1)) #根据第一个字段和第二个字段[(‘daxiong’, 20), (‘maodou’, 30), (‘xiaoming’, 60)]operator.methodcaller()函数会按照提供的函数来计算排序。 messages = [‘critical!!!’, ‘hurry!’, ‘standby’, ‘immediate!!’]sorted(messages, key=methodcaller(‘count’, ‘!’))[‘standby’, ‘hurry!’, ‘immediate!!’, ‘critical!!!’]123456首先通过count函数对&quot;!&quot;来计算出现次数，然后按照出现次数进行排序。### CMPcmp参数是Python2.4之前使用的排序方法。 def numeric_compare(x, y): return x - y sorted([5, 2, 4, 1, 3], cmp=numeric_compare)[1, 2, 3, 4, 5]def reverse_numeric(x, y): return y - xsorted([5, 2, 4, 1, 3], cmp=reverse_numeric)[5, 4, 3, 2, 1]在functools.cmp_to_key函数提供了比较功能 sorted([5, 2, 4, 1, 3], key=cmp_to_key(reverse_numeric))[5, 4, 3, 2, 1] def cmp_to_key(mycmp): ‘Convert a cmp= function into a key= function’ class K(object): def init(self, obj, *args): self.obj = obj def lt(self, other): return mycmp(self.obj, other.obj) &lt; 0 def gt(self, other): return mycmp(self.obj, other.obj) &gt; 0 def eq(self, other): return mycmp(self.obj, other.obj) == 0 def le(self, other): return mycmp(self.obj, other.obj) &lt;= 0 def ge(self, other): return mycmp(self.obj, other.obj) &gt;= 0 def ne(self, other): return mycmp(self.obj, other.obj) != 0 return K```","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python propety","slug":"python-propety","date":"2015-04-16T15:50:11.000Z","updated":"2017-04-08T09:27:26.000Z","comments":true,"path":"2015/04/16/python-propety/","link":"","permalink":"http://simyy.com/2015/04/16/python-propety/","excerpt":"在2.6版本中，添加了一种新的类成员函数的访问方式–property。","text":"在2.6版本中，添加了一种新的类成员函数的访问方式–property。 原型12345678class property([fget[, fset[, fdel[, doc]]]])fget：获取属性fset：设置属性fdel：删除属性doc：属性含义 用法让成员函数通过属性方式调用12345678910111213141516171819202122class C(object): def __init__(self): self._x = None def getx(self): return self._x def setx(self, value): self._x = value def delx(self): del self._x x = property(getx, setx, delx, \"I'm the 'x' property.\")a = C()print C.x.__doc__ #打印docprint a.x #调用a.getx()a.x = 100 #调用a.setx()print a.xtry: del a.x #调用a.delx() print a.x #已被删除，报错except Exception, e: print e 输出结果： 1234I&apos;m the &apos;x&apos; property.None100&apos;C&apos; object has no attribute &apos;_x&apos; 利用property装饰器，让成员函数称为只读的123456789101112131415class Parrot(object): def __init__(self): self._voltage = 100000 @property def voltage(self): \"\"\"Get the current voltage.\"\"\" return self._voltagea = Parrot()print a.voltage #通过属性调用voltage函数try: print a.voltage() #不允许调用函数，为只读的except Exception as e: print e 输出结果： 12100000&apos;int&apos; object is not callable 利用property装饰器实现property函数的功能12345678910111213141516class C(object): def __init__(self): self._x = None @property def x(self): \"\"\"I'm the 'x' property.\"\"\" return self._x @x.setter def x(self, value): self._x = value @x.deleter def x(self): del self._x 其他应用bottle源码中的应用12345678910111213141516171819202122232425262728293031class Request(threading.local): \"\"\" Represents a single request using thread-local namespace. \"\"\" ... @property def method(self): ''' Returns the request method (GET,POST,PUT,DELETE,...) ''' return self._environ.get('REQUEST_METHOD', 'GET').upper() @property def query_string(self): ''' Content of QUERY_STRING ''' return self._environ.get('QUERY_STRING', '') @property def input_length(self): ''' Content of CONTENT_LENGTH ''' try: return int(self._environ.get('CONTENT_LENGTH', '0')) except ValueError: return 0 @property def COOKIES(self): \"\"\"Returns a dict with COOKIES.\"\"\" if self._COOKIES is None: raw_dict = Cookie.SimpleCookie(self._environ.get('HTTP_COOKIE','')) self._COOKIES = &#123;&#125; for cookie in raw_dict.values(): self._COOKIES[cookie.key] = cookie.value return self._COOKIES 在django model中的应用，实现连表查询1234567891011121314from django.db import modelsclass Person(models.Model): name = models.CharField(max_length=30) tel = models.CharField(max_length=30)class Score(models.Model): pid = models.IntegerField() score = models.IntegerField() def get_person_name(): return Person.objects.get(id=pid) name = property(get_person_name) #name称为Score表的属性，通过与Person表联合查询获取name","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"Python _、__和__xx__的区别","slug":"python-xx","date":"2015-04-12T16:49:10.000Z","updated":"2017-04-08T09:26:51.000Z","comments":true,"path":"2015/04/13/python-xx/","link":"","permalink":"http://simyy.com/2015/04/13/python-xx/","excerpt":"本文为译文，版权属于原作者，在此翻译为中文分享给大家。英文原文地址：Difference between _, __ and __xx__ in Python","text":"本文为译文，版权属于原作者，在此翻译为中文分享给大家。英文原文地址：Difference between _, __ and __xx__ in Python “_“单下划线Python中不存在真正的私有方法。为了实现类似于c++中私有方法，可以在类的方法或属性前加一个“_”单下划线，意味着该方法或属性不应该去调用，它并不属于API。 在使用property时，经常出现这个问题： 12345678910class BaseForm(StrAndUnicode): ... def _get_errors(self): \"Returns an ErrorDict for the data provided for the form\" if self._errors is None: self.full_clean() return self._errors errors = property(_get_errors) 上面的代码片段来自于django源码（django/forms/forms.py）。这里的errors是一个属性，属于API的一部分，但是_get_errors是私有的，是不应该访问的，但可以通过errors来访问该错误结果。 “__“双下划线这个双下划线更会造成更多混乱，但它并不是用来标识一个方法或属性是私有的，真正作用是用来避免子类覆盖其内容。 让我们来看一个例子： 12345class A(object): def __method(self): print \"I'm a method in A\" def method(self): self.__method() a = A() a.method() 输出是这样的： 12$ python example.py I&apos;m a method in A 很好，出现了预计的结果。 我们给A添加一个子类，并重新实现一个__method： 123456class B(A): def __method(self): print \"I'm a method in B\" b = B() b.method() 现在，结果是这样的： 12$ python example.pyI&apos;m a method in A 就像我们看到的一样，B.method()不能调用B._method的方法。实际上，它是”\\_“两个下划线的功能的正常显示。 因此，在我们创建一个以”__“两个下划线开始的方法时，这意味着这个方法不能被重写，它只允许在该类的内部中使用。 在Python中如是做的？很简单，它只是把方法重命名了，如下： 12a = A()a._A__method() # never use this!! please! 12$ python example.py I&apos;m a method in A 如果你试图调用a.__method，它还是无法运行的，就如上面所说，只可以在类的内部调用__method。 “__xx__“前后各双下划线当你看到”__this__“的时，就知道不要调用它。为什么？因为它的意思是它是用于Python调用的，如下： 123456&gt;&gt;&gt; name = &quot;igor&quot; &gt;&gt;&gt; name.__len__() 4 &gt;&gt;&gt; len(name) 4 &gt;&gt;&gt; number = 10 &gt;&gt;&gt; number.__add__(20) 30 &gt;&gt;&gt; number + 20 30 “__xx__”经常是操作符或本地函数调用的magic methods。在上面的例子中，提供了一种重写类的操作符的功能。 在特殊的情况下，它只是python调用的hook。例如，__init__()函数是当对象被创建初始化时调用的;__new__()是用来创建实例。 1234567891011121314class CrazyNumber(object): def __init__(self, n): self.n = n def __add__(self, other): return self.n - other def __sub__(self, other): return self.n + other def __str__(self): return str(self.n) num = CrazyNumber(10) print num # 10print num + 5 # 5print num - 20 # 30 另一个例子 1234567891011class Room(object): def __init__(self): self.people = [] def add(self, person): self.people.append(person) def __len__(self): return len(self.people) room = Room() room.add(\"Igor\") print len(room) # 1 结论 使用_one_underline来表示该方法或属性是私有的，不属于API； 当创建一个用于python调用或一些特殊情况时，使用__two_underline__； 使用__just_to_underlines，来避免子类的重写！","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"redis（二）高级用法","slug":"redis-advance-usage","date":"2015-04-07T16:49:45.000Z","updated":"2017-04-08T09:26:09.000Z","comments":true,"path":"2015/04/08/redis-advance-usage/","link":"","permalink":"http://simyy.com/2015/04/08/redis-advance-usage/","excerpt":"介绍redis高级用法。","text":"介绍redis高级用法。 事务redis的事务是一组命令的集合。事务同命令一样都是redis的最小执行单元，一个事务中的命令要么执行要么都不执行。 首先需要multi命令来开始事务，用exec命令来执行事务。1234567891011121314127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; hset user:1 name xiaomingQUEUED127.0.0.1:6379&gt; hset user:1 name daxiongQUEUED127.0.0.1:6379&gt; exec1) (integer) 02) (integer) 0127.0.0.1:6379&gt; hgetall user:11) &quot;name&quot;2) &quot;daxiong&quot;3) &quot;score&quot;4) &quot;61&quot; multi代表事务的开始，返回ok表示成功； exec代表事务的执行，返回各个命令的执行结果； 在multi和exec中间添加需要执行的命令。 在multi开始后，所有命令都不会执行，而是全部暂时保存起来，在执行exec命令后会按照命令保存的顺序依次执行各个命令。 如果事务执行过程中存在失败的情况下（某一个命令执行失败后其他命令会继续执行），需要开发人员自行处理后果。 注意：redis不支持回滚操作，导致redis的错误异常需要开发人员处理。 watchwatch命令可以监控一个或多个键值的变化，一旦其中一个键被改变，之后的事务就不会执行，而且监控会一直持续到exec命令。1234567891011121314127.0.0.1:6379&gt; set key 1OK127.0.0.1:6379&gt; watch keyOK127.0.0.1:6379&gt; set key 2OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set key 3QUEUED127.0.0.1:6379&gt; exec(nil)127.0.0.1:6379&gt; get key&quot;2&quot; 生存时间（1）设置key的超时时间，超时后redis会自动删除给key值，类似于memcache中的超时时间。 123456789101112131415expire key seconds//设置成功返回1，失败返回0127.0.0.1:6379&gt; set session:aabb uid1122OK127.0.0.1:6379&gt; expire session:aabb 300(integer) 1127.0.0.1:6379&gt; del session:aabb(integer) 1127.0.0.1:6379&gt; expire session:aabb 300(integer) 0 127.0.0.1:6379&gt; expire session:aabb 300(integer) 1127.0.0.1:6379&gt; ttl session:aabb (integer) 290 （2）查询剩余超时时间12345ttl key127.0.0.1:6379&gt; expire session:aabb 300(integer) 1127.0.0.1:6379&gt; ttl session:aabb (integer) 290 （3）取消超时时间12345678127.0.0.1:6379&gt; get session:aabb&quot;300&quot;127.0.0.1:6379&gt; ttl session:aabb (integer) 280127.0.0.1:6379&gt; persist session:aabb(integer) 1127.0.0.1:6379&gt; ttl session:aabb(integer) -1 （4）如果使用设置相关的命令，会取消该键的超时间 缓存数据在某些情况下，需要缓存一部分网站数据，而网站数据由需要持续的更新（假如需要两个小时更新一次），那么可以采用redis进行缓存这部分数据，设置数据的超时时间为2小时，每当有请求访问的时候首先到redis中查找该数据是否存在，如果存在直接读取，如果不存在的话重新从数据库中读取该数据加载到redis中。 在缓存数据的时候需要考虑到被缓存数据的大小，如果缓存数据较大，会占用过多的内存资源，有必要在配置文件中限制内存的使用大小（maxmemory）。 当超过maxmemory的限制后，redis会根据maxmemory-policy参数指定的策略（包括LRU等算法）来删除不需要的键。 排序sort命令支持对集合类型、类表类型、有序集合类型进行排序。12345678910127.0.0.1:6379&gt; lpush list 1 2 6 3 4 9 8(integer) 7127.0.0.1:6379&gt; sort list1) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;6&quot;6) &quot;8&quot;7) &quot;9&quot; 可以对有序集合的值进行排序：1234567127.0.0.1:6379&gt; zadd set 50 2 40 3 20 1 60 5(integer) 4127.0.0.1:6379&gt; sort set1) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;5&quot; sort命令可以添加desc来实现倒序排序12345127.0.0.1:6379&gt; sort set desc1) &quot;5&quot;2) &quot;3&quot;3) &quot;2&quot;4) &quot;1&quot; BY参数很多时候我们需要根据ID对应的对象的某一个属性进行排序，那么如何才能把多个不同的数据进行关联查询呢？（1）首先，向userids中添加三个用户id12127.0.0.1:6379&gt; lpush userids 1 2 3(integer) 3 （2）其次，分别对三个用户添加分数123456127.0.0.1:6379&gt; set user_score_1 50OK127.0.0.1:6379&gt; set user_score_2 30OK127.0.0.1:6379&gt; set user_score_3 70OK （3）最后，使用sort、by命令来对对用户按照默认情况以及分数的递增和递减进行排序。123456789101112127.0.0.1:6379&gt; sort userids1) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;127.0.0.1:6379&gt; sort userids by user_score_*1) &quot;2&quot;2) &quot;1&quot;3) &quot;3&quot;127.0.0.1:6379&gt; sort userids by user_score_* desc1) &quot;3&quot;2) &quot;1&quot;3) &quot;2&quot; GET参数get参数并不影响排序，它的作用是使sort命令返回的结果不再是元素自身的值，而是get参数中指定的键值，同by参数一样，支持字符串类型和散列类型的键。12345678127.0.0.1:6379&gt; sort userids by user_score_* get user_name_*1) &quot;xiaoming&quot;2) &quot;daxiong&quot;3) &quot;xiaohong&quot;127.0.0.1:6379&gt; sort userids by user_score_* desc get user_name_*1) &quot;xiaohong&quot;2) &quot;daxiong&quot;3) &quot;xiaoming&quot; STORE参数12345678910store参数用于结果保存。 sort命令是redis的复杂命令之一，使用不好会造成性能的瓶颈。 sort命令的时间复杂度是O（n+mlog(m)），其中n是排序列表（集合和有序集合）中元素的个数，m是返回元素的个数。Redis在排序前会建立一个长度为n的的容器来存储待排序元素，虽然是一个临时的过程，但是多个较大数据的排序操作则会严重影响系统的性能。 因此，在开发中需要注意：（1）尽可能减少排序键中的元素个数，降低n（2）使用Limit参数只获取需要的数据，降低n（3）如果要排序的数据量较大，尽可能使用store名来缓存结果。 任务队列任务队列一般适用于生产者和消费者之间通信的，那么在redis中很容易想到使用列表类型来实现任务队列，具体方法是创建一个任务队列，生产者主动lpush数据，而消费者去rpop数据，保持一个先进先出的循序。但是这样存在一个问题，消费者需要主动去请求数据，周期性的请求会造成资源的浪费，因此，redis提供了一个brpop的命令来解决这个问题。BRPOP key timeoutbrpop命令接收两个参数，第一个参数key为键值，第二个参数timeout为超时时间。BRPOP命令取数据时候，如果暂时不存在数据，该命令会一直阻塞直到达到超时时间。如果timeout设置为0，那么就会无限等待下去。优先级队列 基于任务队列，如何实现优先级队列呢？ 那么可以选择多个任务队列，而每个任务队列的任务优先级是不同的。 redis提供了下面的命令，会从左边第一个key开始读下去知道返回一个数据。1brpop key [key...] timetout 发布/订阅模式redis提供了rabitmq类似的发布订阅模式，通过生产者使用下面的命令来发布消息，1PUBLISH CHANNEL MESSAGE 消费者通过下面的消息来订阅消息，1SUBSCRIBE CHANNEL MESSAGE 生产者：向channel.test发布消息1234127.0.0.1:6379&gt; publish channel.test hello(integer) 0 #返回0表明订阅者为0，没有发布消息127.0.0.1:6379&gt; publish channel.test hello(integer) 1 #返回n表明订阅者为n，成功发布给1个消费者 消费者：订阅channel.test消息123456789127.0.0.1:6379&gt; subscribe channel.test Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;channel.test&quot;3) (integer) 1#接收到来自channel.test的消息1) &quot;message&quot;2) &quot;channel.test&quot;3) &quot;hello&quot; 管道redis的底层通信协议对管道提供了支持。通过管道可以一次性发送多条命令并在执行完后一次性将结果返回，当一组命令中每条命令都不依赖之前命令的执行结果时就可以将这组命令一起通过管道发出。管道通过减少客户端与redis的通信次数来实现降低往返实验累计值的目的。节省空间 （1）精简键名和键值（2）redis为每种数据类型提供了两种内部编码。例如散列类型的存储是通过散列表来实现的，redis会根据数据的多少来选择编码类型，当数据较少的时候会采用紧凑但性能稍差的内部编码方式，而数据变多时会把编码方式改为散列表。","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"redis（一） 安装以及基本数据类型操作","slug":"redis-install-base-op","date":"2015-04-03T16:10:44.000Z","updated":"2017-04-08T09:26:00.000Z","comments":true,"path":"2015/04/04/redis-install-base-op/","link":"","permalink":"http://simyy.com/2015/04/04/redis-install-base-op/","excerpt":"介绍redis基本用法。","text":"介绍redis基本用法。 redis安装和使用redis安装12345wget http://download.redis.io/redis-stable.tar.gztar zxvf redis-stable.tar.gzcd redis-stable.tar.gzmakemake install redis启动123redis-serverredis关闭redis-cli shutdown 配置方法一12redis-server /path/to/redis.confredis-server /path/to/redis.conf --loglevel warning //设置日志级别 方法二12redis-cliredis &gt; CONFIG SET loglevel warning 多数据库的选择默认情况下，客户端自动选择0号数据库。可以使用select命令来选择其他数据库。1select 1 redis不可以设置数据库的名称，数据库是通过编号来使用的。redis只有一个全局的密码，不存在访问某个数据库的密码。 数据库基本操作（1）添加数据和查找数据，通过set/get命令添加和获取数据12345678127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; set bar 1OK127.0.0.1:6379[1]&gt; get bar&quot;1&quot;127.0.0.1:6379[1]&gt; get aaa(nil) （2）判断一个键是否存在，exists命令可以判断key是否存在，存在返回1，不存在返回01234127.0.0.1:6379[1]&gt; exists bar(integer) 1127.0.0.1:6379[1]&gt; exists aaa(integer) 0 （3）删除数据，del命令删除key，返回值为成功删除key的个数1234127.0.0.1:6379[1]&gt; del aaa(integer) 0127.0.0.1:6379[1]&gt; del bar(integer) 1 （4）获取value的类型1234127.0.0.1:6379[1]&gt; set bar 1OK127.0.0.1:6379[1]&gt; type barstring （5）自增和自减，incr 、decr以及incrby和decrby实现整数的加减12345678910127.0.0.1:6379[1]&gt; INCR ab(integer) 1127.0.0.1:6379[1]&gt; INCR ab(integer) 2127.0.0.1:6379[1]&gt; INCRBY ab 2(integer) 4127.0.0.1:6379[1]&gt; DECR ab(integer) 3127.0.0.1:6379[1]&gt; DECRBY ab 2(integer) 1 （6）增加指定浮点数，incrbyfloat可以指定自增的浮点数12127.0.0.1:6379[1]&gt; INCRBYFLOAT ab 1.1&quot;2.1&quot; （7）向尾部追加，append命令可以向字符尾部追加内容123456127.0.0.1:6379[1]&gt; set key helloOK127.0.0.1:6379[1]&gt; append key world(integer) 10127.0.0.1:6379[1]&gt; get key&quot;helloworld&quot; （8）获取字符串长度，strlen获取value的长度12127.0.0.1:6379[1]&gt; STRLEN key(integer) 10 （9）批量设置和获取，通过mset和mget命令可以批量执行设置和获取12345127.0.0.1:6379[1]&gt; mset key1 va1 key2 va2OK127.0.0.1:6379[1]&gt; mget key1 key21) &quot;va1&quot;2) &quot;va2&quot; （10）位操作1234getbit key offsetsetbit key offsetbitcount key [start] [end]bitop operation destkey key [key ...] 散列类型其实这里的散列类型就是类似于非结构化数据，json数据一样。{ “id”:1, “name”:”test”} 基本操作（1）存储方法，可以通过hset和hget来设置和获取数据12345678127.0.0.1:6379[1]&gt; hset user id 1(integer) 1127.0.0.1:6379[1]&gt; hset user name test(integer) 1127.0.0.1:6379[1]&gt; hget user id&quot;1&quot;127.0.0.1:6379[1]&gt; hget user name&quot;test&quot; （2）判断字段是否存在，hexists命令，返回1存在，返回0不存在1234127.0.0.1:6379[1]&gt; hexists user id(integer) 1127.0.0.1:6379[1]&gt; hexists user age(integer) 0 （3）使用hsetnx(hset if not exists)如果不存在赋值1234127.0.0.1:6379[1]&gt; hsetnx user age 111(integer) 1127.0.0.1:6379[1]&gt; hget user age&quot;111&quot; （4）增加数字1234127.0.0.1:6379[1]&gt; hincrby user score 60(integer) 60127.0.0.1:6379[1]&gt; hget user score&quot;60&quot; （5）删除字段1234127.0.0.1:6379[1]&gt; hdel user score(integer) 1127.0.0.1:6379[1]&gt; hget user score(nil) 如何存储数据？（1）使用散列类型存储数据，散列数据包括三部分（键、字段、字段值）1234567891011121314键 字段 字段值post:id field value127.0.0.1:6379[1]&gt; incr userid(integer) 1127.0.0.1:6379[1]&gt; hset user:1 name test(integer) 1127.0.0.1:6379[1]&gt; hset user:1 score 90(integer) 1127.0.0.1:6379[1]&gt; incr userid(integer) 2127.0.0.1:6379[1]&gt; hset user:2 name test2(integer) 1127.0.0.1:6379[1]&gt; hset user:2 score 99(integer) 1 添加了两条数据，id为1的名字为test分数为90，而id为2的名字为test2分数为90. （2）获取多个字段的数据，需要使用hmget命令，并制定字段名称123127.0.0.1:6379[1]&gt; hmget user:1 name score1) &quot;test&quot;2) &quot;90&quot; （3）获取一行数据，不需要指定字段名称，只需要指出键名12345127.0.0.1:6379[1]&gt; hgetall user:21) &quot;name&quot;2) &quot;test2&quot;3) &quot;score&quot;4) &quot;99&quot; （4）只获取字段名或字段值123456127.0.0.1:6379[1]&gt; hkeys user:11) &quot;name&quot;2) &quot;score&quot;127.0.0.1:6379[1]&gt; hvals user:11) &quot;test&quot;2) &quot;90&quot; （5）获取字段数量12127.0.0.1:6379[1]&gt; hlen user:1(integer) 2 需要注意的是，散列类型无法获取所有存在的键值，也就是id，如果删除了中间某个id的话，只可以使用exist命令来判断key是否存在。 列表类型类表类型解决了上述的问题。类表类型是有序的，值是可以重复的。列表类型是通过双向链表实现的，向列表两端添加元素的时间复杂度为O（1），获取两端元素的速度也是最快。 基本操作列表的基本操作命令都是以l开头的。（1）添加和弹出元素使用lpush和rpush以及lpop和rpop分别从列表的左侧和右侧添加和删除元素。123456789127.0.0.1:6379&gt; lpush user test(integer) 1127.0.0.1:6379&gt; rpush user test1(integer) 2127.0.0.1:6379&gt; lpop user &quot;test&quot;127.0.0.1:6379&gt; rpop user&quot;test1&quot;127.0.0.1:6379&gt; （2）llen查看元素个数123456127.0.0.1:6379&gt; llen user(integer) 0127.0.0.1:6379&gt; lpush user test(integer) 1127.0.0.1:6379&gt; llen user(integer) 1 （3）获取列表片段123456789127.0.0.1:6379&gt; lrange user 0 31) &quot;test&quot;2) &quot;test1&quot;3) &quot;test2&quot;4) &quot;test3&quot;127.0.0.1:6379&gt; lrange user 0 21) &quot;test&quot;2) &quot;test1&quot;3) &quot;test2&quot; （4）从列表中删除元素123lrem key count value@count 为删除个数，大于0的话从左边开始，小于0的话从右边开始@value 要删除的值 我们可以把数据的id存储在列表中，当某一行数据被删除时候，只需要删除为该value为key即可，而查询数据的时候，需要先从列表中读取所有的id，再从散列表中读取数据。 （5）获取和设置索引元素值 （6）向列表中插入元素12345678910111213141516127.0.0.1:6379&gt; lrange user 0 -11) &quot;test&quot;2) &quot;test1&quot;3) &quot;test2&quot;4) &quot;test3&quot;127.0.0.1:6379&gt; linsert user before test1 test0(integer) 5127.0.0.1:6379&gt; linsert user after test1 test1.5(integer) 6127.0.0.1:6379&gt; lrange user 0 -11) &quot;test&quot;2) &quot;test0&quot;3) &quot;test1&quot;4) &quot;test1.5&quot;5) &quot;test2&quot;6) &quot;test3&quot; （7）将元素从一个列表转移到另外一个列表1rdroplpush source destination 从source列表的右侧弹出一个数据，添加到destination列表的左侧 集合类型集合中不允许重复数据的出现，数据是唯一的，无序的。空的散列表的HASH TABLE实现，因此操作时间复杂度为O(1)。（1）增加sadd和删除srem、获取所有元素smembers，返回值标识成功添加元素的数量：12345678910111213127.0.0.1:6379&gt; sadd key 1(integer) 1127.0.0.1:6379&gt; sadd key 2(integer) 1127.0.0.1:6379&gt; sadd key 2(integer) 0127.0.0.1:6379&gt; smembers key1) &quot;1&quot;2) &quot;2&quot;127.0.0.1:6379&gt; srem key 2(integer) 1127.0.0.1:6379&gt; smembers key1) &quot;1&quot; （2）判断元素是否在集合中可以使用sismember命令。1234127.0.0.1:6379&gt; sismember key 1(integer) 1127.0.0.1:6379&gt; sismember key 3(integer) 0 （3）集合运算123sdiff key [key...]sinter key [key...]sunion key [key...] （4）获取元素个数12127.0.0.1:6379&gt; scard key(integer) 1 （5）集合运算并保存结果123sdiffstore destination key [key...]sinterstore destination key[key...]sunionstore destination key[key...] （6）随机获取集合的元素12345srandmember key [count] 127.0.0.1:6379&gt; srandmember key 31) &quot;4&quot;2) &quot;2&quot;3) &quot;3&quot; 需要注意，srandmember取到的随机元素在集合比较小的情况下并不是完全随机的，由于redis的存储方法是利用哈希桶+数据链的存储方式，当桶和每个桶内元素的个数都非常小时就会造成取到相同数据的情况。 有序集合类型 sorted set与普通的集合类型相比，有序集合为集合中的每一个元素都关联了一个分数，这使得我们不仅可以完成插入、删除和判断元素是否存在等集合的基本操作，还能够得到分数最高（或最低）的前n个元素、获得指定分数范围内的元素等与分有关的操作，虽然集合中的元素必须是不同的，但是其分数是可以相同的。 有序结合和列表的使用区别： 列表类型是通过链来实现的，获取靠近两端数据的速度极快，而当元素增多后，访问中间元素的数据就会变慢，所以她更适合实现如“新鲜事”或“日志”这样少访问中间元素的应用。 有序集合类型是使用散列和跳跃表实现的，所以即使读取位于中间部分数据的速度也很快（时间复杂度为O（log(n)））。 列表中不能简单地调整某个元素的位置，但有序集合可以通过调整分数来实现位置的调整。 有序集合更加消耗内存。 ###基本操作 （1）zadd添加元素，并设置分数score1234127.0.0.1:6379&gt; zadd students 60 xiaoming(integer) 1127.0.0.1:6379&gt; zadd students 80 daxiong(integer) 1 （2）zscore获取元素分数12127.0.0.1:6379&gt; zscore students xiaoming&quot;60&quot; （3）获取排名在某个范围的列表，元素是从0开始的123127.0.0.1:6379&gt; zrange students 0 11) &quot;xiaoming&quot;2) &quot;daxiong&quot; （4）获取指定分数范围的元素，用于筛选数据12345127.0.0.1:6379&gt; zrangebyscore students 0 701) &quot;xiaoming&quot;127.0.0.1:6379&gt; zrangebyscore students 0 1001) &quot;xiaoming&quot;2) &quot;daxiong&quot; limit命令可以实现sql语句中的Limit效果获取从0开始的前2个数据123127.0.0.1:6379&gt; zrangebyscore students 0 100 limit 0 21) &quot;xiaoming&quot;2) &quot;daxiong&quot; 获取从1开始的前2个数据12127.0.0.1:6379&gt; zrangebyscore students 0 100 limit 1 21) &quot;daxiong&quot; （5）增加和减少某个元素的分数1234127.0.0.1:6379&gt; zincrby students 5 xiaoming&quot;65&quot;127.0.0.1:6379&gt; zincrby students -5 xiaoming&quot;60&quot; （6）获取集合中元素个数12127.0.0.1:6379&gt; zcard students(integer) 2 （7）获取指定分数范围内的元素个数123456zcount key min max127.0.0.1:6379&gt; zcount students 0 70(integer) 1127.0.0.1:6379&gt; zcount students 0 90(integer) 2 （8）删除一个或多个元素1zrem key [key...] （9）按照排名范围来删除元素1zremrangebyrank key start stop （10）按照分数范围来删除元素1zremrangebyscore key min max （11）获取元素的排名，注意元素从0开始排序12127.0.0.1:6379&gt; zrank students xiaoming(integer) 0 参考redis入门指南http://redis.readthedocs.org/en/latest/","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://simyy.com/tags/redis/"}]},{"title":"RabbitMQ topics","slug":"rabbitmq-topics","date":"2015-03-16T13:30:10.000Z","updated":"2017-04-08T09:26:24.000Z","comments":true,"path":"2015/03/16/rabbitmq-topics/","link":"","permalink":"http://simyy.com/2015/03/16/rabbitmq-topics/","excerpt":"rabbitmq中的topic exchange将路由键和某模式进行匹配，从而类似于正则匹配的方式去接收喜欢的信息。","text":"rabbitmq中的topic exchange将路由键和某模式进行匹配，从而类似于正则匹配的方式去接收喜欢的信息。 topic exchange如果想使用topic模式，那么可以随意设置routing_key。相反，需要按照一定的要求设定该值。 routing_key在topic模式中应该选择一组拥有特定属性的单词作为该值。 12\\* (star) can substitute for exactly one word.# (hash) can substitute for zero or more words. 例如，如果生产者的routing_key设置为test1.test2.test3，那么消费着中绑定消息队列的routing_key必须可以匹配生产者的routing_key。 123456789#生产者routing_key = 'test1.test2.test3'channel.basic_publish(exchange='topic_test', routing_key=routing_key, body=message)#消费者routing_key = 'test1.*' #可以routing_key = '*.test2.*' #可以routing_key = 'test3' #不可以channel.queue_bind(exchange='topic_logs', queue=queue_name, routing_key=binding_key) 例子生产者如下，会依次设置routing_key为A和B，那么需要设置两个消费者的routing_key来分别读取消息。 123456789101112131415161718192021#!/usr/bin/env python# coding=utf-8import pikaimport sysimport timeconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='topic_test',type='topic')message = \"test \"for i in range(20): for item in ['A', 'B']: routing_key = item channel.basic_publish(exchange='topic_test',routing_key=routing_key, body=message+item) print \" [x] Sent %r:%r\" % (routing_key, message) time.sleep(2)connection.close() 消费者如下，启动命令分别为: 12python receive.py Apython receive.py B 消费者如下： #!/usr/bin/env python # coding=utf-8 import pika import sys def callback(ch, method, properties, body): print \" [x] %r:%r\" % (method.routing_key, body,) connection = pika.BlockingConnection(pika.ConnectionParameters( host='localhost')) channel = connection.channel() channel.exchange_declare(exchange='topic_test', type='topic') result = channel.queue_declare(exclusive=True) queue_name = result.method.queue binding_key = sys.argv[1] print \"Usage: %s [binding_key]...\" % (sys.argv[1]) channel.queue_bind(exchange='topic_test', queue=queue_name, routing_key=binding_key) print ' [*] Waiting for logs. To exit press CTRL+C' channel.basic_consume(callback, queue=queue_name, no_ack=True) channel.start_consuming()","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"RabbitMQ Routing","slug":"rabbitmq-routing","date":"2015-03-16T13:29:10.000Z","updated":"2017-04-08T09:26:27.000Z","comments":true,"path":"2015/03/16/rabbitmq-routing/","link":"","permalink":"http://simyy.com/2015/03/16/rabbitmq-routing/","excerpt":"rabbitmq可以通过路由选择订阅者来发布消息。","text":"rabbitmq可以通过路由选择订阅者来发布消息。 Bindings通过下面的函数绑定Exchange与消息队列： 1channel.queue_bind(exchange=exchange_name, queue=queue_name) 可以通过添加routing_key来做路由选择，如下： 1channel.queue_bind(exchange=exchange_name, queue=queue_name, routing_key=&apos;black&apos;) Direct Exchange为了使用直接转发，可以设置Exchange的类型为direct。 在rabbitmq中直接转发的算法很简单，如果binding key与routing key相同，消息会直接添加到相应的消息队列中取。 Multiple bindingsrabbitmq允许一个Exchange绑定多个消息队列，那么该Exchange会把消息分别发布到绑定的多个消息队列中。 例子生产者 123456789101112131415161718192021#!/usr/bin/env python# coding=utf-8import pikaimport sysimport timeconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='news', type='fanout')for i in range(100): message = str(i) + 'Hello World!' if i%2 == 0: channel.basic_publish(exchange='news', routing_key='0', body=message) else: channel.basic_publish(exchange='news', routing_key='1', body=message) print \" [x] Sent %r\" % (message,) time.sleep(2)connection.close() 消费者 123456789101112131415161718192021222324import pikaimport sysparameters = pika.URLParameters('amqp://mtest:root@rabbit-server:5672/%2F')connection = pika.BlockingConnection(parameters)channel = connection.channel()channel.exchange_declare(exchange='news', type='fanout')result = channel.queue_declare(exclusive=True)queue_name = result.method.queuechannel.queue_bind(exchange='news', queue=queue_name, routing_key=sys.argv[1])print ' [*] Waiting for news. To exit press CTRL+C'def callback(ch, method, properties, body): print \" [x] %r\" % (body,)channel.basic_consume(callback, queue=queue_name, no_ack=True)channel.start_consuming() parameters = pika.URLParameters('amqp://guest:guest@rabbit-server1:5672/%2F')","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"RabbitMQ Publish/Subscribe","slug":"rabbitmq-publish-subscribe","date":"2015-03-16T13:25:10.000Z","updated":"2017-04-08T09:26:32.000Z","comments":true,"path":"2015/03/16/rabbitmq-publish-subscribe/","link":"","permalink":"http://simyy.com/2015/03/16/rabbitmq-publish-subscribe/","excerpt":"rabbitmq支持一对多的模式，一般称为发布/订阅。也就是说，生产者产生一条消息后，rabbitmq会把该消息分发给所有的消费者。","text":"rabbitmq支持一对多的模式，一般称为发布/订阅。也就是说，生产者产生一条消息后，rabbitmq会把该消息分发给所有的消费者。 Exchanges之前的教程中，仅仅使用了基本的消息模型： 生产者产生消息 把消息添加到消息队列 消费者接收消息 而在rabbitmq完整的消息模型中，并不是这样的。事实上，生产者并不知道消息是否发送到队列，而是把消息直接发送给Exchanges。 Exchanges的功能理解起来非常简单，它只负责接收生产者发送的数据并把这些数据添加到消息队列。但是，在存在多个消息队列的情况下，Exchanges必须知道每条消息要添加到哪一个消息队列。 rabbitmq提供了几种Exchanges，包括:direct, topic, headers and fanout。 这里，仅仅介绍fanout的使用。 1channel.exchange_declare(exchange=&apos;news&apos;, type=&apos;fanout&apos;) 那么，发布消息： 1channel.basic_publish(exchange=&apos;news&apos;, routing_key=&apos;&apos;, body=message) Temporary queues由于在生产者和消费者中需要指定相同的消息队列才能实现消息通信，那么如果不特殊指定某个消息队列会如何呢？那么需要使用默认参数让系统给生成一个特定的消息队列。 1result = channel.queue_declare() Bindings为了发送指定发送的消息队列，必须创建exchange和消息队列之间的关系： 1channel.queue_bind(exchange=&apos;news&apos;, queue=result.method.queue) 例子作为生产者的publish： 12345678910111213141516171819#!/usr/bin/env python# coding=utf-8import pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='news',type='fanout')for i in range(100): message = str(i) + 'Hello World!' channel.basic_publish(exchange='news', routing_key='', body=message) print \" [x] Sent %r\" % (message,) import time time.sleep(2)connection.close() 作为消费者的subscribe： 123456789101112131415161718192021222324#!/usr/bin/env python# coding=utf-8import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='news', type='fanout')result = channel.queue_declare(exclusive=True)queue_name = result.method.queuechannel.queue_bind(exchange='news',queue=queue_name)print ' [*] Waiting for news. To exit press CTRL+C'def callback(ch, method, properties, body): print \" [x] %r\" % (body,)channel.basic_consume(callback, queue=queue_name, no_ack=True)channel.start_consuming()","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"RabbitMQ Work Queues","slug":"rabbitmq-work-queues","date":"2015-03-15T04:32:10.000Z","updated":"2017-04-08T09:26:19.000Z","comments":true,"path":"2015/03/15/rabbitmq-work-queues/","link":"","permalink":"http://simyy.com/2015/03/15/rabbitmq-work-queues/","excerpt":"RabbitMQ使用Work Queues的主要目的是为了避免资源使用密集的任务，它不同于定时任务处理的方式，而是把任务封装为消息添加到队列中。而消息队列正是共享于多个工作者中使用，它们可以随意pop出数据进行处理。","text":"RabbitMQ使用Work Queues的主要目的是为了避免资源使用密集的任务，它不同于定时任务处理的方式，而是把任务封装为消息添加到队列中。而消息队列正是共享于多个工作者中使用，它们可以随意pop出数据进行处理。 消息的持久化 Message durability为了保证rabbitmq意外重启等原因造成的消息丢失，通过设置消息的durable来实现数据的持久化，但是需要生产者和消费者同时设置持久化才能生效。 需要注意的是，rabbitmq并不允许更改已经创建的消息队列的属性，假如之前已经创建过非持久化的hello消息队列，那么会返回一个错误信息。 设置消息队列的可持久化属性（第二个参数）： 1channel.queue_declare(queue=&apos;hello&apos;, durable=True) 在消息发送时，需要指定delivery_mode来实现消息持久化：1channel.basic_publish(exchange=&apos;&apos;, routing_key=&quot;task_queue&quot;, body=message, properties=pika.BasicProperties(delivery_mode = 2, # make message persistent)) 平均分配 Fair dispatchrabbitmq实现了消息均分的功能，通过设置basic.qos方法的prefetch_count来实现。它会告诉rabbitmq的生产者不要给一个消费者分配过多的任务，也就是说不要在消费者处理完成已经接收到的任务之前分配新的任务。 1channel.basic_qos(prefetch_count=1) 其中prefetch_count为可以接受处理的任务个数，如果未达到上限rabbitmq会继续向消费者推送任务。 实例生产者 1234567891011121314151617181920#!/usr/bin/env python# coding=utf-8import pikaimport timeconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))channel = connection.channel()channel.queue_declare(queue='task_queue', durable=True)for i in range(100): message = str(i) + ' Hello World!' channel.basic_publish(exchange='', routing_key='task_queue', body=message, properties=pika.BasicProperties(delivery_mode = 2, # make message persistent)) print \" [x] Sent %r\" % (message,) time.sleep(1)connection.close() 消费者 #!/usr/bin/env python # coding=utf-8 import pika import time connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost')) channel = connection.channel() channel.queue_declare(queue='task_queue', durable=True) print ' [*] Waiting for messages. To exit press CTRL+C' def callback(ch, method, properties, body): print \" [x] Received %r\" % (body,) time.sleep(2) print \" [x] Done\" ch.basic_ack(delivery_tag = method.delivery_tag) channel.basic_qos(prefetch_count=1) channel.basic_consume(callback, queue='task_queue') channel.start_consuming()","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"RabbitMQ Hello world","slug":"rabbitmq-hello-world","date":"2015-03-14T13:42:10.000Z","updated":"2017-04-08T09:26:34.000Z","comments":true,"path":"2015/03/14/rabbitmq-hello-world/","link":"","permalink":"http://simyy.com/2015/03/14/rabbitmq-hello-world/","excerpt":"RabbitMQ使用AMQP通信协议（AMQP是一个提供统一消息服务的应用层标准协议，基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。）。安装","text":"RabbitMQ使用AMQP通信协议（AMQP是一个提供统一消息服务的应用层标准协议，基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。）。安装具体教程参考：http://www.rabbitmq.com/tutorials/tutorial-one-python.html 安装python的开发库1sudo pip install pika==0.9.8 安装rabbitmq服务 1sudo apt-get install rabbitmq-server 启动rabbit服务 12rabbitmq-server startHello World 发送 通过使用pika第三方库来连接，本例中为localhost本地连接 通过queue_declare来创建一个消息队列 发送消息basic_publish 关闭链接close 123456789101112131415#!/usr/bin/env python# coding=utf-8import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))channel = connection.channel()channel.queue_declare(queue='hello')channel.basic_publish(exchange='', routing_key='hello', body='hello world!')print 'Sent \"hello world\"'connection.close() 接收 通过使用pika第三方库来连接，本例中为localhost本地连接。 通过queue_declare来创建一个消息队列（推荐，由于接收之前消息队列有可能并没有创建过） 指定消息队列和回调函数basic_consume 开始接受消息start_consuming #!/usr/bin/env python # coding=utf-8 import pika def callback(ch, method, properties, body): print 'receive %r'%body connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() channel.queue_declare(queue='hello') channel.basic_consume(callback, queue='hello', no_ack=True) channel.start_consuming()","categories":[{"name":"mq","slug":"mq","permalink":"http://simyy.com/categories/mq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://simyy.com/tags/rabbitmq/"}]},{"title":"php后台开发（二）Laravel框架","slug":"php-Laravel","date":"2015-03-11T14:39:43.000Z","updated":"2017-04-08T09:28:21.000Z","comments":true,"path":"2015/03/11/php-Laravel/","link":"","permalink":"http://simyy.com/2015/03/11/php-Laravel/","excerpt":"为了提高后台的开发效率，往往需要选择一套适合自己的开发框架，因此，选择了功能比较完善的Laravel框架，仔细学来，感觉和Python语言的框架Django非常类似。","text":"为了提高后台的开发效率，往往需要选择一套适合自己的开发框架，因此，选择了功能比较完善的Laravel框架，仔细学来，感觉和Python语言的框架Django非常类似。 Laravel框架 Laravel是一套web应用开发框架，它具有富于表达性且简洁的语法，并提供了验证（authentication）、路由（routing）、session和缓存（caching）等开发过程中经常用到的工具或功能。 框架安装安装composer123http://docs.phpcomposer.com/download/curl -sS https://getcomposer.org/installer | phpmv composer.phar /usr/bin/composer 由于国外的景象不稳定，所以连接国内的景象，具体方法参考：http://pkg.phpcomposer.com/。（选择方法1的配置方法） 安装Laravel通过composer安装Lavarel。1composer global require &quot;laravel/installer=~1.1&quot; 把 ~/.composer/vendor/bin 路径添加到 PATH 环境变量里, 这样laravel 可执行文件才能被命令行找到。1ln -s ~/.composer/vendor/bin/laravel /usr/local/bin/laravel 创建新工程安装依赖库安装启动MCrypt PHP Extension123sudo apt-get install php5-mcryptsudo php5enmod mcrypt启动Mod_rewrite 模块 常见的情况修改二级route，会出现404情况，解决方法就是启动该模块。1sudo a2enmod rewrite 安装方法（1）Laravel安装器安装1laravel new demo 安装方法（2）Composer 的 create-project 命令安装1composer create-project laravel/laravel project_name 安装方法（3）免安装下载完整工程12点击下面的链接下载文件，版本为4.2http://files.cnblogs.com/files/coder2012/complete-laravel4-master.zip 使用Laravel配置apache修改配置文件apache2.conf1vim /etc/apache2/apache2.conf 修改并增加内容：1234567891011&lt;Directory /&gt; Options FollowSymLinks AllowOverride All Require all granted &lt;/Directory&gt;&lt;Directory /opt/work/laravel/public&gt; Options Indexes FollowSymLinks AllowOverride All Require all granted&lt;/Directory&gt; 修改配置文件000-default.conf1vim /etc/apache2/sites-available/000-default.conf 修改内容：1DocumentRoot /opt/work/laravel/public 重启apache1/etc/init.d/apache2 restart 访问http://localhost/，如果看到如下图片则表明成功。","categories":[{"name":"php","slug":"php","permalink":"http://simyy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"http://simyy.com/tags/php/"},{"name":"framework","slug":"framework","permalink":"http://simyy.com/tags/framework/"}]},{"title":"php后台开发（一）hello world","slug":"php-hello","date":"2015-03-09T03:34:18.000Z","updated":"2017-04-08T09:28:31.000Z","comments":true,"path":"2015/03/09/php-hello/","link":"","permalink":"http://simyy.com/2015/03/09/php-hello/","excerpt":"php hello world!","text":"php hello world! 环境安装开发环境为Ubuntu 12.04，选择linux+apache+php的开发环境 安装 apache21sudo apt-get install apache2 安装php1sudo apt-get install php5 安装mysql(注意安装过程中，需要输入root密码)1sudo apt-get install mysql-server 安装apache的解析模块12sudo apt-get install libapache2-mod-php5sudo apt-get install libapache2-mod-auth-mysql 安装php的mysql连接模块1sudo apt-get install php5-mysql 安装处理图片的GD库1sudo apt-get install php5-gd 启动apache默认情况下根目录为/var/www/html 输入测试地址http://localhost/ 改变默认开发路径apache2的默认文件目录的配置路径：/etc/apache2/sites-enabled/000-default.conf 修改DocumentRoot后的内容为开发目录的路径1DocumentRoot /opt/webroot/xxx apache2的默认配置文件的路径：/etc/apache2/apache2.conf 添加文件路径的权限12345&lt;Directory /opt/webroot/xx &gt; Option Indexes FollowSymLinks AllowOverride None Require all granted&lt;/Directory&gt; hello world 在开发目录下，打开一个文件index.php, 输入123&lt;?php echo \"&lt;h1&gt;hello world!&lt;/h1&gt;\";?&gt; 打开:址http://localhost/就会看到结果。","categories":[{"name":"php","slug":"php","permalink":"http://simyy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"http://simyy.com/tags/php/"},{"name":"framework","slug":"framework","permalink":"http://simyy.com/tags/framework/"}]},{"title":"Python __setattr__, __getattr__, __delattr__, __call__","slug":"python-set-get-del-call-attr","date":"2015-03-04T16:14:05.000Z","updated":"2017-04-08T09:27:22.000Z","comments":true,"path":"2015/03/05/python-set-get-del-call-attr/","link":"","permalink":"http://simyy.com/2015/03/05/python-set-get-del-call-attr/","excerpt":"介绍python的几个内建函数。","text":"介绍python的几个内建函数。 getattrgetattr函数属于内建函数，可以通过函数名称获取12value = obj.attributevalue = getattr(obj, \"attribute\") 使用getattr来实现工厂模式 一个模块支持html、text、xml等格式的打印，根据传入的formate参数的不同，调用不同的函数实现几种格式的输出12345import statsout def output(data, format=\"text\"): output_function = getattr(statsout, \"output_%s\" %format) return output_function(data) call__call__方法用于实例自身的调用：123456789101112class storage(dict): # __call__方法用于实例自身的调用 #达到()调用的效果 def __call__ (self, key): try: return self[key] except KeyError, k: return Nones = storage()s['key'] = 'value'print s(key) #调用__call__ getattr从对象中读取某个属性时，首先需要从self.dicts中搜索该属性，再从getattr中查找。12345678910111213class A(object): def __init__(self): self.name = 'from __dicts__: zdy' def __getattr__(self, item): if item == 'name': return 'from __getattr__: zdy' elif item == 'age': return 26 a = A() print a.name # 从__dict__里获得的 print a.age # 从__getattr__获得的 setattr__setattr__函数是用来设置对象的属性，通过object中的setattr函数来设置属性： 12345class A(object): def __setattr__(self, *args, **kwargs): print 'call func set attr' return object.__setattr__(self, *args, **kwargs) __delattr__ __delattr__函数式用来删除对象的属性： 1234class A(object): def __delattr__(self, *args, **kwargs): print 'call func del attr' return object.__delattr__(self, *args, **kwargs) 例子 完整例子可以参考微博API：http://github.liaoxuefeng.com/sinaweibopy/12345678910111213141516171819202122232425262728293031323334353637class _Executable(object): def __init__(self, client, method, path): self._client = client self._method = method self._path = path #__call__函数实现_Executable函数对象为可调用的 def __call__(self, **kw): method = _METHOD_MAP[self._method] if method==_HTTP_POST and 'pic' in kw: method = _HTTP_UPLOAD return _http_call('%s%s.json' % (self._client.api_url, self._path), method, self._client.access_token, **kw) def __str__(self): return '_Executable (%s %s)' % (self._method, self._path) __repr__ = __str__class _Callable(object): def __init__(self, client, name): self._client = client self._name = name def __getattr__(self, attr): if attr=='get': #初始化_Executable对象，调用__init__函数 return _Executable(self._client, 'GET', self._name) if attr=='post': return _Executable(self._client, 'POST', self._name) name = '%s/%s' % (self._name, attr) return _Callable(self._client, name) def __str__(self): return '_Callable (%s)' % self._name __repr__ = __str__ 而在源码中，存在下面代码片段：12345678910class APIClient(object): ''' API client using synchronized invocation. ''' ... def __getattr__(self, attr): if '__' in attr: return getattr(self.get, attr) return _Callable(self, attr) 因此，加入我们初始化对象，并调用某函数如下：123client = APIClient(...)#会调用__getattr__函数，从而调用__call__函数client.something.get()","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"memcache 事件模型","slug":"memcache-event-model","date":"2015-02-09T06:27:10.000Z","updated":"2017-04-08T09:29:28.000Z","comments":true,"path":"2015/02/09/memcache-event-model/","link":"","permalink":"http://simyy.com/2015/02/09/memcache-event-model/","excerpt":"在memcachedd中，作者为了专注于缓存的设计，使用了libevent来开发事件模型。memcachedd的时间模型同nginx的类似，拥有一个主进行（master）以及多个工作者线程（woker）。","text":"在memcachedd中，作者为了专注于缓存的设计，使用了libevent来开发事件模型。memcachedd的时间模型同nginx的类似，拥有一个主进行（master）以及多个工作者线程（woker）。 流程图在memcached中，是先对工作者线程进行初始化并启动，然后才会创建启动主线程。 工作者线程初始化memcached对工作者线程进行初始化，参数分别为线程数量以及main_base， 12/* start up worker threads if MT mode */thread_init(settings.num_threads, main_base); 在memcachedd中为了避免多线程共享资源的使用使用了很多锁，这里对锁不做介绍。 线程的结构体1234567891011typedef struct &#123; pthread_t thread_id; /* unique ID of this thread 线程ID*/ struct event_base *base; /* libevent handle this thread uses libevent事件*/ struct event notify_event; /* listen event for notify pipe 注册事件*/ int notify_receive_fd; /* receiving end of notify pipe 管道中接收端*/ int notify_send_fd; /* sending end of notify pipe 管道中发送端*/ struct thread_stats stats; /* Stats generated by this thread 线程状态*/ struct conn_queue *new_conn_queue; /* queue of new connections to handle 消息队列*/ cache_t *suffix_cache; /* suffix cache */ uint8_t item_lock_type; /* use fine-grained or global item lock */&#125; LIBEVENT_THREAD; 初始化工作者线程1234567891011121314151617for (i = 0; i &lt; nthreads; i++) &#123; int fds[2]; /* 创建管道 */ if (pipe(fds)) &#123; perror(\"Can't create notify pipe\"); exit(1); &#125; /* 设置线程管道的读写入口 */ threads[i].notify_receive_fd = fds[0]; threads[i].notify_send_fd = fds[1]; /* 设置线程属性 */ setup_thread(&amp;threads[i]); /* Reserve three fds for the libevent base, and two for the pipe */ stats.reserved_fds += 5;&#125; 设置线程属性123456789101112131415161718192021222324/* * Set up a thread's information. */static void setup_thread(LIBEVENT_THREAD *me) &#123; me-&gt;base = event_init(); //初始化线程事件 if (! me-&gt;base) &#123; fprintf(stderr, \"Can't allocate event base\\n\"); exit(1); &#125; /* 初始化监听事件 */ /* Listen for notifications from other threads */ event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd, EV_READ | EV_PERSIST, thread_libevent_process, me); /* 把事件绑定到线程事件 */ event_base_set(me-&gt;base, &amp;me-&gt;notify_event); /* 注册事件到监听状态 */ if (event_add(&amp;me-&gt;notify_event, 0) == -1) &#123; fprintf(stderr, \"Can't monitor libevent notify pipe\\n\"); exit(1); &#125; ...&#125; READ回调函数1234567891011121314151617/* * Processes an incoming \"handle a new connection\" item. This is called when * input arrives on the libevent wakeup pipe. */static void thread_libevent_process(int fd, short which, void *arg) &#123; ... /* 从管道读取消息 */ if (read(fd, buf, 1) != 1) if (settings.verbose &gt; 0) fprintf(stderr, \"Can't read from libevent pipe\\n\"); item = cq_pop(me-&gt;new_conn_queue); //读取连接 ...&#125; 启动工作者线程1234/* Create threads after we've done all the libevent setup. */for (i = 0; i &lt; nthreads; i++) &#123; create_worker(worker_libevent, &amp;threads[i]);&#125; create_woker函数创建工作者线程， 12345678910111213141516/* * Creates a worker thread. */static void create_worker(void *(*func)(void *), void *arg) &#123; pthread_t thread; pthread_attr_t attr; int ret; pthread_attr_init(&amp;attr); if ((ret = pthread_create(&amp;thread, &amp;attr, func, arg)) != 0) &#123; fprintf(stderr, \"Can't create thread: %s\\n\", strerror(ret)); exit(1); &#125;&#125; worker_libevent函数进入线程循环监听状态， 12345678910111213141516171819202122/* * Worker thread: main event loop */static void *worker_libevent(void *arg) &#123; LIBEVENT_THREAD *me = arg; /* Any per-thread setup can happen here; thread_init() will block until * all threads have finished initializing. */ /* set an indexable thread-specific memory item for the lock type. * this could be unnecessary if we pass the conn *c struct through * all item_lock calls... */ me-&gt;item_lock_type = ITEM_LOCK_GRANULAR; pthread_setspecific(item_lock_type_key, &amp;me-&gt;item_lock_type); register_thread_initialized(); event_base_loop(me-&gt;base, 0); return NULL;&#125; 主线程初始化1234static struct event_base* mian_base;/* initialize main thread libevent instance */main_base = event_init(); 在memcached.c的主函数中，使用libevent的事件初始化函数来初始化main_base。 初始化socket这里只介绍tcp连接，其中使用server_sockets来调用server_socket来初始化连接。 1234567891011if (settings.port &amp;&amp; server_sockets(settings.port, tcp_transport, portnumber_file)) &#123; vperror(\"failed to listzhefen on TCP port %d\", settings.port); exit(EX_OSERR);&#125;static int server_sockets(int port, enum network_transport transport, FILE *portnumber_file) &#123; if (settings.inter == NULL) &#123; return server_socket(settings.inter, port, transport, portnumber_file); &#125; ...&#125; 而在server_socket中完成了socket的初始化、绑定等操作。 主线程事件在主线程中通过conn_new函数来建立主线程和工作者线程之间的关系。 12345678910/* 设置线程事件 */event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);event_base_set(base, &amp;c-&gt;event);c-&gt;ev_flags = event_flags;/* 注册事件到监听 */if (event_add(&amp;c-&gt;event, 0) == -1) &#123; perror(\"event_add\"); return NULL;&#125; 事件处理上面中设置了事件的回调函数event_handler，而在event_handler中，主要调用了driver_machine函数。 driver_machine看名字就知道，想发动机一样的函数，那么该函数主要是处理各种事件以及相应的处理方法。 这里只简要介绍一个函数调用dispatch_conn_new。 12345678910111213141516171819202122232425262728293031void dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags, int read_buffer_size, enum network_transport transport) &#123; CQ_ITEM *item = cqi_new(); char buf[1]; if (item == NULL) &#123; close(sfd); /* given that malloc failed this may also fail, but let's try */ fprintf(stderr, \"Failed to allocate memory for connection object\\n\"); return ; &#125; int tid = (last_thread + 1) % settings.num_threads; LIBEVENT_THREAD *thread = threads + tid; //循环获取工作者线程 last_thread = tid; item-&gt;sfd = sfd; item-&gt;init_state = init_state; item-&gt;event_flags = event_flags; item-&gt;read_buffer_size = read_buffer_size; item-&gt;transport = transport; cq_push(thread-&gt;new_conn_queue, item); //连接加入懂啊队列 memcachedD_CONN_DISPATCH(sfd, thread-&gt;thread_id); buf[0] = 'c'; if (write(thread-&gt;notify_send_fd, buf, 1) != 1) &#123;//向管道写入消息 perror(\"Writing to thread notify pipe\"); &#125;&#125;","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"memcache","slug":"memcache","permalink":"http://simyy.com/tags/memcache/"}]},{"title":"memcache 概述","slug":"memcache-summary","date":"2015-02-07T04:07:10.000Z","updated":"2017-04-08T09:29:23.000Z","comments":true,"path":"2015/02/07/memcache-summary/","link":"","permalink":"http://simyy.com/2015/02/07/memcache-summary/","excerpt":"memcache是一种支持分布式的缓存系统，基于网络连接（当然它也可以使用localhost）方式完成服务，本身它是一个独立于应用的程序或守护进程（Daemon方式）。","text":"memcache是一种支持分布式的缓存系统，基于网络连接（当然它也可以使用localhost）方式完成服务，本身它是一个独立于应用的程序或守护进程（Daemon方式）。 本地缓存memcached是“分布式”的内存对象缓存系统，那么就是说，那些不需要“分布”的，不需要共享的，或者干脆规模小到只有一台服务器的应用，memcached不会带来任何好处，相反还会拖慢系统效率，因为网络连接同样需要资源，即使是UNIX本地连接也一样。memcached本地读写速度要比直接PHP内存数组慢几十倍，而APC、共享内存方式都和直接数组差不多。可见，如果只是本地级缓存，使用memcached是非常不划算的。 并发连接memcached使用libevent库实现网络连接服务，理论上可以处理无限多的连接，但是它和Apache不同，它更多的时候是面向稳定的持续连接的，所以它实际的并发能力是有限制的。在保守情况下memcached的最大同时连接数为200，这和Linux线程能力有关系，这个数值是可以调整的。关于libevent可以参考相关文档。 内存管理memcached内存使用方式也和APC不同。APC是基于共享内存和MMAP的，memcachd有自己的内存分配算法和管理方式，它和共享内存没有关系，也没有共享内存的限制，通常情况下，每个memcached进程可以管理2GB的内存空间，如果需要更多的空间，可以增加进程数。 降低数据库压力memcached在很多时候都是作为数据库前端cache使用的。因为它比数据库少了很多SQL解析、磁盘操作等开销，而且它是使用内存来管理数据的，所以它可以提供比直接读取数据库更好的性能，在大型系统中，访问同样的数据是很频繁的，memcached可以大大降低数据库压力，使系统执行效率提升。 memcached也经常作为服务器之间数据共享的存储媒介，例如在SSO系统中保存系统单点登陆状态的数据就可以保存在memcached中，被多个应用共享。 应用由于memcache是分布式的，在web项目中可以为不同的服务器提供共享资源。由于memcache是把数据直接存储在内存中，那么为了降低数据库的压力，可以再web与数据库之间增加一层memcache缓存。而且在memcache中，内存中存储的数据是根据哈希来实现存储的，那么这样查找数据的销量就会远远高于数据库。","categories":[{"name":"cache","slug":"cache","permalink":"http://simyy.com/categories/cache/"}],"tags":[{"name":"memcache","slug":"memcache","permalink":"http://simyy.com/tags/memcache/"}]},{"title":"python 文件操作","slug":"python-file-operate","date":"2014-12-04T03:25:00.000Z","updated":"2017-04-08T09:27:56.000Z","comments":true,"path":"2014/12/04/python-file-operate/","link":"","permalink":"http://simyy.com/2014/12/04/python-file-operate/","excerpt":"python文件相关操作涉及到的函数介绍。","text":"python文件相关操作涉及到的函数介绍。 1234567891011121314151617181920212223242526272829303132333435363738394041# os 模块os.sep 可以取代操作系统特定的路径分隔符。windows下为 &apos;\\\\&apos;os.name 字符串指示你正在使用的平台。比如对于Windows，它是&apos;nt&apos;，而对于Linux/Unix用户，它是 &apos;posix&apos;os.getcwd() 函数得到当前工作目录，即当前Python脚本工作的目录路径os.getenv() 获取一个环境变量，如果没有返回noneos.putenv(key, value) 设置一个环境变量值os.listdir(path) 返回指定目录下的所有文件和目录名os.remove(path) 函数用来删除一个文件os.system(command) 函数用来运行shell命令os.linesep 字符串给出当前平台使用的行终止符。例如，Windows使用 &apos;\\r\\n&apos;，Linux使用 &apos;\\n&apos; 而Mac使用 &apos;\\r&apos;os.path.split(path) 函数返回一个路径的目录名和文件名os.path.isfile() 和os.path.isdir()函数分别检验给出的路径是一个文件还是目录os.path.exists() 函数用来检验给出的路径是否真地存在os.curdir 返回当前目录 (&apos;.&apos;)os.mkdir(path) 创建一个目录os.makedirs(path) 递归的创建目录os.chdir(dirname) 改变工作目录到dirname os.path.getsize(name) 获得文件大小，如果name是目录返回0Los.path.abspath(name) 获得绝对路径os.path.normpath(path) 规范path字符串形式os.path.splitext() 分离文件名与扩展名os.path.join(path,name) 连接目录与文件名或目录os.path.basename(path) 返回文件名os.path.dirname(path) 返回文件路径os.walk(top,topdown=True,onerror=None) 遍历迭代目录os.rename(src, dst) 重命名file或者directory src到dst 如果dst是一个存在的directory, 将抛出OSError. 在Unix, 如果dst在存且是一个file, 如果用户有权限的话，它将被安静的替换. 操作将会失败在某些Unix 中如果src和dst在不同的文件系统中. 如果成功, 这命名操作将会是一个原子操作 (这是POSIX 需要). 在 Windows上, 如果dst已经存在, 将抛出OSError，即使它是一个文件. 在unix，Windows中有效。os.renames(old, new) 递归重命名文件夹或者文件。像rename()# shutil 模块shutil.copyfile( src, dst) 从源src复制到dst中去。当然前提是目标地址是具备可写权限。抛出的异常信息为IOException. 如果当前的dst已存在的话就会被覆盖掉shutil.move( src, dst) 移动文件或重命名shutil.copymode( src, dst) 只是会复制其权限其他的东西是不会被复制的shutil.copystat( src, dst) 复制权限、最后访问时间、最后修改时间shutil.copy( src, dst) 复制一个文件到一个文件或一个目录shutil.copy2( src, dst) 在copy上的基础上再复制文件最后访问时间与修改时间也复制过来了，类似于cp –p的东西shutil.copy2( src, dst) 如果两个位置的文件系统是一样的话相当于是rename操作，只是改名；如果是不在相同的文件系统的话就是做move操作shutil.copytree( olddir, newdir, True/Flase)把olddir拷贝一份newdir，如果第3个参数是True，则复制目录时将保持文件夹下的符号连接，如果第3个参数是False，则将在复制的目录下生成物理副本来替代符号连接shutil.rmtree( src ) 递归删除一个目录以及目录内的所有内容","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 装饰器","slug":"python-decorator","date":"2014-11-08T16:40:12.000Z","updated":"2017-04-08T09:28:01.000Z","comments":true,"path":"2014/11/09/python-decorator/","link":"","permalink":"http://simyy.com/2014/11/09/python-decorator/","excerpt":"python装饰器为python提供了一种语法糖，本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。","text":"python装饰器为python提供了一种语法糖，本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。 未使用装饰器的函数嵌套deco运行，但myfunc并没有运行 12345678def deco(func): print 'before func' return func def myfunc(): print 'myfunc() called' myfunc = deco(myfunc) 需要的deco中调用myfunc，这样才可以执行 12345678910def deco(func): print 'before func' func() print 'after func' return func def myfunc(): print 'myfunc() called' myfunc = deco(myfunc) 使用装饰器@函数名 但是它执行了两次 1234567891011def deco(func): print 'before func' func() print 'after func' return func @decodef myfunc(): print 'myfunc() called'myfunc() 正确的用法 123456789101112def deco(func): def _deco(): print 'before func' func() print 'after func' return _deco @decodef myfunc(): print 'myfunc() called' myfunc() 带参数的装饰器@带参数，使用嵌套的方法，其中装饰器有参数，函数无参数 12345678910111213141516171819202122232425262728293031323334353637def deco(arg): def _deco(func): print arg def __deco(): print 'before func' func() print 'after func' return __deco return _deco @deco('deco')def myfunc(): print 'myfunc() called' myfunc()``` ## 函数参数传递，装饰器和函数均有参数只有一个固定参数arg和str```pythondef deco(arg): def _deco(func): print arg def __deco(str): print 'before func' func(str) print 'after func' return __deco return _deco @deco('deco')def myfunc(str): print 'myfunc() called ', str myfunc('hello') 未知参数个数 123456789101112131415161718192021def deco(arg): def _deco(func): print arg def __deco(*args, **kwargs): print 'before func' func(*args, **kwargs) print 'after func' return __deco return _deco @deco('deco1')def myfunc1(str): print 'myfunc1() called ', str@deco('deco2')def myfunc2(str1,str2): print 'myfunc2() called ', str1, str2 myfunc1('hello') myfunc2('hello', 'world') class作为修饰器1234567891011121314151617class myDecorator(object): def __init__(self, fn): print \"inside myDecorator.__init__()\" self.fn = fn def __call__(self): self.fn() print \"inside myDecorator.__call__()\" @myDecoratordef aFunction(): print \"inside aFunction()\" print \"Finished decorating aFunction()\" aFunction() 1234567891011121314151617181920class myDecorator(object): def __init__(self, str): print \"inside myDecorator.__init__()\" self.str = str print self.str def __call__(self, fn): def wrapped(*args, **kwargs): fn() print \"inside myDecorator.__call__()\" return wrapped @myDecorator('this is str')def aFunction(): print \"inside aFunction()\" print \"Finished decorating aFunction()\" aFunction() 实例给函数做缓存 — 斐波拉契数列1234567891011121314151617181920212223from functools import wrapsdef memo(fn): cache = &#123;&#125; miss = object() @wraps(fn) def wrapper(*args): result = cache.get(args, miss) if result is miss: result = fn(*args) cache[args] = result return result return wrapper @memodef fib(n): if n &lt; 2: return n return fib(n - 1) + fib(n - 2)print fib(10) 注册回调函数 — web请求回调12345678910111213141516171819202122232425262728class MyApp(): def __init__(self): self.func_map = &#123;&#125; def register(self, name): def func_wrapper(func): self.func_map[name] = func return func return func_wrapper def call_method(self, name=None): func = self.func_map.get(name, None) if func is None: raise Exception(&quot;No function registered against - &quot; + str(name)) return func() app = MyApp() @app.register(&apos;/&apos;)def main_page_func(): return &quot;This is the main page.&quot; @app.register(&apos;/next_page&apos;)def next_page_func(): return &quot;This is the next page.&quot; print app.call_method(&apos;/&apos;)print app.call_method(&apos;/next_page&apos;) mysql封装 – 很好用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import umysqlfrom functools import wraps class Configuraion: def __init__(self, env): if env == &quot;Prod&quot;: self.host = &quot;coolshell.cn&quot; self.port = 3306 self.db = &quot;coolshell&quot; self.user = &quot;coolshell&quot; self.passwd = &quot;fuckgfw&quot; elif env == &quot;Test&quot;: self.host = &apos;localhost&apos; self.port = 3300 self.user = &apos;coolshell&apos; self.db = &apos;coolshell&apos; self.passwd = &apos;fuckgfw&apos; def mysql(sql): _conf = Configuraion(env=&quot;Prod&quot;) def on_sql_error(err): print err sys.exit(-1) def handle_sql_result(rs): if rs.rows &gt; 0: fieldnames = [f[0] for f in rs.fields] return [dict(zip(fieldnames, r)) for r in rs.rows] else: return [] def decorator(fn): @wraps(fn) def wrapper(*args, **kwargs): mysqlconn = umysql.Connection() mysqlconn.settimeout(5) mysqlconn.connect(_conf.host, _conf.port, _conf.user, \\ _conf.passwd, _conf.db, True, &apos;utf8&apos;) try: rs = mysqlconn.query(sql, &#123;&#125;) except umysql.Error as e: on_sql_error(e) data = handle_sql_result(rs) kwargs[&quot;data&quot;] = data result = fn(*args, **kwargs) mysqlconn.close() return result return wrapper return decorator @mysql(sql = &quot;select * from coolshell&quot; )def get_coolshell(data): ... ... ... ..``` ### 线程异步from threading import Threadfrom functools import wraps def async(func): @wraps(func) def async_func(*args, **kwargs): func_hl = Thread(target = func, args = args, kwargs = kwargs) func_hl.start() return func_hl return async_func if __name__ == &apos;__main__&apos;: from time import sleep @async def print_somedata(): print &apos;starting print_somedata&apos; sleep(2) print &apos;print_somedata: 2 sec passed&apos; sleep(2) print &apos;print_somedata: 2 sec passed&apos; sleep(2) print &apos;finished print_somedata&apos; def main(): print_somedata() print &apos;back in main&apos; print_somedata() print &apos;back in main&apos; main() 注意functools.wraps()函数的作用：调用经过装饰的函数，相当于调用一个新函数，那查看函数参数，注释，甚至函数名的时候，就只能看到装饰器的相关信息，被包装函数的信息被丢掉了。 而wraps则可以帮你转移这些信息，具体参见http://stackoverflow.com/questions/308999/what-does-functools-wraps-do","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 序列","slug":"python-list","date":"2014-11-01T09:33:11.000Z","updated":"2017-04-08T09:27:46.000Z","comments":true,"path":"2014/11/01/python-list/","link":"","permalink":"http://simyy.com/2014/11/01/python-list/","excerpt":"序列list是python中最基本的数据结构，序列都可以进行的操作包括索引，切片，加，乘，检查成员。","text":"序列list是python中最基本的数据结构，序列都可以进行的操作包括索引，切片，加，乘，检查成员。列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 一个循环在C语言等其他语言中，for循环一般是这样的 123456789x = list()for i in range(10) x.append(x)``` 如果python也这么写，那么真该看下python的基础教程了~```pythonx = [x for x in range(10)] 在上面的例子中，不仅可以嵌套for，甚至可以嵌套if语句 再看看，原来是什么样子1x = [x for x in range(10) if x &gt; 5] 两个循环呢？原来可能是这样的？ 1234567x = list()for i in range(10): for j in range(10): x.append(i + j)``` 现在可以这样了！！！ x = [x + y for x in range(10) for y in range(10)]```","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"webpy中的session实现","slug":"python-webpy-session","date":"2014-10-22T07:14:00.000Z","updated":"2017-04-08T09:27:00.000Z","comments":true,"path":"2014/10/22/python-webpy-session/","link":"","permalink":"http://simyy.com/2014/10/22/python-webpy-session/","excerpt":"虽然工作中使用的是django，但是自己并不喜欢那种大而全的东西~什么都给你准备好了，自己好像一个机器人一样赶着重复的基本工作，从在学校时候就养成了追究原理的习惯，从而有了这篇session的使用和说明。","text":"虽然工作中使用的是django，但是自己并不喜欢那种大而全的东西~什么都给你准备好了，自己好像一个机器人一样赶着重复的基本工作，从在学校时候就养成了追究原理的习惯，从而有了这篇session的使用和说明。 webpy中的session下面为官方的例子，用session来存储页面访问的次数，从而实现对访问次数的记录。 需要注意的是，官方说明在调试情况下，session并不能正常的运行，所以需要在非调试摸下测试，那么就有了下面的这个例子。 1234567891011121314151617181920212223import web#非调试模式web.config.debug = Falseurls = ( \"/count\", \"count\", \"/reset\", \"reset\")app = web.application(urls, locals())session = web.session.Session(app, web.session.DiskStore('sessions'), initializer=&#123;'count': 0&#125;)class count: def GET(self): session.count += 1 return str(session.count)class reset: def GET(self): session.kill() return \"\"if __name__ == \"__main__\": app.run() 在官方文档中，对上述debug模式的现象给出了这样的解释： session与调试模试下的重调用相冲突(有点类似firefox下著名的Firebug插件，使用Firebug插件分析网页时，会在火狐浏览器之外单独对该网页发起请求，所以相当于同时访问该网页两次) 为了解决上述问题，官方给出了进一步的解决方法，如下 1234567891011121314151617181920import weburls = (\"/\", \"hello\")app = web.application(urls, globals())if web.config.get('_session') is None: session = web.session.Session(app, web.session.DiskStore('sessions'), &#123;'count': 0&#125;) web.config._session = sessionelse: session = web.config._sessionclass hello: def GET(self): print 'session', session session.count += 1 return 'Hello, %s!' % session.countif __name__ == \"__main__\": app.run() 由于web.session.Session会重载两次，但是在上面的_session并不会重载两次，因为上面多了一个判断_session是否存在于web.config中。 其实，在web.py文件中，定义了config，而Storage在下面的图中并没有特殊的结果，像字典一样~ 12345#web.pyconfig = storage()#utils.pystorage = Storage webpy的子程序中使用session虽然官方文档中提到，只能在主程序中使用session，但是通过添加__init__.py可以条用到该页面的session，也就是说一样使用session。 官方给出的方法更加合理化一点，通过应用处理器，加载钩子(loadhooks) 在webpy中，应用处理器为app.add_processor(my_processor)，下面的代码添加到上述的完整例子中，可以再处理请求前和处理请求后分别条用my_loadhook()``和my_unloadhook(), 12345678def my_loadhook(): print \"my load hook\"def my_unloadhook(): print \"my unload hook\"app.add_processor(web.loadhook(my_loadhook))app.add_processor(web.unloadhook(my_unloadhook)) 从而，可以再web.loadhook()中加载session信息，在处理之前从web.ctx.session中获取session了，甚至可以在应用处理器中添加认证等操作。 12345678910111213141516#main.pydef session_hook(): web.ctx.session = sessionapp.add_processor(web.loadhook(session_hook))#views.pyclass edit: def GET(self): try: session = web.ctx.session username = session.username if not username: return web.redirect(&apos;/login&apos;) except Exception as e: return web.redirect(&apos;/login&apos;) return render_template(&apos;edit.html&apos;) session id对于服务器来说，怎样才能区分不同客户端呢，怎样才能区分不同客户端的session呢？ 是通过sessionid来实现的，最初我还傻傻的分不清session和cookie，以及不同用户之间的信息室如何分配的！ 生成sessionid的代码段，其中包含了随机数、时间、ip以及秘钥, 1234567891011def _generate_session_id(self): \"\"\"Generate a random id for session\"\"\" while True: rand = os.urandom(16) now = time.time() secret_key = self._config.secret_key session_id = sha1(\"%s%s%s%s\" % (rand, now, utils.safestr(web.ctx.ip), secret_key)) session_id = session_id.hexdigest() if session_id not in self.store: break return session_id 在客户端访问服务器时，服务器会根据上述信息来计算一个针对客户端唯一的sessionid，并通过cookie保存在客户端中。 客户端用cookie保存了sessionID，当我们请求服务器的时候，会把这个sessionID一起发给服务器，服务器会到内存中搜索对应的sessionID，如果找到了对应的 sessionID，说明我们处于登录状态，有相应的权限；如果没有找到对应的sessionID，这说明：要么是我们把浏览器关掉了（后面会说明为什 么），要么session超时了（没有请求服务器超过20分钟），session被服务器清除了，则服务器会给你分配一个新的sessionID。你得重新登录并把这个新的sessionID保存在cookie中。 session的结构上面提到了session在webpy中式一种dict的方式存储， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140class Session(object): \"\"\"Session management for web.py \"\"\" __slots__ = [ \"store\", \"_initializer\", \"_last_cleanup_time\", \"_config\", \"_data\", \"__getitem__\", \"__setitem__\", \"__delitem__\" ] def __init__(self, app, store, initializer=None): self.store = store self._initializer = initializer self._last_cleanup_time = 0 self._config = utils.storage(web.config.session_parameters) self._data = utils.threadeddict() self.__getitem__ = self._data.__getitem__ self.__setitem__ = self._data.__setitem__ self.__delitem__ = self._data.__delitem__ if app: app.add_processor(self._processor) def __contains__(self, name): return name in self._data def __getattr__(self, name): return getattr(self._data, name) def __setattr__(self, name, value): if name in self.__slots__: object.__setattr__(self, name, value) else: setattr(self._data, name, value) def __delattr__(self, name): delattr(self._data, name) def _processor(self, handler): \"\"\"Application processor to setup session for every request\"\"\" self._cleanup() self._load() try: return handler() finally: self._save() def _load(self): \"\"\"Load the session from the store, by the id from cookie\"\"\" cookie_name = self._config.cookie_name cookie_domain = self._config.cookie_domain cookie_path = self._config.cookie_path httponly = self._config.httponly self.session_id = web.cookies().get(cookie_name) # protection against session_id tampering if self.session_id and not self._valid_session_id(self.session_id): self.session_id = None self._check_expiry() if self.session_id: d = self.store[self.session_id] self.update(d) self._validate_ip() if not self.session_id: self.session_id = self._generate_session_id() if self._initializer: if isinstance(self._initializer, dict): self.update(deepcopy(self._initializer)) elif hasattr(self._initializer, '__call__'): self._initializer() self.ip = web.ctx.ip def _check_expiry(self): # check for expiry if self.session_id and self.session_id not in self.store: if self._config.ignore_expiry: self.session_id = None else: return self.expired() def _validate_ip(self): # check for change of IP if self.session_id and self.get('ip', None) != web.ctx.ip: if not self._config.ignore_change_ip: return self.expired() def _save(self): if not self.get('_killed'): self._setcookie(self.session_id) self.store[self.session_id] = dict(self._data) else: self._setcookie(self.session_id, expires=-1) def _setcookie(self, session_id, expires='', **kw): cookie_name = self._config.cookie_name cookie_domain = self._config.cookie_domain cookie_path = self._config.cookie_path httponly = self._config.httponly secure = self._config.secure web.setcookie(cookie_name, session_id, expires=expires, domain=cookie_domain, httponly=httponly, secure=secure, path=cookie_path) def _generate_session_id(self): \"\"\"Generate a random id for session\"\"\" while True: rand = os.urandom(16) now = time.time() secret_key = self._config.secret_key session_id = sha1(\"%s%s%s%s\" %(rand, now, utils.safestr(web.ctx.ip), secret_key)) session_id = session_id.hexdigest() if session_id not in self.store: break return session_id def _valid_session_id(self, session_id): rx = utils.re_compile('^[0-9a-fA-F]+$') return rx.match(session_id) def _cleanup(self): \"\"\"Cleanup the stored sessions\"\"\" current_time = time.time() timeout = self._config.timeout if current_time - self._last_cleanup_time &gt; timeout: self.store.cleanup(timeout) self._last_cleanup_time = current_time def expired(self): \"\"\"Called when an expired session is atime\"\"\" self._killed = True self._save() raise SessionExpired(self._config.expired_message) def kill(self): \"\"\"Kill the session, make it no longer available\"\"\" del self.store[self.session_id] self._killed = True 在webpy的session中，存储方式包括两种DiskStore和DBStore，分别为硬盘存储和数据库存储。 123456789101112131415161718192021222324252627class DiskStore(Store): \"\"\" Store for saving a session on disk \"\"\" ...class DBStore(Store): \"\"\" Store for saving a session in database Needs a table with following columns: session_id CHAR(128) UNIQUE NOT NULL, atime DATATIME NOT NULL DEFAULT current_timestamp, data TEXT \"\"\" ...``` 而session的存储也可以看出来，把sessionid作为key来存储session信息```pythondef _save(self): if not self.get('_killed'): self._set_cookie(self.session_id) self.store[self.session_id] = dict(self._data) else: self._setcookie(self.session_id, expires=-1) 参考 http://doc.outofmemory.cn/python/webpy-cookbook/ http://webpy.org/docs/0.3/tutorial","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"framework","slug":"framework","permalink":"http://simyy.com/tags/framework/"},{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"python 守护进程","slug":"python-deamon","date":"2014-10-22T07:14:00.000Z","updated":"2017-04-08T09:28:06.000Z","comments":true,"path":"2014/10/22/python-deamon/","link":"","permalink":"http://simyy.com/2014/10/22/python-deamon/","excerpt":"python守护进程与linux c实现原理是一样的。","text":"python守护进程与linux c实现原理是一样的。 第一个fork是为了让shell返回，同时让你完成setsid（从你的控制终端移除，这样就不会意外地收到信号）。setsid使得这个进程成为“会话领导（session leader）”，即如果这个进程打开任何终端，该终端就会成为此进程的控制终端。我们不需要一个守护进程有任何控制终端，所以我们又fork一次。在第二次fork之后，此进程不再是一个“会话领导”，这样它就能打开任何文件（包括终端）且不会意外地再次获得一个控制终端 另外说明：umask()函数为进程设置文件模式创建屏蔽字，并返回以前的值在shell命令行输入：umask 就可知当前文件模式创建屏蔽字常见的几种umask值是002，022和027，002阻止其他用户写你的文件，022阻止同组成员和其他用户写你的文件，027阻止同组成员写你的文件以及其他用户读写或执行你的文件rwx-rwx-rwx 代表是777 所有的人都具有权限读写与执行 chmod()改变文件的权限位int dup(int filedes) 返回新文件描述符一定是当前文件描述符中的最小数值int dup2(int filedes, int filedes2);这两个函数返回的新文件描述符与参数filedes共享同一个文件表项。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*-coding:utf-8-*-import sys, os'''将当前进程fork为一个守护进程 注意：如果你的守护进程是由inetd启动的，不要这样做！inetd完成了 所有需要做的事情，包括重定向标准文件描述符，需要做的事情只有 chdir() 和 umask()了'''def daemonize(stdin='/dev/null',stdout= '/dev/null', stderr= 'dev/null'): '''Fork当前进程为守护进程，重定向标准文件描述符 （默认情况下定向到/dev/null） ''' #Perform first fork. try: pid = os.fork() if pid &gt; 0: sys.exit(0) #first parent out except OSError, e: sys.stderr.write(\"fork #1 failed: (%d) %s\\n\" %(e.errno, e.strerror)) sys.exit(1) #从母体环境脱离 os.chdir(\"/\") os.umask(0) os.setsid() #执行第二次fork try: pid = os.fork() if pid &gt; 0: sys.exit(0) #second parent out except OSError, e: sys.stderr.write(\"fork #2 failed: (%d) %s]n\" %(e.errno,e.strerror)) sys.exit(1) #进程已经是守护进程了，重定向标准文件描述符 for f in sys.stdout, sys.stderr: f.flush() si = file(stdin, 'r') so = file(stdout,'a+') se = file(stderr,'a+',0) os.dup2(si.fileno(), sys.stdin.fileno()) os.dup2(so.fileno(), sys.stdout.fileno()) os.dup2(se.fileno(), sys.stderr.fileno())def _example_main(): '''示例函数：每秒打印一个数字和时间戳''' import time sys.stdout.write('Daemon started with pid %d\\n' % os.getpid()) sys.stdout.write('Daemon stdout output\\n') sys.stderr.write('Daemon stderr output\\n') c = 0 while True: sys.stdout.write('%d: %s\\n' %(c, time.ctime())) sys.stdout.flush() c = c+1 time.sleep(1)if __name__ == \"__main__\": daemonize() _example_main()","categories":[{"name":"python","slug":"python","permalink":"http://simyy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://simyy.com/tags/python/"}]},{"title":"哈希算法（二）一致性哈希","slug":"hash-consistent","date":"2014-09-17T15:11:00.000Z","updated":"2017-04-08T09:34:54.000Z","comments":true,"path":"2014/09/17/hash-consistent/","link":"","permalink":"http://simyy.com/2014/09/17/hash-consistent/","excerpt":"一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，经常用于分布式、负载均衡等。","text":"一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，经常用于分布式、负载均衡等。 原理一致哈希是一种特殊的哈希算法。在使用一致哈希算法后，哈希表中平均只需要对 个关键字重新映射，其中 是关键字的数量，是映射节点数量。 然而在传统的哈希表中，添加或删除一个映射节点的几乎需要对所有关键字进行重新映射。 原来的映射大概是这样的，如下图， 没当加入或删除一个新的节点可能都会造成每个节点的映射发生变化，如果黄色的节点代表服务器，那么每一次更新服务器的数量都会造成每个服务器上蓝色的映射节点都会发生变化，当集群数量庞大时每次增删节点所需要的修改操作就会过于庞大。 而在一致性哈希中映射是这样的，如下图，一般一致性hash取值范围为-2^32~2^32，分布在一个圆上 下面画的比较丑，就凑合看吧~~ 其中，黄色节点作为映射节点（实节点），蓝色节点为需要映射到映射节点的key节点，首先，看左边的图，把8个蓝色的key通过hash取值散列在一个范围为0~2^32的圆上其次，选择三个黄色节点作为映射节点，按照圆的顺时针方向，把蓝色节点与黄色节点建立映射关系最后，由于1节点负载为4，最大，那么我们为了降低1节点的负载情况，增加黄色的映射节点4，依然按照顺时针的方向修改原映射，那么只需要改变蓝色的节点7、8以及黄色节点1 实现 一般为了方便起见，我们把黄色的映射节点称为实节点，也就是固定不变的，而蓝色的节点称为虚节点，虚节点需要映射到实节点，每次实节点的增删只会影响距离它最近的节点。 在这里使用C++实现了ConsistentHash算法 在存储节点方面，本程序只是简单的使用链表，最好的方式当然是红黑树了，当然为了简单起见，就用了链表，主要是理解一致性hash的原理~~ 源码下载 参考http://blog.csdn.net/cywosp/article/details/23397179http://zh.wikipedia.org/wiki/一致哈希http://baike.baidu.com/view/1588037.htm?fr=aladdin","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"哈希算法（一）","slug":"hash-basic","date":"2014-09-04T15:11:00.000Z","updated":"2017-04-08T09:34:57.000Z","comments":true,"path":"2014/09/04/hash-basic/","link":"","permalink":"http://simyy.com/2014/09/04/hash-basic/","excerpt":"本文介绍基本的哈希算法。","text":"本文介绍基本的哈希算法。哈希是大家比较常见一个词语，在编程中也经常用到，但是大多数人都是知其然而不知其所以然，再加上这几天想写一个一致性哈希算法，突然想想对哈希也不是很清楚，所以，抽点时间总结下Hash知识。本文参考了很多博文，感谢大家的无私分享。 基本概念Hash，一般翻译做“散列”，也有直接音译为“哈希”的。那么哈希函数的是什么样的？大概就是 value = hash(key)，我们希望key和value之间是唯一的映射关系。 大家使用的最多的就是哈希表（Hash table，也叫散列表），是根据关键码值(Key value)而直接进行访问的数据结构，通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度，这个映射函数叫做哈希函数或散列函数。 实际中的Hash主要有两种应用：加密和压缩。 在加密方面，Hash哈希是把一些不同长度的信息转化成杂乱的128位的编码,这些编码值叫做HASH值，最广泛应用的Hash算法有MD4、MD5、SHA1。 在压缩方面，Hash哈希是指把一个大范围映射到一个小范围，往往是为了节省空间，使得数据容易保存。 Hash的特点主要原理就是把大范围映射到小范围，因此输入范围必须和小范围相当或者比它更小，否则增加冲突。 Hash函数逼近单向函数，所以可以用来对数据进行加密。 单项函数：如果某个函数在给定输入的时候，很容易计算出其结果来；而当给定结果的时候，很难计算出输入来。 不同的应用对Hash函数有着不同的要求：用于加密的Hash函数主要考虑它和单项函数的差距，而用于查找的Hash函数主要考虑它映射到小范围的冲突率。 Hash算法Hash的产生方式大体可以分为三种基本方法：加法、乘法和移位。 加法哈希是通过遍历数据中的元素然后每次对某个初始值进行加操作，其中加的值和这个数据的一个元素相关。1234567891011121314/******************************** *加法哈希 *@key 输入字符串 *@prime 素数 ********************************/int additiveHash(string key, int prime)&#123; int hash, i; for(hash = key.length(), i = 0; i &lt; key.length(); ++i) &#123; hash += int(key.at(i)); &#125; return (hash%prime);&#125; 乘法哈希是通过遍历数据中的元素然后每次对初始值进行乘法操作，其中乘的值无需和数据有关系。123456789101112131415161718192021222324252627282930313233/******************************** *乘法哈希 *@key 输入字符串 *@prime 素数 ********************************/int bernstein(string key)&#123; int hash, i; for(hash = 0, i = 0; i &lt; key.length(); ++i) &#123; hash = 33*hash + int(key.at(i)); &#125; return hash;&#125;/******************************** *32位FNV算法(乘法) *@key 输入字符串 *@prime 素数 ********************************/int M_SHIFT = 0;int M_MASK = 0x8765fed1;int FNVHash(string key)&#123; int hash = (int)2166136261L; for(int i = 0; i &lt; key.length(); ++i) &#123; hash = (hash * 16777619)^int(key.at(i)); &#125; if(M_SHIFT == 0) return hash; return (hash ^ (hash &gt;&gt; M_SHIFT)) &amp; M_MASK;&#125; 在JAVA中，哈希函数使用的就是乘法哈希：12345678910111213/** * JAVA自己带的算法 */public static int java(String str) &#123; int h = 0; int off = 0; int len = str.length(); for (int i = 0; i &lt; len; i++) &#123; h = 31 * h + str.charAt(off++); &#125; return h;&#125; 移位哈希是通过遍历数据中的元素然后每次对初始值进行移位操作。1234567891011121314/******************************** *旋转哈希（移位） *@key 输入字符串 *@prime 素数 ********************************/int rotatingHash(string key, int prime)&#123; int hash, i; for(hash = key.length(), i = 0; i &lt; key.length(); ++i) &#123; hash = (hash &lt;&lt; 4)^(hash &gt;&gt; 28)^int(key.at(i)); &#125; return (hash%prime);&#125; 实际情况下，很多哈希函数都是包含加法、乘法和移位操作来实现的，例如MD5。 问题为什么prime的取值是素数？ 有科学依据么？ 有待验证 有人是这样说的取素数，可以降低碰撞的概率。但是并没有很好的说明原因，如果哪位有想法希望能留下您的想法，分享给大家。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://simyy.com/tags/algorithm/"}]},{"title":"C语言中的内存分配与释放","slug":"malloc-and-free-in-c","date":"2013-06-23T04:53:38.000Z","updated":"2017-04-08T09:29:43.000Z","comments":true,"path":"2013/06/23/malloc-and-free-in-c/","link":"","permalink":"http://simyy.com/2013/06/23/malloc-and-free-in-c/","excerpt":"对C语言一直都是抱着学习的态度，很多都不懂，今天突然被问道C语言的内存分配问题，说了一些自己知道的，但感觉回答的并不完善，所以才有这篇笔记，总结一下C语言中内存分配的主要内容。","text":"对C语言一直都是抱着学习的态度，很多都不懂，今天突然被问道C语言的内存分配问题，说了一些自己知道的，但感觉回答的并不完善，所以才有这篇笔记，总结一下C语言中内存分配的主要内容。 相关问题 刚刚在一篇博文看到一个简单的问题： 1234567891011121314151617181920212223//code1char* toStr() &#123; char *s = \"abcdefghijkl\"; return s;&#125;int main()&#123; cout &lt;&lt; toStr() &lt;&lt; endl; return 0;&#125;//code2char* toStr() &#123; char s[] = \"abcdefghijkl\"; return s;&#125;int main()&#123; cout &lt;&lt; toStr() &lt;&lt; endl; return 0;&#125; 两段代码都很简单，输出一段字符，类型不同，一个是char*字符串，一个是char[]数据。 结果你知道吗? 这个我确实知道，相信大部分人也都回知道，必然有一个不好使，或者两个都不好使！！！都对就没意思了~ 结果：第一个正确输出，第二个输出乱码。 原因在于局部变量的作用域和内存分配的问题，第一char*是指向一个常量，作用域为函数内部，被分配在程序的常量区，直到整个程序结束才被销毁，所以在程序结束前常量还是存在的。而第二个是数组存放的，作用域为函数内部，被分配在栈中，就会在函数调用结束后被释放掉，这时你再调用，肯定就错误了。 内存分配什么是局部变量、全局变量和静态变量？ 顾名思义，局部变量就是在一个有限的范围内的变量，作用域是有限的，对于程序来说，在一个函数体内部声明的普通变量都是局部变量，局部变量会在栈上申请空间，函数结束后，申请的空间会自动释放。而全局变量是在函数体外申请的，会被存放在全局（静态区）上，知道程序结束后才会被结束，这样它的作用域就是整个程序。静态变量和全局变量的存储方式相同，在函数体内声明为static就可以使此变量像全局变量一样使用，不用担心函数结束而被释放。 相关函数： 123456789void *malloc(size_t size);void free(void *p);/*一般这样用Struct elem *p;p = (struct elem*)malloc(sizeof(struct elem))void free(p)*/ malloc原理malloc函数的实质体现在，它有一个将可用的内存块连接为一个长长的列表的所谓空闲链表。调用malloc函数时，它沿连接表寻找一个大到足以满足用户请求所需要的内存块。然后，将该内存块一分为二（一块的大小与用户请求的大小相等，另一块的大小就是剩下的字节）。接下来，将分配给用户的那块内存传给用户，并将剩下的那块（如果有的话）返回到连接表上。调用free函数时，它将用户释放的内存块连接到空闲链上。到最后，空闲链会被切成很多的小内存片段，如果这时用户申请一个大的内存片段，那么空闲链上可能没有可以满足用户要求的片段了。于是，malloc函数请求延时，并开始在空闲链上翻箱倒柜地检查各内存片段，对它们进行整理，将相邻的小空闲块合并成较大的内存块。如果无法获得符合要求的内存块，malloc函数会返回NULL指针，因此在调用malloc动态申请内存块时，一定要进行返回值的判断。 分类 栈区（stack）—由编译器自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 堆区（heap）—一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表 全局区（静态区）（static）—全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 程序结束后由系统释放。 常量区—常量字符串就是放在这里的，直到程序结束后由系统释放。上面的问题就在这里！！！代码区—存放函数体的二进制代码。 直接搬运的代码，确实很好！！容易理解 123456789101112131415//main.cpp int a = 0; //全局初始化区char *p1; //全局未初始化区 main()&#123; int b; //栈 char s[] = \"abc\"; //栈 char *p2; //栈 char *p3 = \"123456\"; //123456\\\\0在常量区，p3在栈上。 static int c =0;//全局（静态）初始化区 p1 = (char *)malloc(10); p2 = (char *)malloc(20);//分配得来得10和20字节的区域就在堆区。 strcpy(p1, \"123456\"); //123456\\\\0放在常量区，编译器可能会将它与p3所指向的\"123456\"优化成一个地方。 &#125; 此外，还有realloc(重新分配内存)、calloc（初始化为0）、alloca（在栈上申请内存，自动释放）等。","categories":[{"name":"c","slug":"c","permalink":"http://simyy.com/categories/c/"}],"tags":[{"name":"c","slug":"c","permalink":"http://simyy.com/tags/c/"}]},{"title":"Linux内核设计与实现笔记（二） 内存管理、进程地址空间","slug":"linux-memory","date":"2013-06-01T03:10:20.000Z","updated":"2017-04-08T09:34:47.000Z","comments":true,"path":"2013/06/01/linux-memory/","link":"","permalink":"http://simyy.com/2013/06/01/linux-memory/","excerpt":"linux内存管理。","text":"linux内存管理。 内存管理页物理页作为内存管理的基本单位。内存管理单元通常以页page为单位进行处理。 区由于页位于内存中特定的物理地址上，所以不能将其用于一些特定的任务，故内核把页划分为不同的区。 硬件在内存寻址方面的问题 一些硬件只能通过内存地址来执行直接内存访问（DMA - Direct Memory Access）;一些体系结构其内存的物理寻址范围大于虚拟寻址范围，故，内存不能永久地映射到内核空间。 解决方法，通过创建三种不同的分区： ZONE_DMA–专门执行DMA ZONE_NORMAL–正常映射的页 ZONE_HIGHMEM–高端内存，不能永久映射到内核空间 获得页内核提供了一种请求内存的底层机制，并提供了对它进行访问的几个接口，以页为单位分配内存,1234567891011121314struct page* alloc_pages(unsigned int gfp_mask, unsigned int order)//该函数分配2^order个连续的物理页，并返回第一个页的page结构体struct page* alloc_page(unsigned int gfp_mask)//order=0void* page_address(struct page* page)//该函数返回page物理页当前的逻辑地址unsigned long _get_free_pages(unsigned it gfp_mask, unsigned int order)//该函数分配2^order个连续的物理页，但返回第一个页的逻辑地址unsigned long _get_free_page(unsigned it gfp_mask）//order=0 释放页申请空间了，自然总要释放掉,1234void _free_pages(struct page *page, unsigned int order)void free_pages(unsigned long addr, unsigned int order)void free_page(unsigned long addr)//释放页时，要谨慎，如果释放错误的页，可能会导致系统崩溃 kmalloc与vmallockmalloc与malloc类似，可以获得以字节为单位的一块内核内存，并且内存区在物理上是连续的。12345678void* kmalloc(size_t size, int flags)//flags是分类器标志void kfree(const void* ptr)//这个要对应使用，谨慎vmalloc的不同之处在于，分配的内存虚拟地址是连续的，而物理地址则是无需连续的。void* vmalloc(unsigned long size)void vfree(void* addr) 大多数情况下，只有硬件设备需要得到物理地址连续的内存。vmalloc仅在为了获得大块内存时才使用。 Slab层slab分配器扮演了通用数据结构缓存层的角色，通过slab层可以缓存频繁分配和释放的数据结构，避免内存碎片，提高性能。 slab层把不同的对象划分为高速缓存组，每个高速缓存都存放不同类型的对象。然后高速缓存又被划分为不同的slab。slab由一个或多个物理上连续的页组成。每个slab有三种状态：满、部分满或空。1page + page + ... + page = a slab 123456789101112//创建高速缓存kmem_cache_t* kem_cache_create(const char* name...)//销毁高速缓存int kmem_cache_destroy(kmem_cache_t *cachep)//获取对象void* kmem_cache_alloc(kmem_cache_t *cachep, int flags)//该函数从给定的高速缓存中返回一个指向对象的指针。如果告诉缓存的所有slab中都没有空闲对象，那么slab层必须通过kmem_getpages获取新的页//释放对象void kmem_cache_free(...) CPU的分配一般来说，每个CPU的数据存放在一个数组中。数组中的每一项对应着系统上存在的一个处理器。由于这个数据对于当前处理器是唯一的，其他处理器不能访问它，故不需要加锁进行操作。 使用每个CPU数据可以减少数据锁定（省去数据上锁），大大减少缓存失效（避免同步，不断刷新缓存）。1234567891011121314151617181920//创建一个类型为type，名字为name的实例DECLARE_PER_CPUT(type, name);DEFINE_PER_CPU(type, name);//增加处理器上的name值get_cput_var(name)++;//激活内核抢占put_cput_var(name);//增加指定处理器CPU上的name值per_cpu(name, cpu)++;运行时的每个CPU数据//分配对象void* alloc_percpu(type)void* _alloc_percpu(size_t size, size_t align)//释放对象void free_percpu(const void*) 进程地址空间Linux操作系统采用虚拟内存技术，因此，系统中的所有进程之间以虚拟方式共享内存。 现代采用虚拟内存的操作系统通常都使用独立连续的地址空间，而不是分段的。因此，进程地址空间之间彼此互不相干，两个不同的进程可以在它们各自的地址空间的相同地址内存放不同的数据。但是，进程之间也可以选择共享地址空间，这样的进程就是所谓的线程。 进程只能访问有效范围内的内存地址。每个内存区域也具有相应进程必须遵循的特定访问属性，如只读、只写等属性。如果一个进程访问了不在有效范围中的地址，或以不正确的方式访问有效地址，那么内核就会种植该进程，并返回“段错误”信息。 内存描述符12345//内存描述符结构体 linx/sched.hstruct mm_struct&#123; struct vm_area_struct *mmap; ....&#125; fork函数通过利用copy_mm函数复制父进程的内存描述符，而子进程中的mm_struct是通过allocate_mm宏从mm_cachep slab缓存中得到的。 如果父进程希望和其子进程共享地址空间，那么在调用clone时，设置CLONE_VM标志，内核就不需要调用alloc_mm函数了，而仅仅需要用copy_mm函数将内存域指向进程的内存描述符。调用exit_mm函数，销毁内存描述符。 注意：内核线程没有进程地址空间，也没有相关的内存描述符。所以内核线程对应的进程描述符中mm域为NULL，这也正式内核线程的真实含义–没有用户上下文。 内存区域内存区域在内核中经常被称为虚拟内存区域或VMA。内核将每个内存区域作为一个单独的内存管理对象，每个内存区域都有一直的属性。12345//内存区域结构体struct vm_area_struct&#123; struct mm_struct *vm_mm; ...&#125; 操作内存区域为了方便执行对内存区域的操作，内核定义了许多的辅助函数。linux/mm.h1234567//搜索内存区域--通过红黑树 mm/mmap.cstruct vm_area_struct *find_vma(struct mm_struct* mm, unsigned long addr)//该函数在指定的地址空间中搜索第一个vm_end大于addr的内存区域struct vm_area_struct* find_vma_prev(struct mm_struct *mm, unsigned long addr, struct vm_area_struct **pprev)//它返回第一个小于addr的VMAstatic inline struct vm_area_struct* find_vma_intersection(struct mm_struct *mm, unsigned long start_addr, unsigned long end_addr)//返回第一个和指定地址区间相交的VMA 创建删除地址空间内核使用do_mmap()函数创建一个新的线性地址空间。这可能会导致扩展已存在的内存区域（和一个已经存在的相邻地址空间的访问权限相同）或创建一个新的区域。123456//创建地址区间unsigned long do_mmap(....)void* mmap(..)//删除地址空间int do_munmap(...)int munmap(...) 页表虽然应用程序操作的对象是映射到物理内存上的虚拟内存，但是处理器直接操作的确实物理内存。所以每当一个程序访问一个虚拟地址时，首先必须将虚拟地址转化为物理地址，然后处理器才能解析地址访问请求。地址的转换工作需要通过查询页表才能完成。也就是说，地址转换需要虚拟地址分段，每段虚拟地址都是一个索引指向页表，而页表项指向下一级别的页表或最终物理页面。 Linux使用三级页表完成地址转换，利用多级页表能够节约地址转换占用的存放空间。 顶级页–页全局目录(PGD)，指向PMD 二级页–中间页目录(PMD)，指向页表 最低级页–页表，指向物理页 搜索页表的工作是硬件完成的。 由于几乎每次对虚拟内存的访问都需解析它，才可以得到物理内存中的对应地址，所以也表操作的性能非常关键。为了加快搜索速度，多数体系结构都实现了一个翻译后缓冲器（translation lookaside buffer,TLB）。","categories":[{"name":"cs","slug":"cs","permalink":"http://simyy.com/categories/cs/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://simyy.com/tags/linux/"}]}]}